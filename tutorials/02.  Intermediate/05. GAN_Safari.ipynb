{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The GAN model in this code is based on [this repository](https://github.com/davidADSP/GDL_code/blob/master/models/GAN.py) and just simplify for understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Flatten, Dense, Conv2D, Activation, Dropout, Reshape\n",
    "from keras.layers import BatchNormalization, UpSampling2D, Conv2DTranspose\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import os\n",
    "from os import walk, getcwd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "weight_init = RandomNormal(mean=0., stddev=0.02)\n",
    "z_dim = 100\n",
    "batch_size = 64        \n",
    "epochs = 2000 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv0 (Conv2D)               (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "relu0 (Activation)           (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "Dropout0 (Dropout)           (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 7, 7, 64)          102464    \n",
      "_________________________________________________________________\n",
      "relu1 (Activation)           (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "Dropout1 (Dropout)           (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 4, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "relu2 (Activation)           (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "Dropout2 (Dropout)           (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 4, 4, 128)         409728    \n",
      "_________________________________________________________________\n",
      "relu3 (Activation)           (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "Dropout3 (Dropout)           (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 720,833\n",
      "Trainable params: 720,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Discriminator part\n",
    "\n",
    "discriminator = Sequential([\n",
    "    Conv2D(filters =64, \n",
    "           kernel_size = (5,5), \n",
    "           strides = 2,\n",
    "           padding = 'same',\n",
    "           input_shape=(28, 28, 1),\n",
    "           name = \"conv0\"),\n",
    "    Activation('relu', name = \"relu0\"),\n",
    "    Dropout(rate = 0.4, name = \"Dropout0\"),\n",
    "\n",
    "    Conv2D(filters =64, \n",
    "           kernel_size = (5,5), \n",
    "           strides = 2,\n",
    "           padding = 'same',\n",
    "           name = \"conv1\"),\n",
    "    Activation('relu', name = \"relu1\"),\n",
    "    Dropout(rate = 0.4, name = \"Dropout1\"),\n",
    "    \n",
    "    Conv2D(filters =128, \n",
    "           kernel_size = (5,5), \n",
    "           strides = 2,\n",
    "           padding = 'same',\n",
    "           name = \"conv2\"),\n",
    "    Activation('relu', name = \"relu2\"),\n",
    "    Dropout(rate = 0.4, name = \"Dropout2\"),\n",
    "\n",
    "    Conv2D(filters =128, \n",
    "           kernel_size = (5,5), \n",
    "           strides = 1,\n",
    "           padding = 'same',\n",
    "           name = \"conv3\"),\n",
    "    Activation('relu', name = \"relu3\"),\n",
    "    Dropout(rate = 0.4, name = \"Dropout3\"),    \n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(1, activation='sigmoid', kernel_initializer=weight_init),\n",
    "])\n",
    "\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense0 (Dense)               (None, 3136)              316736    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 3136)              12544     \n",
      "_________________________________________________________________\n",
      "relu0 (Activation)           (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "reshape0 (Reshape)           (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "upsample0 (UpSampling2D)     (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv0 (Conv2D)               (None, 14, 14, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "relu1 (Activation)           (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "upsample1 (UpSampling2D)     (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "relu2 (Activation)           (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2DTranspose)      (None, 28, 28, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "relu3 (Activation)           (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2DTranspose)      (None, 28, 28, 1)         1601      \n",
      "_________________________________________________________________\n",
      "tanh (Activation)            (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 844,161\n",
      "Trainable params: 837,377\n",
      "Non-trainable params: 6,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Generator part\n",
    "\n",
    "generator = Sequential([\n",
    "    Dense(np.prod((7,7,64)), \n",
    "          kernel_initializer = weight_init,\n",
    "          input_shape=(z_dim,),\n",
    "          name = \"dense0\"),\n",
    "    BatchNormalization(momentum = 0.9),\n",
    "    Activation('relu', name = \"relu0\"),\n",
    "    Reshape((7,7,64), name = \"reshape0\"),\n",
    "    UpSampling2D(name = \"upsample0\"),\n",
    "    Conv2D(filters = 128,\n",
    "          kernel_size = (5,5),\n",
    "          padding = 'same',\n",
    "          strides = 1,\n",
    "          name = \"conv0\",\n",
    "          kernel_initializer = weight_init),\n",
    "    BatchNormalization(momentum = 0.9),    \n",
    "    \n",
    "    Activation('relu', name = \"relu1\"),\n",
    "    UpSampling2D(name = \"upsample1\"),\n",
    "    Conv2D(filters = 64,\n",
    "          kernel_size = (5,5),\n",
    "          padding = 'same',\n",
    "          strides = 1,\n",
    "          name = \"conv1\",\n",
    "          kernel_initializer = weight_init),\n",
    "    BatchNormalization(momentum = 0.9),        \n",
    "    \n",
    "    Activation('relu', name = \"relu2\"),\n",
    "    Conv2DTranspose(filters = 64,\n",
    "          kernel_size = (5,5),\n",
    "          padding = 'same',\n",
    "          strides = 1,\n",
    "          name = \"conv2\",\n",
    "          kernel_initializer = weight_init),\n",
    "    BatchNormalization(momentum = 0.9),       \n",
    "    \n",
    "    Activation('relu', name = \"relu3\"),\n",
    "    Conv2DTranspose(filters = 1,\n",
    "          kernel_size = (5,5),\n",
    "          padding = 'same',\n",
    "          strides = 1,\n",
    "          name = \"conv3\",\n",
    "          kernel_initializer = weight_init),    \n",
    "    \n",
    "    Activation('tanh', name = 'tanh')\n",
    "])\n",
    "\n",
    "\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Compile\n",
    "\n",
    "def set_trainable(m, val):\n",
    "    m.trainable = val\n",
    "    for l in m.layers:\n",
    "        l.trainable = val\n",
    "\n",
    "### COMPILE DISCRIMINATOR        \n",
    "discriminator.compile(optimizer=RMSprop(lr=0.0008),\n",
    "                     loss = 'binary_crossentropy',\n",
    "                     metrics = ['accuracy'])\n",
    "\n",
    "### COMPILE THE FULL GAN\n",
    "set_trainable(discriminator, False)\n",
    "\n",
    "model_input = Input(shape=(100,), name='model_input')\n",
    "model_output = discriminator(generator(model_input))\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(optimizer=RMSprop(lr=0.0004),\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "set_trainable(discriminator, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save image\n",
    "def sample_images(epoch):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, z_dim))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    gen_imgs = 0.5 * (gen_imgs + 1)\n",
    "    gen_imgs = np.clip(gen_imgs, 0, 1)\n",
    "\n",
    "    fig, axs = plt.subplots(r, c, figsize=(15,15))\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(np.squeeze(gen_imgs[cnt, :,:,:]), cmap = 'gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"../result/camel/sample_%d.png\" % epoch)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Part\n",
    "\n",
    "def train_discriminator(x_train, batch_size):\n",
    "    valid = np.ones((batch_size,1))\n",
    "    fake = np.zeros((batch_size,1))\n",
    "\n",
    "    idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "    true_imgs = x_train[idx]\n",
    "    \n",
    "    noise = np.random.normal(0, 1, (batch_size, z_dim))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "    \n",
    "    d_loss_real, d_acc_real =   discriminator.train_on_batch(true_imgs, valid)\n",
    "    d_loss_fake, d_acc_fake =   discriminator.train_on_batch(gen_imgs, fake)\n",
    "    d_loss =  0.5 * (d_loss_real + d_loss_fake)\n",
    "    d_acc = 0.5 * (d_acc_real + d_acc_fake)\n",
    "\n",
    "    return [d_loss, d_loss_real, d_loss_fake, d_acc, d_acc_real, d_acc_fake]    \n",
    "    \n",
    "    \n",
    "def train_generator(batch_size):\n",
    "    valid = np.ones((batch_size,1))\n",
    "    noise = np.random.normal(0, 1, (batch_size, z_dim))\n",
    "    return model.train_on_batch(noise, valid)\n",
    "\n",
    "def train(x_train, batch_size, epochs):\n",
    "    for epoch in range(0, epochs):\n",
    "        d = train_discriminator(x_train, batch_size)\n",
    "        g = train_generator(batch_size)\n",
    "        \n",
    "        print (\"%d [D loss: (%.3f)(R %.3f, F %.3f)] [D acc: (%.3f)(%.3f, %.3f)] [G loss: %.3f] [G acc: %.3f]\" % (epoch, d[0], d[1], d[2], d[3], d[4], d[5], g[0], g[1]))\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            sample_images(epoch)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Data\n",
    "def load_safari(folder):\n",
    "    mypath = os.path.join(\"../data\", folder)\n",
    "    txt_name_list = []\n",
    "    for (dirpath, dirnames, filenames) in walk(mypath):\n",
    "        for f in filenames:\n",
    "            if f != '.DS_Store':\n",
    "                txt_name_list.append(f)\n",
    "                break\n",
    "\n",
    "    slice_train = int(80000/len(txt_name_list))  ###Setting value to be 80000 for the final dataset\n",
    "    i = 0\n",
    "    seed = np.random.randint(1, 10e6)\n",
    "\n",
    "    for txt_name in txt_name_list:\n",
    "        txt_path = os.path.join(mypath,txt_name)\n",
    "        x = np.load(txt_path)\n",
    "        x = (x.astype('float32') - 127.5) / 127.5\n",
    "        # x = x.astype('float32') / 255.0\n",
    "        \n",
    "        x = x.reshape(x.shape[0], 28, 28, 1)\n",
    "        \n",
    "        y = [i] * len(x)  \n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(x)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(y)\n",
    "        x = x[:slice_train]\n",
    "        y = y[:slice_train]\n",
    "        if i != 0: \n",
    "            xtotal = np.concatenate((x,xtotal), axis=0)\n",
    "            ytotal = np.concatenate((y,ytotal), axis=0)\n",
    "        else:\n",
    "            xtotal = x\n",
    "            ytotal = y\n",
    "        i += 1\n",
    "        \n",
    "    return xtotal, ytotal\n",
    "\n",
    "(x_train, y_train) = load_safari('camel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: (0.739)(R 0.676, F 0.801)] [D acc: (0.398)(0.797, 0.000)] [G loss: 0.675] [G acc: 1.000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: (0.744)(R 0.534, F 0.954)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.513] [G acc: 1.000]\n",
      "2 [D loss: (2.129)(R 0.408, F 3.849)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.582] [G acc: 1.000]\n",
      "3 [D loss: (0.637)(R 0.580, F 0.693)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.543] [G acc: 1.000]\n",
      "4 [D loss: (0.622)(R 0.548, F 0.695)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.492] [G acc: 1.000]\n",
      "5 [D loss: (0.606)(R 0.507, F 0.704)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.418] [G acc: 1.000]\n",
      "6 [D loss: (0.575)(R 0.432, F 0.717)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.288] [G acc: 1.000]\n",
      "7 [D loss: (0.567)(R 0.318, F 0.816)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.191] [G acc: 1.000]\n",
      "8 [D loss: (0.650)(R 0.232, F 1.067)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.245] [G acc: 1.000]\n",
      "9 [D loss: (0.628)(R 0.273, F 0.983)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.276] [G acc: 1.000]\n",
      "10 [D loss: (0.717)(R 0.292, F 1.142)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.425] [G acc: 1.000]\n",
      "11 [D loss: (0.686)(R 0.393, F 0.978)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.484] [G acc: 1.000]\n",
      "12 [D loss: (0.685)(R 0.424, F 0.946)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.546] [G acc: 1.000]\n",
      "13 [D loss: (0.680)(R 0.442, F 0.919)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.588] [G acc: 1.000]\n",
      "14 [D loss: (0.667)(R 0.423, F 0.910)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.624] [G acc: 1.000]\n",
      "15 [D loss: (0.615)(R 0.402, F 0.829)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.648] [G acc: 1.000]\n",
      "16 [D loss: (0.555)(R 0.309, F 0.800)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.674] [G acc: 1.000]\n",
      "17 [D loss: (0.466)(R 0.183, F 0.748)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.692] [G acc: 0.578]\n",
      "18 [D loss: (0.374)(R 0.051, F 0.697)] [D acc: (0.633)(1.000, 0.266)] [G loss: 0.715] [G acc: 0.000]\n",
      "19 [D loss: (0.343)(R 0.012, F 0.673)] [D acc: (1.000)(1.000, 1.000)] [G loss: 0.779] [G acc: 0.000]\n",
      "20 [D loss: (0.311)(R 0.005, F 0.617)] [D acc: (1.000)(1.000, 1.000)] [G loss: 1.161] [G acc: 0.000]\n",
      "21 [D loss: (0.204)(R 0.002, F 0.406)] [D acc: (1.000)(1.000, 1.000)] [G loss: 1.913] [G acc: 0.000]\n",
      "22 [D loss: (3.216)(R 0.131, F 6.302)] [D acc: (0.484)(0.969, 0.000)] [G loss: 0.606] [G acc: 0.859]\n",
      "23 [D loss: (1.317)(R 0.151, F 2.482)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.913] [G acc: 0.000]\n",
      "24 [D loss: (0.720)(R 0.561, F 0.878)] [D acc: (0.406)(0.797, 0.016)] [G loss: 0.781] [G acc: 0.109]\n",
      "25 [D loss: (0.596)(R 0.393, F 0.799)] [D acc: (0.586)(0.984, 0.188)] [G loss: 0.937] [G acc: 0.000]\n",
      "26 [D loss: (0.532)(R 0.385, F 0.679)] [D acc: (0.773)(0.969, 0.578)] [G loss: 0.829] [G acc: 0.062]\n",
      "27 [D loss: (0.388)(R 0.350, F 0.425)] [D acc: (0.906)(0.922, 0.891)] [G loss: 0.451] [G acc: 0.984]\n",
      "28 [D loss: (0.175)(R 0.237, F 0.114)] [D acc: (0.984)(0.969, 1.000)] [G loss: 0.152] [G acc: 1.000]\n",
      "29 [D loss: (0.054)(R 0.082, F 0.025)] [D acc: (1.000)(1.000, 1.000)] [G loss: 0.052] [G acc: 1.000]\n",
      "30 [D loss: (0.048)(R 0.077, F 0.018)] [D acc: (0.992)(0.984, 1.000)] [G loss: 0.024] [G acc: 1.000]\n",
      "31 [D loss: (0.071)(R 0.117, F 0.024)] [D acc: (0.992)(0.984, 1.000)] [G loss: 0.014] [G acc: 1.000]\n",
      "32 [D loss: (0.008)(R 0.013, F 0.003)] [D acc: (1.000)(1.000, 1.000)] [G loss: 0.009] [G acc: 1.000]\n",
      "33 [D loss: (0.026)(R 0.023, F 0.029)] [D acc: (0.992)(1.000, 0.984)] [G loss: 0.006] [G acc: 1.000]\n",
      "34 [D loss: (0.478)(R 0.092, F 0.863)] [D acc: (0.773)(0.953, 0.594)] [G loss: 0.070] [G acc: 1.000]\n",
      "35 [D loss: (2.483)(R 0.681, F 4.284)] [D acc: (0.344)(0.688, 0.000)] [G loss: 0.591] [G acc: 0.984]\n",
      "36 [D loss: (0.723)(R 0.567, F 0.879)] [D acc: (0.516)(0.969, 0.062)] [G loss: 0.558] [G acc: 1.000]\n",
      "37 [D loss: (0.689)(R 0.566, F 0.813)] [D acc: (0.555)(0.984, 0.125)] [G loss: 0.536] [G acc: 1.000]\n",
      "38 [D loss: (0.741)(R 0.563, F 0.919)] [D acc: (0.492)(0.984, 0.000)] [G loss: 0.565] [G acc: 1.000]\n",
      "39 [D loss: (0.771)(R 0.583, F 0.960)] [D acc: (0.484)(0.969, 0.000)] [G loss: 0.645] [G acc: 0.906]\n",
      "40 [D loss: (0.723)(R 0.632, F 0.814)] [D acc: (0.461)(0.922, 0.000)] [G loss: 0.678] [G acc: 0.594]\n",
      "41 [D loss: (0.710)(R 0.655, F 0.766)] [D acc: (0.445)(0.844, 0.047)] [G loss: 0.700] [G acc: 0.406]\n",
      "42 [D loss: (0.697)(R 0.664, F 0.729)] [D acc: (0.445)(0.797, 0.094)] [G loss: 0.703] [G acc: 0.391]\n",
      "43 [D loss: (0.698)(R 0.671, F 0.725)] [D acc: (0.453)(0.797, 0.109)] [G loss: 0.705] [G acc: 0.422]\n",
      "44 [D loss: (0.699)(R 0.672, F 0.725)] [D acc: (0.469)(0.703, 0.234)] [G loss: 0.712] [G acc: 0.234]\n",
      "45 [D loss: (0.696)(R 0.676, F 0.717)] [D acc: (0.500)(0.734, 0.266)] [G loss: 0.711] [G acc: 0.312]\n",
      "46 [D loss: (0.705)(R 0.690, F 0.720)] [D acc: (0.469)(0.688, 0.250)] [G loss: 0.706] [G acc: 0.219]\n",
      "47 [D loss: (0.691)(R 0.665, F 0.717)] [D acc: (0.508)(0.812, 0.203)] [G loss: 0.703] [G acc: 0.297]\n",
      "48 [D loss: (0.700)(R 0.656, F 0.744)] [D acc: (0.508)(0.859, 0.156)] [G loss: 0.697] [G acc: 0.453]\n",
      "49 [D loss: (0.706)(R 0.671, F 0.741)] [D acc: (0.461)(0.766, 0.156)] [G loss: 0.701] [G acc: 0.359]\n",
      "50 [D loss: (0.700)(R 0.662, F 0.737)] [D acc: (0.422)(0.781, 0.062)] [G loss: 0.697] [G acc: 0.375]\n",
      "51 [D loss: (0.707)(R 0.667, F 0.746)] [D acc: (0.367)(0.672, 0.062)] [G loss: 0.700] [G acc: 0.344]\n",
      "52 [D loss: (0.691)(R 0.680, F 0.702)] [D acc: (0.477)(0.625, 0.328)] [G loss: 0.699] [G acc: 0.344]\n",
      "53 [D loss: (0.693)(R 0.675, F 0.711)] [D acc: (0.469)(0.578, 0.359)] [G loss: 0.699] [G acc: 0.375]\n",
      "54 [D loss: (0.692)(R 0.668, F 0.716)] [D acc: (0.469)(0.688, 0.250)] [G loss: 0.700] [G acc: 0.312]\n",
      "55 [D loss: (0.726)(R 0.656, F 0.797)] [D acc: (0.469)(0.797, 0.141)] [G loss: 0.702] [G acc: 0.250]\n",
      "56 [D loss: (0.689)(R 0.685, F 0.693)] [D acc: (0.508)(0.609, 0.406)] [G loss: 0.699] [G acc: 0.328]\n",
      "57 [D loss: (0.698)(R 0.683, F 0.713)] [D acc: (0.469)(0.641, 0.297)] [G loss: 0.699] [G acc: 0.312]\n",
      "58 [D loss: (0.688)(R 0.681, F 0.695)] [D acc: (0.609)(0.781, 0.438)] [G loss: 0.700] [G acc: 0.391]\n",
      "59 [D loss: (0.702)(R 0.684, F 0.720)] [D acc: (0.398)(0.672, 0.125)] [G loss: 0.698] [G acc: 0.344]\n",
      "60 [D loss: (0.692)(R 0.683, F 0.701)] [D acc: (0.562)(0.766, 0.359)] [G loss: 0.698] [G acc: 0.359]\n",
      "61 [D loss: (0.698)(R 0.680, F 0.717)] [D acc: (0.492)(0.781, 0.203)] [G loss: 0.695] [G acc: 0.438]\n",
      "62 [D loss: (0.696)(R 0.682, F 0.709)] [D acc: (0.508)(0.750, 0.266)] [G loss: 0.694] [G acc: 0.359]\n",
      "63 [D loss: (0.695)(R 0.673, F 0.717)] [D acc: (0.516)(0.844, 0.188)] [G loss: 0.694] [G acc: 0.453]\n",
      "64 [D loss: (0.696)(R 0.676, F 0.716)] [D acc: (0.438)(0.766, 0.109)] [G loss: 0.698] [G acc: 0.250]\n",
      "65 [D loss: (0.695)(R 0.673, F 0.716)] [D acc: (0.523)(0.891, 0.156)] [G loss: 0.700] [G acc: 0.297]\n",
      "66 [D loss: (0.694)(R 0.678, F 0.710)] [D acc: (0.500)(0.797, 0.203)] [G loss: 0.701] [G acc: 0.141]\n",
      "67 [D loss: (0.696)(R 0.688, F 0.703)] [D acc: (0.484)(0.703, 0.266)] [G loss: 0.702] [G acc: 0.109]\n",
      "68 [D loss: (0.694)(R 0.683, F 0.706)] [D acc: (0.445)(0.734, 0.156)] [G loss: 0.702] [G acc: 0.062]\n",
      "69 [D loss: (0.692)(R 0.686, F 0.698)] [D acc: (0.484)(0.578, 0.391)] [G loss: 0.701] [G acc: 0.109]\n",
      "70 [D loss: (0.693)(R 0.682, F 0.705)] [D acc: (0.555)(0.828, 0.281)] [G loss: 0.703] [G acc: 0.094]\n",
      "71 [D loss: (0.690)(R 0.681, F 0.699)] [D acc: (0.562)(0.859, 0.266)] [G loss: 0.704] [G acc: 0.125]\n",
      "72 [D loss: (0.701)(R 0.676, F 0.727)] [D acc: (0.422)(0.766, 0.078)] [G loss: 0.702] [G acc: 0.078]\n",
      "73 [D loss: (0.690)(R 0.685, F 0.694)] [D acc: (0.641)(0.734, 0.547)] [G loss: 0.702] [G acc: 0.062]\n",
      "74 [D loss: (0.694)(R 0.685, F 0.702)] [D acc: (0.500)(0.719, 0.281)] [G loss: 0.700] [G acc: 0.172]\n",
      "75 [D loss: (0.693)(R 0.683, F 0.703)] [D acc: (0.500)(0.797, 0.203)] [G loss: 0.703] [G acc: 0.047]\n",
      "76 [D loss: (0.694)(R 0.687, F 0.700)] [D acc: (0.555)(0.703, 0.406)] [G loss: 0.701] [G acc: 0.156]\n",
      "77 [D loss: (0.692)(R 0.685, F 0.698)] [D acc: (0.531)(0.672, 0.391)] [G loss: 0.699] [G acc: 0.203]\n",
      "78 [D loss: (0.691)(R 0.683, F 0.699)] [D acc: (0.586)(0.797, 0.375)] [G loss: 0.704] [G acc: 0.062]\n",
      "79 [D loss: (0.691)(R 0.678, F 0.703)] [D acc: (0.539)(0.828, 0.250)] [G loss: 0.706] [G acc: 0.141]\n",
      "80 [D loss: (0.690)(R 0.682, F 0.699)] [D acc: (0.578)(0.688, 0.469)] [G loss: 0.697] [G acc: 0.234]\n",
      "81 [D loss: (0.692)(R 0.670, F 0.713)] [D acc: (0.562)(0.906, 0.219)] [G loss: 0.711] [G acc: 0.109]\n",
      "82 [D loss: (0.693)(R 0.698, F 0.688)] [D acc: (0.625)(0.641, 0.609)] [G loss: 0.752] [G acc: 0.000]\n",
      "83 [D loss: (0.734)(R 0.660, F 0.807)] [D acc: (0.406)(0.812, 0.000)] [G loss: 0.712] [G acc: 0.016]\n",
      "84 [D loss: (0.686)(R 0.683, F 0.690)] [D acc: (0.656)(0.688, 0.625)] [G loss: 0.711] [G acc: 0.078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 [D loss: (0.684)(R 0.669, F 0.699)] [D acc: (0.633)(0.875, 0.391)] [G loss: 0.702] [G acc: 0.250]\n",
      "86 [D loss: (0.694)(R 0.640, F 0.749)] [D acc: (0.539)(0.938, 0.141)] [G loss: 0.696] [G acc: 0.422]\n",
      "87 [D loss: (0.700)(R 0.645, F 0.755)] [D acc: (0.500)(0.906, 0.094)] [G loss: 0.708] [G acc: 0.094]\n",
      "88 [D loss: (0.685)(R 0.656, F 0.714)] [D acc: (0.500)(0.797, 0.203)] [G loss: 0.702] [G acc: 0.125]\n",
      "89 [D loss: (0.694)(R 0.657, F 0.731)] [D acc: (0.461)(0.844, 0.078)] [G loss: 0.707] [G acc: 0.109]\n",
      "90 [D loss: (0.682)(R 0.666, F 0.697)] [D acc: (0.656)(0.812, 0.500)] [G loss: 0.715] [G acc: 0.016]\n",
      "91 [D loss: (0.698)(R 0.641, F 0.754)] [D acc: (0.484)(0.891, 0.078)] [G loss: 0.705] [G acc: 0.062]\n",
      "92 [D loss: (0.682)(R 0.661, F 0.702)] [D acc: (0.609)(0.797, 0.422)] [G loss: 0.704] [G acc: 0.172]\n",
      "93 [D loss: (0.689)(R 0.641, F 0.737)] [D acc: (0.516)(0.906, 0.125)] [G loss: 0.710] [G acc: 0.047]\n",
      "94 [D loss: (0.681)(R 0.659, F 0.704)] [D acc: (0.602)(0.891, 0.312)] [G loss: 0.707] [G acc: 0.078]\n",
      "95 [D loss: (0.680)(R 0.642, F 0.719)] [D acc: (0.570)(0.891, 0.250)] [G loss: 0.705] [G acc: 0.141]\n",
      "96 [D loss: (0.688)(R 0.644, F 0.732)] [D acc: (0.531)(0.875, 0.188)] [G loss: 0.708] [G acc: 0.078]\n",
      "97 [D loss: (0.676)(R 0.634, F 0.719)] [D acc: (0.570)(0.906, 0.234)] [G loss: 0.706] [G acc: 0.234]\n",
      "98 [D loss: (0.685)(R 0.621, F 0.749)] [D acc: (0.547)(0.953, 0.141)] [G loss: 0.705] [G acc: 0.156]\n",
      "99 [D loss: (0.675)(R 0.625, F 0.724)] [D acc: (0.570)(0.953, 0.188)] [G loss: 0.708] [G acc: 0.156]\n",
      "100 [D loss: (0.675)(R 0.619, F 0.731)] [D acc: (0.641)(0.906, 0.375)] [G loss: 1.353] [G acc: 0.000]\n",
      "101 [D loss: (0.823)(R 0.860, F 0.785)] [D acc: (0.227)(0.359, 0.094)] [G loss: 0.789] [G acc: 0.000]\n",
      "102 [D loss: (0.611)(R 0.605, F 0.617)] [D acc: (0.945)(0.891, 1.000)] [G loss: 0.961] [G acc: 0.016]\n",
      "103 [D loss: (0.719)(R 0.432, F 1.006)] [D acc: (0.523)(0.938, 0.109)] [G loss: 0.897] [G acc: 0.000]\n",
      "104 [D loss: (0.613)(R 0.585, F 0.641)] [D acc: (0.820)(0.797, 0.844)] [G loss: 0.901] [G acc: 0.031]\n",
      "105 [D loss: (0.703)(R 0.494, F 0.912)] [D acc: (0.508)(0.891, 0.125)] [G loss: 0.758] [G acc: 0.141]\n",
      "106 [D loss: (0.677)(R 0.576, F 0.778)] [D acc: (0.492)(0.828, 0.156)] [G loss: 0.720] [G acc: 0.172]\n",
      "107 [D loss: (0.694)(R 0.573, F 0.816)] [D acc: (0.477)(0.875, 0.078)] [G loss: 0.702] [G acc: 0.406]\n",
      "108 [D loss: (0.665)(R 0.550, F 0.780)] [D acc: (0.500)(0.906, 0.094)] [G loss: 0.713] [G acc: 0.188]\n",
      "109 [D loss: (0.653)(R 0.551, F 0.756)] [D acc: (0.570)(0.922, 0.219)] [G loss: 0.736] [G acc: 0.125]\n",
      "110 [D loss: (0.651)(R 0.536, F 0.767)] [D acc: (0.578)(0.906, 0.250)] [G loss: 0.743] [G acc: 0.078]\n",
      "111 [D loss: (0.672)(R 0.541, F 0.804)] [D acc: (0.570)(0.859, 0.281)] [G loss: 0.739] [G acc: 0.109]\n",
      "112 [D loss: (0.674)(R 0.569, F 0.779)] [D acc: (0.555)(0.844, 0.266)] [G loss: 0.736] [G acc: 0.094]\n",
      "113 [D loss: (0.674)(R 0.576, F 0.772)] [D acc: (0.523)(0.812, 0.234)] [G loss: 0.714] [G acc: 0.172]\n",
      "114 [D loss: (0.662)(R 0.513, F 0.812)] [D acc: (0.539)(0.938, 0.141)] [G loss: 0.741] [G acc: 0.047]\n",
      "115 [D loss: (0.642)(R 0.568, F 0.716)] [D acc: (0.609)(0.828, 0.391)] [G loss: 0.722] [G acc: 0.203]\n",
      "116 [D loss: (0.678)(R 0.469, F 0.887)] [D acc: (0.570)(0.938, 0.203)] [G loss: 0.758] [G acc: 0.016]\n",
      "117 [D loss: (0.634)(R 0.564, F 0.703)] [D acc: (0.648)(0.781, 0.516)] [G loss: 0.759] [G acc: 0.016]\n",
      "118 [D loss: (0.632)(R 0.486, F 0.778)] [D acc: (0.562)(0.875, 0.250)] [G loss: 0.763] [G acc: 0.016]\n",
      "119 [D loss: (0.639)(R 0.526, F 0.752)] [D acc: (0.625)(0.891, 0.359)] [G loss: 0.779] [G acc: 0.016]\n",
      "120 [D loss: (0.617)(R 0.506, F 0.729)] [D acc: (0.711)(0.891, 0.531)] [G loss: 0.768] [G acc: 0.047]\n",
      "121 [D loss: (0.622)(R 0.445, F 0.798)] [D acc: (0.586)(0.938, 0.234)] [G loss: 0.790] [G acc: 0.016]\n",
      "122 [D loss: (0.609)(R 0.411, F 0.806)] [D acc: (0.633)(0.938, 0.328)] [G loss: 0.797] [G acc: 0.000]\n",
      "123 [D loss: (0.587)(R 0.473, F 0.701)] [D acc: (0.680)(0.750, 0.609)] [G loss: 0.783] [G acc: 0.031]\n",
      "124 [D loss: (0.689)(R 0.312, F 1.066)] [D acc: (0.516)(0.922, 0.109)] [G loss: 0.865] [G acc: 0.000]\n",
      "125 [D loss: (0.645)(R 0.658, F 0.633)] [D acc: (0.766)(0.562, 0.969)] [G loss: 1.037] [G acc: 0.000]\n",
      "126 [D loss: (0.625)(R 0.474, F 0.776)] [D acc: (0.625)(0.797, 0.453)] [G loss: 1.022] [G acc: 0.000]\n",
      "127 [D loss: (0.597)(R 0.531, F 0.663)] [D acc: (0.789)(0.859, 0.719)] [G loss: 0.901] [G acc: 0.016]\n",
      "128 [D loss: (0.636)(R 0.388, F 0.883)] [D acc: (0.547)(0.859, 0.234)] [G loss: 0.870] [G acc: 0.016]\n",
      "129 [D loss: (0.579)(R 0.461, F 0.696)] [D acc: (0.695)(0.797, 0.594)] [G loss: 0.802] [G acc: 0.109]\n",
      "130 [D loss: (0.715)(R 0.352, F 1.079)] [D acc: (0.477)(0.844, 0.109)] [G loss: 0.862] [G acc: 0.000]\n",
      "131 [D loss: (0.663)(R 0.665, F 0.661)] [D acc: (0.672)(0.516, 0.828)] [G loss: 0.749] [G acc: 0.141]\n",
      "132 [D loss: (0.813)(R 0.551, F 1.075)] [D acc: (0.422)(0.766, 0.078)] [G loss: 0.761] [G acc: 0.031]\n",
      "133 [D loss: (0.651)(R 0.625, F 0.676)] [D acc: (0.758)(0.719, 0.797)] [G loss: 0.747] [G acc: 0.078]\n",
      "134 [D loss: (0.680)(R 0.590, F 0.770)] [D acc: (0.477)(0.703, 0.250)] [G loss: 0.738] [G acc: 0.188]\n",
      "135 [D loss: (0.676)(R 0.596, F 0.756)] [D acc: (0.547)(0.734, 0.359)] [G loss: 0.735] [G acc: 0.172]\n",
      "136 [D loss: (0.666)(R 0.518, F 0.815)] [D acc: (0.555)(0.828, 0.281)] [G loss: 0.741] [G acc: 0.203]\n",
      "137 [D loss: (0.599)(R 0.561, F 0.637)] [D acc: (0.766)(0.812, 0.719)] [G loss: 0.786] [G acc: 0.219]\n",
      "138 [D loss: (1.959)(R 0.493, F 3.425)] [D acc: (0.414)(0.828, 0.000)] [G loss: 1.061] [G acc: 0.016]\n",
      "139 [D loss: (0.625)(R 0.635, F 0.616)] [D acc: (0.766)(0.703, 0.828)] [G loss: 0.851] [G acc: 0.062]\n",
      "140 [D loss: (0.652)(R 0.570, F 0.735)] [D acc: (0.633)(0.766, 0.500)] [G loss: 0.840] [G acc: 0.266]\n",
      "141 [D loss: (0.617)(R 0.599, F 0.634)] [D acc: (0.719)(0.719, 0.719)] [G loss: 1.009] [G acc: 0.031]\n",
      "142 [D loss: (0.472)(R 0.532, F 0.412)] [D acc: (0.820)(0.781, 0.859)] [G loss: 0.728] [G acc: 0.516]\n",
      "143 [D loss: (0.747)(R 0.726, F 0.769)] [D acc: (0.562)(0.719, 0.406)] [G loss: 1.216] [G acc: 0.016]\n",
      "144 [D loss: (0.686)(R 0.590, F 0.781)] [D acc: (0.602)(0.766, 0.438)] [G loss: 0.748] [G acc: 0.219]\n",
      "145 [D loss: (0.367)(R 0.518, F 0.217)] [D acc: (0.898)(0.828, 0.969)] [G loss: 0.666] [G acc: 0.641]\n",
      "146 [D loss: (0.593)(R 0.512, F 0.674)] [D acc: (0.734)(0.859, 0.609)] [G loss: 1.108] [G acc: 0.031]\n",
      "147 [D loss: (0.742)(R 0.564, F 0.920)] [D acc: (0.523)(0.750, 0.297)] [G loss: 0.825] [G acc: 0.422]\n",
      "148 [D loss: (0.727)(R 0.613, F 0.840)] [D acc: (0.469)(0.734, 0.203)] [G loss: 0.934] [G acc: 0.016]\n",
      "149 [D loss: (0.623)(R 0.600, F 0.647)] [D acc: (0.680)(0.781, 0.578)] [G loss: 0.735] [G acc: 0.297]\n",
      "150 [D loss: (0.685)(R 0.621, F 0.748)] [D acc: (0.570)(0.719, 0.422)] [G loss: 0.718] [G acc: 0.547]\n",
      "151 [D loss: (0.641)(R 0.690, F 0.591)] [D acc: (0.656)(0.672, 0.641)] [G loss: 0.670] [G acc: 0.500]\n",
      "152 [D loss: (0.844)(R 0.829, F 0.859)] [D acc: (0.359)(0.453, 0.266)] [G loss: 0.619] [G acc: 0.766]\n",
      "153 [D loss: (0.663)(R 0.569, F 0.757)] [D acc: (0.633)(0.844, 0.422)] [G loss: 0.592] [G acc: 0.828]\n",
      "154 [D loss: (0.786)(R 0.606, F 0.967)] [D acc: (0.398)(0.766, 0.031)] [G loss: 0.711] [G acc: 0.375]\n",
      "155 [D loss: (0.711)(R 0.641, F 0.782)] [D acc: (0.469)(0.734, 0.203)] [G loss: 0.681] [G acc: 0.484]\n",
      "156 [D loss: (0.699)(R 0.650, F 0.748)] [D acc: (0.453)(0.703, 0.203)] [G loss: 0.699] [G acc: 0.406]\n",
      "157 [D loss: (0.675)(R 0.631, F 0.719)] [D acc: (0.625)(0.828, 0.422)] [G loss: 0.698] [G acc: 0.422]\n",
      "158 [D loss: (0.687)(R 0.608, F 0.767)] [D acc: (0.477)(0.750, 0.203)] [G loss: 0.698] [G acc: 0.422]\n",
      "159 [D loss: (0.684)(R 0.586, F 0.782)] [D acc: (0.516)(0.859, 0.172)] [G loss: 0.693] [G acc: 0.391]\n",
      "160 [D loss: (0.698)(R 0.594, F 0.802)] [D acc: (0.469)(0.828, 0.109)] [G loss: 0.694] [G acc: 0.406]\n",
      "161 [D loss: (0.702)(R 0.617, F 0.788)] [D acc: (0.445)(0.766, 0.125)] [G loss: 0.697] [G acc: 0.375]\n",
      "162 [D loss: (0.686)(R 0.619, F 0.752)] [D acc: (0.531)(0.875, 0.188)] [G loss: 0.697] [G acc: 0.406]\n",
      "163 [D loss: (0.682)(R 0.624, F 0.740)] [D acc: (0.484)(0.781, 0.188)] [G loss: 0.699] [G acc: 0.391]\n",
      "164 [D loss: (0.679)(R 0.599, F 0.759)] [D acc: (0.539)(0.875, 0.203)] [G loss: 0.701] [G acc: 0.359]\n",
      "165 [D loss: (0.683)(R 0.610, F 0.756)] [D acc: (0.508)(0.812, 0.203)] [G loss: 0.698] [G acc: 0.391]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166 [D loss: (0.690)(R 0.606, F 0.773)] [D acc: (0.492)(0.766, 0.219)] [G loss: 0.690] [G acc: 0.438]\n",
      "167 [D loss: (0.686)(R 0.614, F 0.757)] [D acc: (0.523)(0.828, 0.219)] [G loss: 0.688] [G acc: 0.438]\n",
      "168 [D loss: (0.694)(R 0.626, F 0.762)] [D acc: (0.453)(0.734, 0.172)] [G loss: 0.669] [G acc: 0.641]\n",
      "169 [D loss: (0.690)(R 0.623, F 0.758)] [D acc: (0.562)(0.875, 0.250)] [G loss: 0.640] [G acc: 0.719]\n",
      "170 [D loss: (0.733)(R 0.637, F 0.829)] [D acc: (0.414)(0.703, 0.125)] [G loss: 0.685] [G acc: 0.531]\n",
      "171 [D loss: (0.694)(R 0.644, F 0.743)] [D acc: (0.500)(0.719, 0.281)] [G loss: 0.695] [G acc: 0.484]\n",
      "172 [D loss: (0.717)(R 0.678, F 0.757)] [D acc: (0.406)(0.516, 0.297)] [G loss: 0.725] [G acc: 0.281]\n",
      "173 [D loss: (0.686)(R 0.673, F 0.699)] [D acc: (0.562)(0.547, 0.578)] [G loss: 0.719] [G acc: 0.188]\n",
      "174 [D loss: (0.694)(R 0.676, F 0.712)] [D acc: (0.492)(0.547, 0.438)] [G loss: 0.722] [G acc: 0.219]\n",
      "175 [D loss: (0.697)(R 0.701, F 0.693)] [D acc: (0.484)(0.375, 0.594)] [G loss: 0.722] [G acc: 0.188]\n",
      "176 [D loss: (0.693)(R 0.693, F 0.692)] [D acc: (0.531)(0.453, 0.609)] [G loss: 0.703] [G acc: 0.297]\n",
      "177 [D loss: (0.693)(R 0.676, F 0.711)] [D acc: (0.531)(0.594, 0.469)] [G loss: 0.711] [G acc: 0.219]\n",
      "178 [D loss: (0.697)(R 0.700, F 0.694)] [D acc: (0.438)(0.312, 0.562)] [G loss: 0.712] [G acc: 0.156]\n",
      "179 [D loss: (0.696)(R 0.696, F 0.696)] [D acc: (0.492)(0.438, 0.547)] [G loss: 0.705] [G acc: 0.234]\n",
      "180 [D loss: (0.691)(R 0.683, F 0.699)] [D acc: (0.531)(0.484, 0.578)] [G loss: 0.718] [G acc: 0.078]\n",
      "181 [D loss: (0.695)(R 0.694, F 0.696)] [D acc: (0.508)(0.406, 0.609)] [G loss: 0.714] [G acc: 0.156]\n",
      "182 [D loss: (0.691)(R 0.680, F 0.703)] [D acc: (0.516)(0.484, 0.547)] [G loss: 0.709] [G acc: 0.188]\n",
      "183 [D loss: (0.688)(R 0.684, F 0.692)] [D acc: (0.500)(0.359, 0.641)] [G loss: 0.714] [G acc: 0.094]\n",
      "184 [D loss: (0.687)(R 0.687, F 0.687)] [D acc: (0.570)(0.453, 0.688)] [G loss: 0.711] [G acc: 0.203]\n",
      "185 [D loss: (0.694)(R 0.685, F 0.704)] [D acc: (0.500)(0.516, 0.484)] [G loss: 0.713] [G acc: 0.109]\n",
      "186 [D loss: (0.688)(R 0.673, F 0.703)] [D acc: (0.562)(0.594, 0.531)] [G loss: 0.719] [G acc: 0.062]\n",
      "187 [D loss: (0.694)(R 0.692, F 0.696)] [D acc: (0.477)(0.422, 0.531)] [G loss: 0.712] [G acc: 0.156]\n",
      "188 [D loss: (0.680)(R 0.675, F 0.686)] [D acc: (0.680)(0.609, 0.750)] [G loss: 0.720] [G acc: 0.078]\n",
      "189 [D loss: (0.696)(R 0.669, F 0.722)] [D acc: (0.422)(0.578, 0.266)] [G loss: 0.716] [G acc: 0.125]\n",
      "190 [D loss: (0.691)(R 0.679, F 0.703)] [D acc: (0.539)(0.578, 0.500)] [G loss: 0.711] [G acc: 0.188]\n",
      "191 [D loss: (0.689)(R 0.664, F 0.713)] [D acc: (0.539)(0.594, 0.484)] [G loss: 0.715] [G acc: 0.328]\n",
      "192 [D loss: (0.703)(R 0.685, F 0.722)] [D acc: (0.445)(0.516, 0.375)] [G loss: 0.701] [G acc: 0.297]\n",
      "193 [D loss: (0.698)(R 0.674, F 0.722)] [D acc: (0.492)(0.625, 0.359)] [G loss: 0.701] [G acc: 0.312]\n",
      "194 [D loss: (0.696)(R 0.685, F 0.708)] [D acc: (0.484)(0.500, 0.469)] [G loss: 0.704] [G acc: 0.141]\n",
      "195 [D loss: (0.696)(R 0.686, F 0.706)] [D acc: (0.484)(0.484, 0.484)] [G loss: 0.706] [G acc: 0.219]\n",
      "196 [D loss: (0.696)(R 0.682, F 0.710)] [D acc: (0.438)(0.438, 0.438)] [G loss: 0.709] [G acc: 0.109]\n",
      "197 [D loss: (0.695)(R 0.698, F 0.692)] [D acc: (0.492)(0.297, 0.688)] [G loss: 0.708] [G acc: 0.109]\n",
      "198 [D loss: (0.688)(R 0.692, F 0.684)] [D acc: (0.602)(0.406, 0.797)] [G loss: 0.708] [G acc: 0.156]\n",
      "199 [D loss: (0.687)(R 0.684, F 0.691)] [D acc: (0.562)(0.484, 0.641)] [G loss: 0.704] [G acc: 0.281]\n",
      "200 [D loss: (0.696)(R 0.682, F 0.710)] [D acc: (0.547)(0.547, 0.547)] [G loss: 0.711] [G acc: 0.109]\n",
      "201 [D loss: (0.692)(R 0.691, F 0.693)] [D acc: (0.531)(0.391, 0.672)] [G loss: 0.708] [G acc: 0.141]\n",
      "202 [D loss: (0.695)(R 0.672, F 0.718)] [D acc: (0.555)(0.609, 0.500)] [G loss: 0.708] [G acc: 0.156]\n",
      "203 [D loss: (0.697)(R 0.676, F 0.718)] [D acc: (0.500)(0.625, 0.375)] [G loss: 0.712] [G acc: 0.078]\n",
      "204 [D loss: (0.688)(R 0.689, F 0.686)] [D acc: (0.562)(0.391, 0.734)] [G loss: 0.709] [G acc: 0.125]\n",
      "205 [D loss: (0.692)(R 0.672, F 0.711)] [D acc: (0.547)(0.656, 0.438)] [G loss: 0.708] [G acc: 0.188]\n",
      "206 [D loss: (0.686)(R 0.671, F 0.701)] [D acc: (0.516)(0.516, 0.516)] [G loss: 0.711] [G acc: 0.172]\n",
      "207 [D loss: (0.689)(R 0.669, F 0.708)] [D acc: (0.570)(0.625, 0.516)] [G loss: 0.710] [G acc: 0.188]\n",
      "208 [D loss: (0.695)(R 0.662, F 0.728)] [D acc: (0.508)(0.719, 0.297)] [G loss: 0.708] [G acc: 0.219]\n",
      "209 [D loss: (0.686)(R 0.675, F 0.697)] [D acc: (0.539)(0.578, 0.500)] [G loss: 0.698] [G acc: 0.375]\n",
      "210 [D loss: (0.711)(R 0.665, F 0.757)] [D acc: (0.430)(0.609, 0.250)] [G loss: 0.708] [G acc: 0.141]\n",
      "211 [D loss: (0.692)(R 0.685, F 0.698)] [D acc: (0.555)(0.500, 0.609)] [G loss: 0.707] [G acc: 0.203]\n",
      "212 [D loss: (0.692)(R 0.679, F 0.705)] [D acc: (0.469)(0.547, 0.391)] [G loss: 0.707] [G acc: 0.219]\n",
      "213 [D loss: (0.691)(R 0.665, F 0.718)] [D acc: (0.539)(0.656, 0.422)] [G loss: 0.709] [G acc: 0.109]\n",
      "214 [D loss: (0.682)(R 0.671, F 0.694)] [D acc: (0.578)(0.625, 0.531)] [G loss: 0.715] [G acc: 0.094]\n",
      "215 [D loss: (0.694)(R 0.654, F 0.733)] [D acc: (0.547)(0.703, 0.391)] [G loss: 0.710] [G acc: 0.156]\n",
      "216 [D loss: (0.686)(R 0.665, F 0.707)] [D acc: (0.578)(0.609, 0.547)] [G loss: 0.708] [G acc: 0.203]\n",
      "217 [D loss: (0.688)(R 0.668, F 0.708)] [D acc: (0.539)(0.672, 0.406)] [G loss: 0.709] [G acc: 0.094]\n",
      "218 [D loss: (0.709)(R 0.645, F 0.773)] [D acc: (0.477)(0.688, 0.266)] [G loss: 0.701] [G acc: 0.297]\n",
      "219 [D loss: (0.686)(R 0.662, F 0.710)] [D acc: (0.531)(0.641, 0.422)] [G loss: 0.706] [G acc: 0.203]\n",
      "220 [D loss: (0.697)(R 0.672, F 0.721)] [D acc: (0.477)(0.516, 0.438)] [G loss: 0.713] [G acc: 0.125]\n",
      "221 [D loss: (0.690)(R 0.679, F 0.701)] [D acc: (0.539)(0.531, 0.547)] [G loss: 0.707] [G acc: 0.203]\n",
      "222 [D loss: (0.686)(R 0.672, F 0.700)] [D acc: (0.586)(0.609, 0.562)] [G loss: 0.706] [G acc: 0.156]\n",
      "223 [D loss: (0.688)(R 0.666, F 0.710)] [D acc: (0.594)(0.656, 0.531)] [G loss: 0.702] [G acc: 0.250]\n",
      "224 [D loss: (0.704)(R 0.644, F 0.764)] [D acc: (0.492)(0.703, 0.281)] [G loss: 0.721] [G acc: 0.078]\n",
      "225 [D loss: (0.687)(R 0.678, F 0.696)] [D acc: (0.562)(0.531, 0.594)] [G loss: 0.711] [G acc: 0.156]\n",
      "226 [D loss: (0.688)(R 0.661, F 0.714)] [D acc: (0.539)(0.641, 0.438)] [G loss: 0.709] [G acc: 0.234]\n",
      "227 [D loss: (0.689)(R 0.654, F 0.724)] [D acc: (0.531)(0.656, 0.406)] [G loss: 0.714] [G acc: 0.172]\n",
      "228 [D loss: (0.685)(R 0.669, F 0.702)] [D acc: (0.562)(0.578, 0.547)] [G loss: 0.708] [G acc: 0.234]\n",
      "229 [D loss: (0.683)(R 0.661, F 0.706)] [D acc: (0.500)(0.562, 0.438)] [G loss: 0.709] [G acc: 0.188]\n",
      "230 [D loss: (0.692)(R 0.642, F 0.742)] [D acc: (0.531)(0.781, 0.281)] [G loss: 0.713] [G acc: 0.156]\n",
      "231 [D loss: (0.690)(R 0.640, F 0.741)] [D acc: (0.500)(0.656, 0.344)] [G loss: 0.722] [G acc: 0.250]\n",
      "232 [D loss: (0.692)(R 0.640, F 0.744)] [D acc: (0.547)(0.766, 0.328)] [G loss: 0.714] [G acc: 0.156]\n",
      "233 [D loss: (0.683)(R 0.657, F 0.710)] [D acc: (0.586)(0.562, 0.609)] [G loss: 0.714] [G acc: 0.266]\n",
      "234 [D loss: (0.684)(R 0.637, F 0.731)] [D acc: (0.586)(0.797, 0.375)] [G loss: 0.717] [G acc: 0.234]\n",
      "235 [D loss: (0.671)(R 0.645, F 0.696)] [D acc: (0.656)(0.688, 0.625)] [G loss: 0.744] [G acc: 0.203]\n",
      "236 [D loss: (0.688)(R 0.672, F 0.705)] [D acc: (0.508)(0.578, 0.438)] [G loss: 0.740] [G acc: 0.172]\n",
      "237 [D loss: (0.708)(R 0.649, F 0.768)] [D acc: (0.469)(0.703, 0.234)] [G loss: 0.701] [G acc: 0.281]\n",
      "238 [D loss: (0.694)(R 0.641, F 0.747)] [D acc: (0.523)(0.766, 0.281)] [G loss: 0.688] [G acc: 0.453]\n",
      "239 [D loss: (0.691)(R 0.643, F 0.739)] [D acc: (0.523)(0.703, 0.344)] [G loss: 0.700] [G acc: 0.312]\n",
      "240 [D loss: (0.677)(R 0.648, F 0.707)] [D acc: (0.594)(0.609, 0.578)] [G loss: 0.704] [G acc: 0.328]\n",
      "241 [D loss: (0.696)(R 0.617, F 0.775)] [D acc: (0.500)(0.750, 0.250)] [G loss: 0.706] [G acc: 0.328]\n",
      "242 [D loss: (0.679)(R 0.636, F 0.723)] [D acc: (0.625)(0.703, 0.547)] [G loss: 0.707] [G acc: 0.250]\n",
      "243 [D loss: (0.686)(R 0.636, F 0.737)] [D acc: (0.539)(0.688, 0.391)] [G loss: 0.700] [G acc: 0.344]\n",
      "244 [D loss: (0.683)(R 0.634, F 0.732)] [D acc: (0.578)(0.719, 0.438)] [G loss: 0.695] [G acc: 0.344]\n",
      "245 [D loss: (0.727)(R 0.630, F 0.823)] [D acc: (0.461)(0.703, 0.219)] [G loss: 0.722] [G acc: 0.156]\n",
      "246 [D loss: (0.686)(R 0.673, F 0.698)] [D acc: (0.602)(0.594, 0.609)] [G loss: 0.712] [G acc: 0.188]\n",
      "247 [D loss: (0.688)(R 0.652, F 0.723)] [D acc: (0.508)(0.609, 0.406)] [G loss: 0.704] [G acc: 0.312]\n",
      "248 [D loss: (0.688)(R 0.652, F 0.723)] [D acc: (0.555)(0.625, 0.484)] [G loss: 0.706] [G acc: 0.344]\n",
      "249 [D loss: (0.682)(R 0.645, F 0.718)] [D acc: (0.648)(0.766, 0.531)] [G loss: 0.702] [G acc: 0.328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 [D loss: (0.676)(R 0.642, F 0.710)] [D acc: (0.602)(0.719, 0.484)] [G loss: 0.719] [G acc: 0.188]\n",
      "251 [D loss: (0.685)(R 0.640, F 0.730)] [D acc: (0.578)(0.750, 0.406)] [G loss: 0.723] [G acc: 0.188]\n",
      "252 [D loss: (0.686)(R 0.635, F 0.737)] [D acc: (0.531)(0.734, 0.328)] [G loss: 0.703] [G acc: 0.281]\n",
      "253 [D loss: (0.684)(R 0.634, F 0.734)] [D acc: (0.586)(0.781, 0.391)] [G loss: 0.703] [G acc: 0.297]\n",
      "254 [D loss: (0.688)(R 0.635, F 0.740)] [D acc: (0.562)(0.750, 0.375)] [G loss: 0.706] [G acc: 0.359]\n",
      "255 [D loss: (0.688)(R 0.640, F 0.737)] [D acc: (0.516)(0.656, 0.375)] [G loss: 0.708] [G acc: 0.328]\n",
      "256 [D loss: (0.684)(R 0.662, F 0.706)] [D acc: (0.562)(0.656, 0.469)] [G loss: 0.695] [G acc: 0.344]\n",
      "257 [D loss: (0.689)(R 0.637, F 0.741)] [D acc: (0.570)(0.766, 0.375)] [G loss: 0.690] [G acc: 0.391]\n",
      "258 [D loss: (0.690)(R 0.651, F 0.729)] [D acc: (0.500)(0.609, 0.391)] [G loss: 0.700] [G acc: 0.406]\n",
      "259 [D loss: (0.695)(R 0.661, F 0.729)] [D acc: (0.516)(0.609, 0.422)] [G loss: 0.702] [G acc: 0.328]\n",
      "260 [D loss: (0.709)(R 0.632, F 0.786)] [D acc: (0.539)(0.828, 0.250)] [G loss: 0.708] [G acc: 0.250]\n",
      "261 [D loss: (0.688)(R 0.660, F 0.716)] [D acc: (0.547)(0.578, 0.516)] [G loss: 0.732] [G acc: 0.094]\n",
      "262 [D loss: (0.684)(R 0.659, F 0.709)] [D acc: (0.555)(0.609, 0.500)] [G loss: 0.711] [G acc: 0.250]\n",
      "263 [D loss: (0.685)(R 0.661, F 0.709)] [D acc: (0.594)(0.625, 0.562)] [G loss: 0.709] [G acc: 0.281]\n",
      "264 [D loss: (0.696)(R 0.676, F 0.716)] [D acc: (0.570)(0.609, 0.531)] [G loss: 0.712] [G acc: 0.234]\n",
      "265 [D loss: (0.693)(R 0.661, F 0.725)] [D acc: (0.594)(0.703, 0.484)] [G loss: 0.700] [G acc: 0.359]\n",
      "266 [D loss: (0.699)(R 0.662, F 0.736)] [D acc: (0.492)(0.641, 0.344)] [G loss: 0.697] [G acc: 0.344]\n",
      "267 [D loss: (0.685)(R 0.667, F 0.702)] [D acc: (0.602)(0.703, 0.500)] [G loss: 0.708] [G acc: 0.219]\n",
      "268 [D loss: (0.694)(R 0.655, F 0.732)] [D acc: (0.523)(0.719, 0.328)] [G loss: 0.702] [G acc: 0.297]\n",
      "269 [D loss: (0.677)(R 0.642, F 0.711)] [D acc: (0.609)(0.688, 0.531)] [G loss: 0.706] [G acc: 0.312]\n",
      "270 [D loss: (0.691)(R 0.641, F 0.741)] [D acc: (0.539)(0.719, 0.359)] [G loss: 0.710] [G acc: 0.266]\n",
      "271 [D loss: (0.684)(R 0.662, F 0.705)] [D acc: (0.555)(0.609, 0.500)] [G loss: 0.703] [G acc: 0.297]\n",
      "272 [D loss: (0.695)(R 0.670, F 0.720)] [D acc: (0.492)(0.609, 0.375)] [G loss: 0.709] [G acc: 0.266]\n",
      "273 [D loss: (0.679)(R 0.659, F 0.699)] [D acc: (0.594)(0.672, 0.516)] [G loss: 0.708] [G acc: 0.312]\n",
      "274 [D loss: (0.675)(R 0.641, F 0.708)] [D acc: (0.609)(0.750, 0.469)] [G loss: 0.718] [G acc: 0.312]\n",
      "275 [D loss: (0.719)(R 0.636, F 0.801)] [D acc: (0.422)(0.703, 0.141)] [G loss: 0.745] [G acc: 0.078]\n",
      "276 [D loss: (0.678)(R 0.652, F 0.704)] [D acc: (0.586)(0.625, 0.547)] [G loss: 0.724] [G acc: 0.109]\n",
      "277 [D loss: (0.700)(R 0.635, F 0.765)] [D acc: (0.547)(0.797, 0.297)] [G loss: 0.718] [G acc: 0.156]\n",
      "278 [D loss: (0.678)(R 0.638, F 0.717)] [D acc: (0.531)(0.656, 0.406)] [G loss: 0.719] [G acc: 0.250]\n",
      "279 [D loss: (0.690)(R 0.635, F 0.745)] [D acc: (0.516)(0.578, 0.453)] [G loss: 0.705] [G acc: 0.281]\n",
      "280 [D loss: (0.680)(R 0.635, F 0.725)] [D acc: (0.617)(0.781, 0.453)] [G loss: 0.677] [G acc: 0.406]\n",
      "281 [D loss: (0.688)(R 0.637, F 0.739)] [D acc: (0.492)(0.703, 0.281)] [G loss: 0.692] [G acc: 0.422]\n",
      "282 [D loss: (0.711)(R 0.655, F 0.766)] [D acc: (0.523)(0.734, 0.312)] [G loss: 0.697] [G acc: 0.344]\n",
      "283 [D loss: (0.684)(R 0.660, F 0.708)] [D acc: (0.500)(0.641, 0.359)] [G loss: 0.709] [G acc: 0.281]\n",
      "284 [D loss: (0.690)(R 0.653, F 0.727)] [D acc: (0.586)(0.719, 0.453)] [G loss: 0.705] [G acc: 0.266]\n",
      "285 [D loss: (0.694)(R 0.630, F 0.758)] [D acc: (0.484)(0.703, 0.266)] [G loss: 0.707] [G acc: 0.297]\n",
      "286 [D loss: (0.681)(R 0.659, F 0.702)] [D acc: (0.578)(0.672, 0.484)] [G loss: 0.701] [G acc: 0.328]\n",
      "287 [D loss: (0.677)(R 0.654, F 0.700)] [D acc: (0.617)(0.672, 0.562)] [G loss: 0.704] [G acc: 0.344]\n",
      "288 [D loss: (0.687)(R 0.637, F 0.737)] [D acc: (0.594)(0.781, 0.406)] [G loss: 0.693] [G acc: 0.375]\n",
      "289 [D loss: (0.687)(R 0.637, F 0.736)] [D acc: (0.539)(0.688, 0.391)] [G loss: 0.702] [G acc: 0.359]\n",
      "290 [D loss: (0.677)(R 0.625, F 0.730)] [D acc: (0.594)(0.766, 0.422)] [G loss: 0.714] [G acc: 0.328]\n",
      "291 [D loss: (0.672)(R 0.622, F 0.722)] [D acc: (0.648)(0.844, 0.453)] [G loss: 0.707] [G acc: 0.391]\n",
      "292 [D loss: (0.691)(R 0.627, F 0.754)] [D acc: (0.508)(0.688, 0.328)] [G loss: 0.739] [G acc: 0.141]\n",
      "293 [D loss: (0.697)(R 0.659, F 0.735)] [D acc: (0.477)(0.531, 0.422)] [G loss: 0.703] [G acc: 0.281]\n",
      "294 [D loss: (0.683)(R 0.644, F 0.722)] [D acc: (0.555)(0.703, 0.406)] [G loss: 0.693] [G acc: 0.422]\n",
      "295 [D loss: (0.688)(R 0.635, F 0.741)] [D acc: (0.484)(0.625, 0.344)] [G loss: 0.694] [G acc: 0.391]\n",
      "296 [D loss: (0.695)(R 0.637, F 0.753)] [D acc: (0.570)(0.766, 0.375)] [G loss: 0.699] [G acc: 0.406]\n",
      "297 [D loss: (0.682)(R 0.623, F 0.741)] [D acc: (0.531)(0.750, 0.312)] [G loss: 0.701] [G acc: 0.391]\n",
      "298 [D loss: (0.687)(R 0.646, F 0.727)] [D acc: (0.578)(0.688, 0.469)] [G loss: 0.707] [G acc: 0.312]\n",
      "299 [D loss: (0.690)(R 0.648, F 0.732)] [D acc: (0.570)(0.766, 0.375)] [G loss: 0.706] [G acc: 0.312]\n",
      "300 [D loss: (0.683)(R 0.639, F 0.728)] [D acc: (0.500)(0.609, 0.391)] [G loss: 0.719] [G acc: 0.250]\n",
      "301 [D loss: (0.686)(R 0.653, F 0.720)] [D acc: (0.531)(0.609, 0.453)] [G loss: 0.704] [G acc: 0.375]\n",
      "302 [D loss: (0.718)(R 0.665, F 0.771)] [D acc: (0.438)(0.578, 0.297)] [G loss: 0.716] [G acc: 0.188]\n",
      "303 [D loss: (0.696)(R 0.675, F 0.716)] [D acc: (0.547)(0.578, 0.516)] [G loss: 0.707] [G acc: 0.234]\n",
      "304 [D loss: (0.683)(R 0.652, F 0.713)] [D acc: (0.609)(0.672, 0.547)] [G loss: 0.713] [G acc: 0.250]\n",
      "305 [D loss: (0.673)(R 0.649, F 0.697)] [D acc: (0.625)(0.641, 0.609)] [G loss: 0.716] [G acc: 0.203]\n",
      "306 [D loss: (0.689)(R 0.662, F 0.717)] [D acc: (0.547)(0.641, 0.453)] [G loss: 0.717] [G acc: 0.250]\n",
      "307 [D loss: (0.676)(R 0.637, F 0.716)] [D acc: (0.617)(0.750, 0.484)] [G loss: 0.727] [G acc: 0.219]\n",
      "308 [D loss: (0.695)(R 0.639, F 0.752)] [D acc: (0.492)(0.688, 0.297)] [G loss: 0.721] [G acc: 0.188]\n",
      "309 [D loss: (0.701)(R 0.662, F 0.739)] [D acc: (0.477)(0.609, 0.344)] [G loss: 0.714] [G acc: 0.234]\n",
      "310 [D loss: (0.690)(R 0.664, F 0.716)] [D acc: (0.531)(0.594, 0.469)] [G loss: 0.701] [G acc: 0.312]\n",
      "311 [D loss: (0.680)(R 0.642, F 0.718)] [D acc: (0.578)(0.750, 0.406)] [G loss: 0.716] [G acc: 0.281]\n",
      "312 [D loss: (0.686)(R 0.652, F 0.720)] [D acc: (0.555)(0.656, 0.453)] [G loss: 0.706] [G acc: 0.312]\n",
      "313 [D loss: (0.671)(R 0.645, F 0.697)] [D acc: (0.641)(0.719, 0.562)] [G loss: 0.700] [G acc: 0.312]\n",
      "314 [D loss: (0.685)(R 0.635, F 0.736)] [D acc: (0.578)(0.734, 0.422)] [G loss: 0.724] [G acc: 0.250]\n",
      "315 [D loss: (0.670)(R 0.633, F 0.706)] [D acc: (0.633)(0.719, 0.547)] [G loss: 0.740] [G acc: 0.188]\n",
      "316 [D loss: (0.687)(R 0.642, F 0.733)] [D acc: (0.547)(0.688, 0.406)] [G loss: 0.762] [G acc: 0.156]\n",
      "317 [D loss: (0.693)(R 0.604, F 0.782)] [D acc: (0.523)(0.734, 0.312)] [G loss: 0.696] [G acc: 0.375]\n",
      "318 [D loss: (0.660)(R 0.602, F 0.717)] [D acc: (0.609)(0.734, 0.484)] [G loss: 0.697] [G acc: 0.484]\n",
      "319 [D loss: (0.702)(R 0.615, F 0.788)] [D acc: (0.500)(0.703, 0.297)] [G loss: 0.707] [G acc: 0.406]\n",
      "320 [D loss: (0.687)(R 0.635, F 0.739)] [D acc: (0.500)(0.641, 0.359)] [G loss: 0.706] [G acc: 0.375]\n",
      "321 [D loss: (0.673)(R 0.626, F 0.721)] [D acc: (0.586)(0.703, 0.469)] [G loss: 0.726] [G acc: 0.281]\n",
      "322 [D loss: (0.673)(R 0.618, F 0.729)] [D acc: (0.562)(0.719, 0.406)] [G loss: 0.734] [G acc: 0.188]\n",
      "323 [D loss: (0.684)(R 0.648, F 0.720)] [D acc: (0.547)(0.656, 0.438)] [G loss: 0.695] [G acc: 0.359]\n",
      "324 [D loss: (0.677)(R 0.615, F 0.738)] [D acc: (0.586)(0.750, 0.422)] [G loss: 0.714] [G acc: 0.281]\n",
      "325 [D loss: (0.689)(R 0.626, F 0.752)] [D acc: (0.516)(0.641, 0.391)] [G loss: 0.699] [G acc: 0.375]\n",
      "326 [D loss: (0.691)(R 0.640, F 0.742)] [D acc: (0.562)(0.688, 0.438)] [G loss: 0.710] [G acc: 0.359]\n",
      "327 [D loss: (0.685)(R 0.637, F 0.733)] [D acc: (0.539)(0.719, 0.359)] [G loss: 0.698] [G acc: 0.375]\n",
      "328 [D loss: (0.680)(R 0.619, F 0.742)] [D acc: (0.547)(0.641, 0.453)] [G loss: 0.711] [G acc: 0.328]\n",
      "329 [D loss: (0.687)(R 0.643, F 0.731)] [D acc: (0.539)(0.641, 0.438)] [G loss: 0.711] [G acc: 0.281]\n",
      "330 [D loss: (0.690)(R 0.627, F 0.753)] [D acc: (0.508)(0.719, 0.297)] [G loss: 0.719] [G acc: 0.266]\n",
      "331 [D loss: (0.676)(R 0.648, F 0.704)] [D acc: (0.562)(0.594, 0.531)] [G loss: 0.711] [G acc: 0.359]\n",
      "332 [D loss: (0.686)(R 0.638, F 0.734)] [D acc: (0.531)(0.672, 0.391)] [G loss: 0.710] [G acc: 0.312]\n",
      "333 [D loss: (0.682)(R 0.641, F 0.724)] [D acc: (0.617)(0.688, 0.547)] [G loss: 0.731] [G acc: 0.172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334 [D loss: (0.674)(R 0.619, F 0.729)] [D acc: (0.562)(0.719, 0.406)] [G loss: 0.722] [G acc: 0.281]\n",
      "335 [D loss: (0.691)(R 0.643, F 0.738)] [D acc: (0.531)(0.609, 0.453)] [G loss: 0.722] [G acc: 0.203]\n",
      "336 [D loss: (0.679)(R 0.651, F 0.707)] [D acc: (0.570)(0.625, 0.516)] [G loss: 0.723] [G acc: 0.203]\n",
      "337 [D loss: (0.690)(R 0.640, F 0.739)] [D acc: (0.523)(0.656, 0.391)] [G loss: 0.706] [G acc: 0.344]\n",
      "338 [D loss: (0.690)(R 0.666, F 0.714)] [D acc: (0.523)(0.531, 0.516)] [G loss: 0.722] [G acc: 0.281]\n",
      "339 [D loss: (0.673)(R 0.633, F 0.713)] [D acc: (0.594)(0.719, 0.469)] [G loss: 0.719] [G acc: 0.297]\n",
      "340 [D loss: (0.678)(R 0.636, F 0.720)] [D acc: (0.625)(0.719, 0.531)] [G loss: 0.719] [G acc: 0.312]\n",
      "341 [D loss: (0.682)(R 0.639, F 0.725)] [D acc: (0.586)(0.703, 0.469)] [G loss: 0.704] [G acc: 0.391]\n",
      "342 [D loss: (0.691)(R 0.653, F 0.729)] [D acc: (0.508)(0.562, 0.453)] [G loss: 0.721] [G acc: 0.234]\n",
      "343 [D loss: (0.678)(R 0.648, F 0.708)] [D acc: (0.578)(0.641, 0.516)] [G loss: 0.705] [G acc: 0.328]\n",
      "344 [D loss: (0.686)(R 0.637, F 0.735)] [D acc: (0.578)(0.672, 0.484)] [G loss: 0.711] [G acc: 0.250]\n",
      "345 [D loss: (0.687)(R 0.640, F 0.733)] [D acc: (0.477)(0.641, 0.312)] [G loss: 0.706] [G acc: 0.266]\n",
      "346 [D loss: (0.685)(R 0.626, F 0.745)] [D acc: (0.547)(0.672, 0.422)] [G loss: 0.730] [G acc: 0.172]\n",
      "347 [D loss: (0.698)(R 0.631, F 0.766)] [D acc: (0.500)(0.688, 0.312)] [G loss: 0.726] [G acc: 0.234]\n",
      "348 [D loss: (0.698)(R 0.679, F 0.717)] [D acc: (0.500)(0.453, 0.547)] [G loss: 0.722] [G acc: 0.188]\n",
      "349 [D loss: (0.676)(R 0.639, F 0.713)] [D acc: (0.547)(0.578, 0.516)] [G loss: 0.725] [G acc: 0.203]\n",
      "350 [D loss: (0.677)(R 0.654, F 0.700)] [D acc: (0.609)(0.656, 0.562)] [G loss: 0.722] [G acc: 0.250]\n",
      "351 [D loss: (0.675)(R 0.628, F 0.723)] [D acc: (0.609)(0.734, 0.484)] [G loss: 0.719] [G acc: 0.203]\n",
      "352 [D loss: (0.658)(R 0.603, F 0.713)] [D acc: (0.609)(0.688, 0.531)] [G loss: 0.741] [G acc: 0.188]\n",
      "353 [D loss: (0.675)(R 0.611, F 0.738)] [D acc: (0.578)(0.641, 0.516)] [G loss: 0.729] [G acc: 0.219]\n",
      "354 [D loss: (0.673)(R 0.621, F 0.724)] [D acc: (0.547)(0.625, 0.469)] [G loss: 0.725] [G acc: 0.188]\n",
      "355 [D loss: (0.666)(R 0.598, F 0.734)] [D acc: (0.641)(0.797, 0.484)] [G loss: 0.714] [G acc: 0.219]\n",
      "356 [D loss: (0.681)(R 0.622, F 0.740)] [D acc: (0.547)(0.672, 0.422)] [G loss: 0.729] [G acc: 0.266]\n",
      "357 [D loss: (0.657)(R 0.612, F 0.703)] [D acc: (0.625)(0.672, 0.578)] [G loss: 0.750] [G acc: 0.188]\n",
      "358 [D loss: (0.675)(R 0.615, F 0.735)] [D acc: (0.539)(0.656, 0.422)] [G loss: 0.737] [G acc: 0.266]\n",
      "359 [D loss: (0.684)(R 0.625, F 0.743)] [D acc: (0.539)(0.688, 0.391)] [G loss: 0.732] [G acc: 0.219]\n",
      "360 [D loss: (0.692)(R 0.608, F 0.776)] [D acc: (0.586)(0.812, 0.359)] [G loss: 0.729] [G acc: 0.219]\n",
      "361 [D loss: (0.661)(R 0.621, F 0.700)] [D acc: (0.633)(0.719, 0.547)] [G loss: 0.759] [G acc: 0.188]\n",
      "362 [D loss: (0.683)(R 0.616, F 0.751)] [D acc: (0.555)(0.688, 0.422)] [G loss: 0.745] [G acc: 0.188]\n",
      "363 [D loss: (0.662)(R 0.603, F 0.721)] [D acc: (0.625)(0.766, 0.484)] [G loss: 0.736] [G acc: 0.172]\n",
      "364 [D loss: (0.676)(R 0.615, F 0.737)] [D acc: (0.531)(0.703, 0.359)] [G loss: 0.725] [G acc: 0.344]\n",
      "365 [D loss: (0.681)(R 0.609, F 0.753)] [D acc: (0.555)(0.703, 0.406)] [G loss: 0.728] [G acc: 0.266]\n",
      "366 [D loss: (0.675)(R 0.626, F 0.723)] [D acc: (0.602)(0.734, 0.469)] [G loss: 0.719] [G acc: 0.344]\n",
      "367 [D loss: (0.666)(R 0.598, F 0.734)] [D acc: (0.617)(0.750, 0.484)] [G loss: 0.738] [G acc: 0.234]\n",
      "368 [D loss: (0.677)(R 0.623, F 0.730)] [D acc: (0.617)(0.719, 0.516)] [G loss: 0.720] [G acc: 0.281]\n",
      "369 [D loss: (0.665)(R 0.603, F 0.727)] [D acc: (0.625)(0.734, 0.516)] [G loss: 0.732] [G acc: 0.250]\n",
      "370 [D loss: (0.657)(R 0.598, F 0.716)] [D acc: (0.617)(0.719, 0.516)] [G loss: 0.731] [G acc: 0.328]\n",
      "371 [D loss: (0.676)(R 0.616, F 0.735)] [D acc: (0.570)(0.688, 0.453)] [G loss: 0.726] [G acc: 0.281]\n",
      "372 [D loss: (0.683)(R 0.593, F 0.772)] [D acc: (0.500)(0.734, 0.266)] [G loss: 0.737] [G acc: 0.203]\n",
      "373 [D loss: (0.676)(R 0.602, F 0.750)] [D acc: (0.547)(0.734, 0.359)] [G loss: 0.730] [G acc: 0.359]\n",
      "374 [D loss: (0.666)(R 0.611, F 0.722)] [D acc: (0.617)(0.719, 0.516)] [G loss: 0.734] [G acc: 0.297]\n",
      "375 [D loss: (0.678)(R 0.570, F 0.787)] [D acc: (0.570)(0.781, 0.359)] [G loss: 0.750] [G acc: 0.188]\n",
      "376 [D loss: (0.638)(R 0.587, F 0.690)] [D acc: (0.672)(0.766, 0.578)] [G loss: 0.784] [G acc: 0.172]\n",
      "377 [D loss: (0.642)(R 0.568, F 0.716)] [D acc: (0.617)(0.750, 0.484)] [G loss: 0.786] [G acc: 0.125]\n",
      "378 [D loss: (0.656)(R 0.553, F 0.759)] [D acc: (0.586)(0.703, 0.469)] [G loss: 0.753] [G acc: 0.234]\n",
      "379 [D loss: (0.641)(R 0.582, F 0.700)] [D acc: (0.688)(0.766, 0.609)] [G loss: 0.732] [G acc: 0.359]\n",
      "380 [D loss: (0.673)(R 0.547, F 0.799)] [D acc: (0.578)(0.781, 0.375)] [G loss: 0.769] [G acc: 0.219]\n",
      "381 [D loss: (0.672)(R 0.546, F 0.799)] [D acc: (0.523)(0.750, 0.297)] [G loss: 0.699] [G acc: 0.391]\n",
      "382 [D loss: (0.649)(R 0.536, F 0.763)] [D acc: (0.648)(0.828, 0.469)] [G loss: 0.734] [G acc: 0.297]\n",
      "383 [D loss: (0.670)(R 0.572, F 0.769)] [D acc: (0.562)(0.719, 0.406)] [G loss: 0.731] [G acc: 0.406]\n",
      "384 [D loss: (0.691)(R 0.575, F 0.807)] [D acc: (0.555)(0.719, 0.391)] [G loss: 0.708] [G acc: 0.375]\n",
      "385 [D loss: (0.672)(R 0.583, F 0.760)] [D acc: (0.578)(0.688, 0.469)] [G loss: 0.714] [G acc: 0.359]\n",
      "386 [D loss: (0.673)(R 0.585, F 0.761)] [D acc: (0.555)(0.656, 0.453)] [G loss: 0.736] [G acc: 0.250]\n",
      "387 [D loss: (0.708)(R 0.641, F 0.775)] [D acc: (0.547)(0.609, 0.484)] [G loss: 0.729] [G acc: 0.297]\n",
      "388 [D loss: (0.682)(R 0.619, F 0.745)] [D acc: (0.539)(0.656, 0.422)] [G loss: 0.721] [G acc: 0.312]\n",
      "389 [D loss: (0.675)(R 0.634, F 0.715)] [D acc: (0.672)(0.703, 0.641)] [G loss: 0.713] [G acc: 0.297]\n",
      "390 [D loss: (0.677)(R 0.606, F 0.748)] [D acc: (0.617)(0.719, 0.516)] [G loss: 0.731] [G acc: 0.312]\n",
      "391 [D loss: (0.659)(R 0.621, F 0.696)] [D acc: (0.625)(0.656, 0.594)] [G loss: 0.753] [G acc: 0.219]\n",
      "392 [D loss: (0.655)(R 0.588, F 0.722)] [D acc: (0.617)(0.672, 0.562)] [G loss: 0.746] [G acc: 0.250]\n",
      "393 [D loss: (0.655)(R 0.602, F 0.708)] [D acc: (0.656)(0.781, 0.531)] [G loss: 0.763] [G acc: 0.188]\n",
      "394 [D loss: (0.676)(R 0.578, F 0.773)] [D acc: (0.641)(0.844, 0.438)] [G loss: 0.725] [G acc: 0.281]\n",
      "395 [D loss: (0.691)(R 0.602, F 0.780)] [D acc: (0.586)(0.719, 0.453)] [G loss: 0.758] [G acc: 0.172]\n",
      "396 [D loss: (0.632)(R 0.561, F 0.704)] [D acc: (0.727)(0.828, 0.625)] [G loss: 0.777] [G acc: 0.219]\n",
      "397 [D loss: (0.649)(R 0.560, F 0.737)] [D acc: (0.617)(0.766, 0.469)] [G loss: 0.768] [G acc: 0.250]\n",
      "398 [D loss: (0.653)(R 0.563, F 0.742)] [D acc: (0.633)(0.797, 0.469)] [G loss: 0.774] [G acc: 0.250]\n",
      "399 [D loss: (0.713)(R 0.565, F 0.861)] [D acc: (0.547)(0.688, 0.406)] [G loss: 0.749] [G acc: 0.312]\n",
      "400 [D loss: (0.650)(R 0.567, F 0.732)] [D acc: (0.609)(0.797, 0.422)] [G loss: 0.755] [G acc: 0.266]\n",
      "401 [D loss: (0.681)(R 0.630, F 0.732)] [D acc: (0.562)(0.641, 0.484)] [G loss: 0.744] [G acc: 0.234]\n",
      "402 [D loss: (0.636)(R 0.509, F 0.762)] [D acc: (0.633)(0.859, 0.406)] [G loss: 0.778] [G acc: 0.172]\n",
      "403 [D loss: (0.672)(R 0.612, F 0.733)] [D acc: (0.586)(0.656, 0.516)] [G loss: 0.798] [G acc: 0.156]\n",
      "404 [D loss: (0.680)(R 0.624, F 0.735)] [D acc: (0.539)(0.578, 0.500)] [G loss: 0.780] [G acc: 0.156]\n",
      "405 [D loss: (0.651)(R 0.590, F 0.712)] [D acc: (0.656)(0.750, 0.562)] [G loss: 0.780] [G acc: 0.141]\n",
      "406 [D loss: (0.647)(R 0.560, F 0.734)] [D acc: (0.617)(0.703, 0.531)] [G loss: 0.777] [G acc: 0.156]\n",
      "407 [D loss: (0.687)(R 0.641, F 0.734)] [D acc: (0.555)(0.562, 0.547)] [G loss: 0.765] [G acc: 0.234]\n",
      "408 [D loss: (0.660)(R 0.601, F 0.720)] [D acc: (0.594)(0.703, 0.484)] [G loss: 0.776] [G acc: 0.141]\n",
      "409 [D loss: (0.663)(R 0.563, F 0.764)] [D acc: (0.570)(0.734, 0.406)] [G loss: 0.753] [G acc: 0.297]\n",
      "410 [D loss: (0.651)(R 0.632, F 0.671)] [D acc: (0.617)(0.609, 0.625)] [G loss: 0.735] [G acc: 0.297]\n",
      "411 [D loss: (0.634)(R 0.554, F 0.715)] [D acc: (0.680)(0.781, 0.578)] [G loss: 0.751] [G acc: 0.281]\n",
      "412 [D loss: (0.651)(R 0.531, F 0.772)] [D acc: (0.648)(0.781, 0.516)] [G loss: 0.734] [G acc: 0.281]\n",
      "413 [D loss: (0.716)(R 0.568, F 0.864)] [D acc: (0.555)(0.719, 0.391)] [G loss: 0.742] [G acc: 0.250]\n",
      "414 [D loss: (0.664)(R 0.607, F 0.721)] [D acc: (0.586)(0.688, 0.484)] [G loss: 0.722] [G acc: 0.281]\n",
      "415 [D loss: (0.660)(R 0.610, F 0.710)] [D acc: (0.641)(0.656, 0.625)] [G loss: 0.741] [G acc: 0.281]\n",
      "416 [D loss: (0.668)(R 0.607, F 0.728)] [D acc: (0.523)(0.594, 0.453)] [G loss: 0.742] [G acc: 0.203]\n",
      "417 [D loss: (0.651)(R 0.556, F 0.747)] [D acc: (0.609)(0.781, 0.438)] [G loss: 0.753] [G acc: 0.234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418 [D loss: (0.655)(R 0.595, F 0.715)] [D acc: (0.633)(0.688, 0.578)] [G loss: 0.758] [G acc: 0.281]\n",
      "419 [D loss: (0.668)(R 0.587, F 0.749)] [D acc: (0.602)(0.688, 0.516)] [G loss: 0.789] [G acc: 0.141]\n",
      "420 [D loss: (0.655)(R 0.567, F 0.742)] [D acc: (0.609)(0.766, 0.453)] [G loss: 0.759] [G acc: 0.188]\n",
      "421 [D loss: (0.651)(R 0.587, F 0.714)] [D acc: (0.672)(0.719, 0.625)] [G loss: 0.797] [G acc: 0.219]\n",
      "422 [D loss: (0.645)(R 0.572, F 0.718)] [D acc: (0.602)(0.672, 0.531)] [G loss: 0.798] [G acc: 0.172]\n",
      "423 [D loss: (0.673)(R 0.567, F 0.779)] [D acc: (0.547)(0.703, 0.391)] [G loss: 0.765] [G acc: 0.203]\n",
      "424 [D loss: (0.649)(R 0.559, F 0.738)] [D acc: (0.625)(0.719, 0.531)] [G loss: 0.763] [G acc: 0.188]\n",
      "425 [D loss: (0.672)(R 0.604, F 0.740)] [D acc: (0.578)(0.703, 0.453)] [G loss: 0.759] [G acc: 0.219]\n",
      "426 [D loss: (0.672)(R 0.588, F 0.756)] [D acc: (0.594)(0.719, 0.469)] [G loss: 0.757] [G acc: 0.188]\n",
      "427 [D loss: (0.652)(R 0.588, F 0.716)] [D acc: (0.641)(0.734, 0.547)] [G loss: 0.742] [G acc: 0.266]\n",
      "428 [D loss: (0.658)(R 0.599, F 0.716)] [D acc: (0.625)(0.656, 0.594)] [G loss: 0.741] [G acc: 0.344]\n",
      "429 [D loss: (0.701)(R 0.600, F 0.801)] [D acc: (0.547)(0.703, 0.391)] [G loss: 0.727] [G acc: 0.234]\n",
      "430 [D loss: (0.651)(R 0.578, F 0.725)] [D acc: (0.633)(0.719, 0.547)] [G loss: 0.754] [G acc: 0.297]\n",
      "431 [D loss: (0.676)(R 0.614, F 0.737)] [D acc: (0.562)(0.625, 0.500)] [G loss: 0.806] [G acc: 0.141]\n",
      "432 [D loss: (0.642)(R 0.589, F 0.694)] [D acc: (0.609)(0.609, 0.609)] [G loss: 0.837] [G acc: 0.078]\n",
      "433 [D loss: (0.654)(R 0.591, F 0.716)] [D acc: (0.656)(0.672, 0.641)] [G loss: 0.824] [G acc: 0.188]\n",
      "434 [D loss: (0.674)(R 0.596, F 0.752)] [D acc: (0.617)(0.688, 0.547)] [G loss: 0.835] [G acc: 0.188]\n",
      "435 [D loss: (0.606)(R 0.561, F 0.651)] [D acc: (0.727)(0.766, 0.688)] [G loss: 0.841] [G acc: 0.141]\n",
      "436 [D loss: (0.684)(R 0.547, F 0.820)] [D acc: (0.602)(0.781, 0.422)] [G loss: 0.766] [G acc: 0.281]\n",
      "437 [D loss: (0.634)(R 0.498, F 0.770)] [D acc: (0.609)(0.812, 0.406)] [G loss: 0.756] [G acc: 0.297]\n",
      "438 [D loss: (0.721)(R 0.616, F 0.827)] [D acc: (0.477)(0.562, 0.391)] [G loss: 0.792] [G acc: 0.250]\n",
      "439 [D loss: (0.686)(R 0.644, F 0.727)] [D acc: (0.586)(0.625, 0.547)] [G loss: 0.744] [G acc: 0.328]\n",
      "440 [D loss: (0.663)(R 0.599, F 0.726)] [D acc: (0.570)(0.688, 0.453)] [G loss: 0.767] [G acc: 0.281]\n",
      "441 [D loss: (0.668)(R 0.565, F 0.771)] [D acc: (0.625)(0.734, 0.516)] [G loss: 0.762] [G acc: 0.266]\n",
      "442 [D loss: (0.683)(R 0.573, F 0.794)] [D acc: (0.539)(0.703, 0.375)] [G loss: 0.782] [G acc: 0.219]\n",
      "443 [D loss: (0.645)(R 0.595, F 0.695)] [D acc: (0.695)(0.719, 0.672)] [G loss: 0.757] [G acc: 0.281]\n",
      "444 [D loss: (0.676)(R 0.610, F 0.743)] [D acc: (0.594)(0.656, 0.531)] [G loss: 0.800] [G acc: 0.141]\n",
      "445 [D loss: (0.647)(R 0.572, F 0.721)] [D acc: (0.594)(0.703, 0.484)] [G loss: 0.805] [G acc: 0.094]\n",
      "446 [D loss: (0.661)(R 0.631, F 0.692)] [D acc: (0.625)(0.641, 0.609)] [G loss: 0.775] [G acc: 0.312]\n",
      "447 [D loss: (0.662)(R 0.579, F 0.745)] [D acc: (0.594)(0.703, 0.484)] [G loss: 0.772] [G acc: 0.250]\n",
      "448 [D loss: (0.669)(R 0.598, F 0.740)] [D acc: (0.555)(0.641, 0.469)] [G loss: 0.810] [G acc: 0.141]\n",
      "449 [D loss: (0.684)(R 0.638, F 0.731)] [D acc: (0.531)(0.547, 0.516)] [G loss: 0.785] [G acc: 0.156]\n",
      "450 [D loss: (0.640)(R 0.612, F 0.668)] [D acc: (0.648)(0.641, 0.656)] [G loss: 0.767] [G acc: 0.328]\n",
      "451 [D loss: (0.668)(R 0.570, F 0.766)] [D acc: (0.633)(0.750, 0.516)] [G loss: 0.781] [G acc: 0.250]\n",
      "452 [D loss: (0.659)(R 0.624, F 0.694)] [D acc: (0.562)(0.562, 0.562)] [G loss: 0.777] [G acc: 0.219]\n",
      "453 [D loss: (0.640)(R 0.612, F 0.668)] [D acc: (0.680)(0.641, 0.719)] [G loss: 0.809] [G acc: 0.172]\n",
      "454 [D loss: (0.656)(R 0.604, F 0.709)] [D acc: (0.602)(0.672, 0.531)] [G loss: 0.790] [G acc: 0.203]\n",
      "455 [D loss: (0.647)(R 0.616, F 0.678)] [D acc: (0.578)(0.578, 0.578)] [G loss: 0.751] [G acc: 0.297]\n",
      "456 [D loss: (0.688)(R 0.589, F 0.787)] [D acc: (0.555)(0.656, 0.453)] [G loss: 0.786] [G acc: 0.234]\n",
      "457 [D loss: (0.659)(R 0.624, F 0.693)] [D acc: (0.578)(0.594, 0.562)] [G loss: 0.791] [G acc: 0.188]\n",
      "458 [D loss: (0.674)(R 0.615, F 0.733)] [D acc: (0.648)(0.719, 0.578)] [G loss: 0.809] [G acc: 0.234]\n",
      "459 [D loss: (0.664)(R 0.597, F 0.731)] [D acc: (0.648)(0.766, 0.531)] [G loss: 0.799] [G acc: 0.188]\n",
      "460 [D loss: (0.717)(R 0.583, F 0.851)] [D acc: (0.586)(0.734, 0.438)] [G loss: 0.782] [G acc: 0.266]\n",
      "461 [D loss: (0.633)(R 0.572, F 0.694)] [D acc: (0.695)(0.766, 0.625)] [G loss: 0.767] [G acc: 0.266]\n",
      "462 [D loss: (0.647)(R 0.549, F 0.745)] [D acc: (0.742)(0.812, 0.672)] [G loss: 0.772] [G acc: 0.297]\n",
      "463 [D loss: (0.644)(R 0.570, F 0.718)] [D acc: (0.602)(0.656, 0.547)] [G loss: 0.766] [G acc: 0.297]\n",
      "464 [D loss: (0.594)(R 0.505, F 0.682)] [D acc: (0.656)(0.750, 0.562)] [G loss: 0.859] [G acc: 0.094]\n",
      "465 [D loss: (0.678)(R 0.434, F 0.921)] [D acc: (0.641)(0.844, 0.438)] [G loss: 0.864] [G acc: 0.109]\n",
      "466 [D loss: (0.639)(R 0.494, F 0.784)] [D acc: (0.594)(0.797, 0.391)] [G loss: 0.842] [G acc: 0.156]\n",
      "467 [D loss: (0.602)(R 0.479, F 0.725)] [D acc: (0.695)(0.812, 0.578)] [G loss: 0.789] [G acc: 0.328]\n",
      "468 [D loss: (0.671)(R 0.545, F 0.797)] [D acc: (0.547)(0.688, 0.406)] [G loss: 0.763] [G acc: 0.328]\n",
      "469 [D loss: (0.667)(R 0.515, F 0.819)] [D acc: (0.594)(0.719, 0.469)] [G loss: 0.757] [G acc: 0.312]\n",
      "470 [D loss: (0.648)(R 0.565, F 0.731)] [D acc: (0.641)(0.625, 0.656)] [G loss: 0.772] [G acc: 0.234]\n",
      "471 [D loss: (0.682)(R 0.591, F 0.772)] [D acc: (0.578)(0.641, 0.516)] [G loss: 0.719] [G acc: 0.453]\n",
      "472 [D loss: (0.660)(R 0.576, F 0.743)] [D acc: (0.578)(0.656, 0.500)] [G loss: 0.748] [G acc: 0.375]\n",
      "473 [D loss: (0.680)(R 0.596, F 0.764)] [D acc: (0.555)(0.688, 0.422)] [G loss: 0.801] [G acc: 0.188]\n",
      "474 [D loss: (0.680)(R 0.593, F 0.768)] [D acc: (0.562)(0.641, 0.484)] [G loss: 0.796] [G acc: 0.172]\n",
      "475 [D loss: (0.673)(R 0.628, F 0.717)] [D acc: (0.570)(0.625, 0.516)] [G loss: 0.780] [G acc: 0.188]\n",
      "476 [D loss: (0.683)(R 0.648, F 0.718)] [D acc: (0.562)(0.609, 0.516)] [G loss: 0.767] [G acc: 0.266]\n",
      "477 [D loss: (0.669)(R 0.653, F 0.686)] [D acc: (0.586)(0.531, 0.641)] [G loss: 0.791] [G acc: 0.219]\n",
      "478 [D loss: (0.670)(R 0.631, F 0.710)] [D acc: (0.586)(0.609, 0.562)] [G loss: 0.806] [G acc: 0.203]\n",
      "479 [D loss: (0.691)(R 0.614, F 0.767)] [D acc: (0.578)(0.641, 0.516)] [G loss: 0.833] [G acc: 0.188]\n",
      "480 [D loss: (0.667)(R 0.669, F 0.665)] [D acc: (0.648)(0.641, 0.656)] [G loss: 0.799] [G acc: 0.203]\n",
      "481 [D loss: (0.635)(R 0.599, F 0.671)] [D acc: (0.672)(0.656, 0.688)] [G loss: 0.769] [G acc: 0.281]\n",
      "482 [D loss: (0.673)(R 0.638, F 0.708)] [D acc: (0.578)(0.578, 0.578)] [G loss: 0.821] [G acc: 0.172]\n",
      "483 [D loss: (0.689)(R 0.657, F 0.722)] [D acc: (0.539)(0.562, 0.516)] [G loss: 0.763] [G acc: 0.219]\n",
      "484 [D loss: (0.649)(R 0.566, F 0.732)] [D acc: (0.648)(0.734, 0.562)] [G loss: 0.786] [G acc: 0.250]\n",
      "485 [D loss: (0.664)(R 0.608, F 0.720)] [D acc: (0.578)(0.656, 0.500)] [G loss: 0.819] [G acc: 0.156]\n",
      "486 [D loss: (0.669)(R 0.658, F 0.679)] [D acc: (0.609)(0.562, 0.656)] [G loss: 0.766] [G acc: 0.188]\n",
      "487 [D loss: (0.665)(R 0.623, F 0.706)] [D acc: (0.594)(0.594, 0.594)] [G loss: 0.809] [G acc: 0.188]\n",
      "488 [D loss: (0.666)(R 0.589, F 0.744)] [D acc: (0.641)(0.719, 0.562)] [G loss: 0.807] [G acc: 0.266]\n",
      "489 [D loss: (0.645)(R 0.604, F 0.687)] [D acc: (0.609)(0.641, 0.578)] [G loss: 0.826] [G acc: 0.156]\n",
      "490 [D loss: (0.624)(R 0.598, F 0.650)] [D acc: (0.664)(0.688, 0.641)] [G loss: 0.839] [G acc: 0.188]\n",
      "491 [D loss: (0.652)(R 0.574, F 0.730)] [D acc: (0.609)(0.672, 0.547)] [G loss: 0.811] [G acc: 0.281]\n",
      "492 [D loss: (0.613)(R 0.539, F 0.687)] [D acc: (0.688)(0.766, 0.609)] [G loss: 0.779] [G acc: 0.297]\n",
      "493 [D loss: (0.684)(R 0.560, F 0.809)] [D acc: (0.594)(0.688, 0.500)] [G loss: 0.845] [G acc: 0.156]\n",
      "494 [D loss: (0.636)(R 0.592, F 0.679)] [D acc: (0.695)(0.688, 0.703)] [G loss: 0.775] [G acc: 0.344]\n",
      "495 [D loss: (0.657)(R 0.532, F 0.782)] [D acc: (0.602)(0.750, 0.453)] [G loss: 0.825] [G acc: 0.250]\n",
      "496 [D loss: (0.615)(R 0.570, F 0.660)] [D acc: (0.656)(0.688, 0.625)] [G loss: 0.787] [G acc: 0.297]\n",
      "497 [D loss: (0.646)(R 0.584, F 0.708)] [D acc: (0.594)(0.562, 0.625)] [G loss: 0.891] [G acc: 0.156]\n",
      "498 [D loss: (0.668)(R 0.561, F 0.775)] [D acc: (0.680)(0.844, 0.516)] [G loss: 0.831] [G acc: 0.219]\n",
      "499 [D loss: (0.620)(R 0.579, F 0.661)] [D acc: (0.703)(0.656, 0.750)] [G loss: 0.841] [G acc: 0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 [D loss: (0.668)(R 0.611, F 0.725)] [D acc: (0.602)(0.609, 0.594)] [G loss: 0.793] [G acc: 0.266]\n",
      "501 [D loss: (0.664)(R 0.563, F 0.765)] [D acc: (0.641)(0.688, 0.594)] [G loss: 0.843] [G acc: 0.141]\n",
      "502 [D loss: (0.621)(R 0.560, F 0.682)] [D acc: (0.672)(0.703, 0.641)] [G loss: 0.796] [G acc: 0.234]\n",
      "503 [D loss: (0.644)(R 0.560, F 0.729)] [D acc: (0.531)(0.641, 0.422)] [G loss: 0.851] [G acc: 0.141]\n",
      "504 [D loss: (0.667)(R 0.615, F 0.719)] [D acc: (0.602)(0.547, 0.656)] [G loss: 0.819] [G acc: 0.203]\n",
      "505 [D loss: (0.639)(R 0.567, F 0.711)] [D acc: (0.594)(0.641, 0.547)] [G loss: 0.820] [G acc: 0.156]\n",
      "506 [D loss: (0.667)(R 0.620, F 0.714)] [D acc: (0.586)(0.609, 0.562)] [G loss: 0.818] [G acc: 0.250]\n",
      "507 [D loss: (0.628)(R 0.545, F 0.711)] [D acc: (0.680)(0.734, 0.625)] [G loss: 0.813] [G acc: 0.172]\n",
      "508 [D loss: (0.624)(R 0.574, F 0.673)] [D acc: (0.664)(0.703, 0.625)] [G loss: 0.843] [G acc: 0.141]\n",
      "509 [D loss: (0.622)(R 0.528, F 0.716)] [D acc: (0.719)(0.781, 0.656)] [G loss: 0.847] [G acc: 0.203]\n",
      "510 [D loss: (0.652)(R 0.577, F 0.727)] [D acc: (0.656)(0.656, 0.656)] [G loss: 0.851] [G acc: 0.172]\n",
      "511 [D loss: (0.670)(R 0.541, F 0.798)] [D acc: (0.602)(0.719, 0.484)] [G loss: 0.811] [G acc: 0.172]\n",
      "512 [D loss: (0.618)(R 0.526, F 0.710)] [D acc: (0.656)(0.719, 0.594)] [G loss: 0.934] [G acc: 0.125]\n",
      "513 [D loss: (0.623)(R 0.576, F 0.669)] [D acc: (0.648)(0.641, 0.656)] [G loss: 0.886] [G acc: 0.109]\n",
      "514 [D loss: (0.681)(R 0.604, F 0.757)] [D acc: (0.562)(0.625, 0.500)] [G loss: 0.864] [G acc: 0.125]\n",
      "515 [D loss: (0.626)(R 0.579, F 0.674)] [D acc: (0.648)(0.625, 0.672)] [G loss: 0.789] [G acc: 0.391]\n",
      "516 [D loss: (0.662)(R 0.663, F 0.662)] [D acc: (0.602)(0.531, 0.672)] [G loss: 0.805] [G acc: 0.312]\n",
      "517 [D loss: (0.604)(R 0.502, F 0.706)] [D acc: (0.633)(0.750, 0.516)] [G loss: 0.853] [G acc: 0.188]\n",
      "518 [D loss: (0.689)(R 0.561, F 0.817)] [D acc: (0.562)(0.672, 0.453)] [G loss: 0.861] [G acc: 0.141]\n",
      "519 [D loss: (0.633)(R 0.610, F 0.656)] [D acc: (0.672)(0.656, 0.688)] [G loss: 0.863] [G acc: 0.219]\n",
      "520 [D loss: (0.682)(R 0.620, F 0.745)] [D acc: (0.578)(0.578, 0.578)] [G loss: 0.832] [G acc: 0.172]\n",
      "521 [D loss: (0.656)(R 0.597, F 0.714)] [D acc: (0.617)(0.625, 0.609)] [G loss: 0.871] [G acc: 0.125]\n",
      "522 [D loss: (0.614)(R 0.567, F 0.661)] [D acc: (0.641)(0.656, 0.625)] [G loss: 0.849] [G acc: 0.188]\n",
      "523 [D loss: (0.640)(R 0.561, F 0.720)] [D acc: (0.625)(0.688, 0.562)] [G loss: 0.856] [G acc: 0.188]\n",
      "524 [D loss: (0.648)(R 0.621, F 0.674)] [D acc: (0.578)(0.562, 0.594)] [G loss: 0.836] [G acc: 0.234]\n",
      "525 [D loss: (0.680)(R 0.585, F 0.774)] [D acc: (0.602)(0.641, 0.562)] [G loss: 0.854] [G acc: 0.156]\n",
      "526 [D loss: (0.602)(R 0.532, F 0.673)] [D acc: (0.688)(0.703, 0.672)] [G loss: 0.890] [G acc: 0.094]\n",
      "527 [D loss: (0.632)(R 0.548, F 0.717)] [D acc: (0.617)(0.656, 0.578)] [G loss: 0.895] [G acc: 0.172]\n",
      "528 [D loss: (0.636)(R 0.637, F 0.636)] [D acc: (0.625)(0.516, 0.734)] [G loss: 0.880] [G acc: 0.109]\n",
      "529 [D loss: (0.630)(R 0.556, F 0.705)] [D acc: (0.672)(0.703, 0.641)] [G loss: 0.875] [G acc: 0.203]\n",
      "530 [D loss: (0.676)(R 0.568, F 0.784)] [D acc: (0.570)(0.688, 0.453)] [G loss: 0.834] [G acc: 0.219]\n",
      "531 [D loss: (0.641)(R 0.537, F 0.746)] [D acc: (0.680)(0.781, 0.578)] [G loss: 0.880] [G acc: 0.156]\n",
      "532 [D loss: (0.627)(R 0.589, F 0.666)] [D acc: (0.625)(0.594, 0.656)] [G loss: 0.862] [G acc: 0.219]\n",
      "533 [D loss: (0.632)(R 0.545, F 0.718)] [D acc: (0.617)(0.688, 0.547)] [G loss: 0.836] [G acc: 0.266]\n",
      "534 [D loss: (0.678)(R 0.563, F 0.793)] [D acc: (0.617)(0.672, 0.562)] [G loss: 0.857] [G acc: 0.188]\n",
      "535 [D loss: (0.624)(R 0.600, F 0.647)] [D acc: (0.688)(0.688, 0.688)] [G loss: 0.818] [G acc: 0.203]\n",
      "536 [D loss: (0.658)(R 0.566, F 0.750)] [D acc: (0.656)(0.688, 0.625)] [G loss: 0.849] [G acc: 0.219]\n",
      "537 [D loss: (0.652)(R 0.511, F 0.793)] [D acc: (0.609)(0.734, 0.484)] [G loss: 0.844] [G acc: 0.250]\n",
      "538 [D loss: (0.647)(R 0.569, F 0.724)] [D acc: (0.656)(0.703, 0.609)] [G loss: 0.825] [G acc: 0.297]\n",
      "539 [D loss: (0.629)(R 0.575, F 0.684)] [D acc: (0.633)(0.625, 0.641)] [G loss: 0.928] [G acc: 0.141]\n",
      "540 [D loss: (0.631)(R 0.581, F 0.682)] [D acc: (0.648)(0.625, 0.672)] [G loss: 0.861] [G acc: 0.141]\n",
      "541 [D loss: (0.637)(R 0.585, F 0.689)] [D acc: (0.633)(0.641, 0.625)] [G loss: 0.858] [G acc: 0.188]\n",
      "542 [D loss: (0.632)(R 0.555, F 0.708)] [D acc: (0.633)(0.672, 0.594)] [G loss: 0.909] [G acc: 0.125]\n",
      "543 [D loss: (0.618)(R 0.596, F 0.640)] [D acc: (0.680)(0.625, 0.734)] [G loss: 0.909] [G acc: 0.250]\n",
      "544 [D loss: (0.664)(R 0.647, F 0.681)] [D acc: (0.609)(0.531, 0.688)] [G loss: 0.888] [G acc: 0.141]\n",
      "545 [D loss: (0.622)(R 0.626, F 0.618)] [D acc: (0.672)(0.578, 0.766)] [G loss: 0.880] [G acc: 0.203]\n",
      "546 [D loss: (0.626)(R 0.558, F 0.695)] [D acc: (0.742)(0.766, 0.719)] [G loss: 0.881] [G acc: 0.188]\n",
      "547 [D loss: (0.658)(R 0.604, F 0.712)] [D acc: (0.609)(0.625, 0.594)] [G loss: 0.842] [G acc: 0.250]\n",
      "548 [D loss: (0.635)(R 0.563, F 0.707)] [D acc: (0.656)(0.703, 0.609)] [G loss: 0.888] [G acc: 0.234]\n",
      "549 [D loss: (0.628)(R 0.546, F 0.710)] [D acc: (0.625)(0.656, 0.594)] [G loss: 0.903] [G acc: 0.156]\n",
      "550 [D loss: (0.678)(R 0.666, F 0.690)] [D acc: (0.547)(0.516, 0.578)] [G loss: 0.870] [G acc: 0.141]\n",
      "551 [D loss: (0.664)(R 0.599, F 0.729)] [D acc: (0.570)(0.609, 0.531)] [G loss: 0.850] [G acc: 0.234]\n",
      "552 [D loss: (0.645)(R 0.572, F 0.718)] [D acc: (0.633)(0.656, 0.609)] [G loss: 0.831] [G acc: 0.203]\n",
      "553 [D loss: (0.649)(R 0.627, F 0.672)] [D acc: (0.625)(0.578, 0.672)] [G loss: 0.873] [G acc: 0.156]\n",
      "554 [D loss: (0.618)(R 0.592, F 0.645)] [D acc: (0.688)(0.656, 0.719)] [G loss: 0.866] [G acc: 0.234]\n",
      "555 [D loss: (0.621)(R 0.527, F 0.714)] [D acc: (0.672)(0.750, 0.594)] [G loss: 0.857] [G acc: 0.141]\n",
      "556 [D loss: (0.681)(R 0.527, F 0.834)] [D acc: (0.641)(0.766, 0.516)] [G loss: 0.919] [G acc: 0.125]\n",
      "557 [D loss: (0.666)(R 0.573, F 0.759)] [D acc: (0.609)(0.719, 0.500)] [G loss: 0.920] [G acc: 0.109]\n",
      "558 [D loss: (0.642)(R 0.608, F 0.676)] [D acc: (0.625)(0.625, 0.625)] [G loss: 0.871] [G acc: 0.125]\n",
      "559 [D loss: (0.630)(R 0.562, F 0.698)] [D acc: (0.672)(0.703, 0.641)] [G loss: 0.877] [G acc: 0.125]\n",
      "560 [D loss: (0.625)(R 0.614, F 0.637)] [D acc: (0.672)(0.625, 0.719)] [G loss: 0.876] [G acc: 0.156]\n",
      "561 [D loss: (0.622)(R 0.524, F 0.721)] [D acc: (0.648)(0.703, 0.594)] [G loss: 0.849] [G acc: 0.109]\n",
      "562 [D loss: (0.591)(R 0.571, F 0.611)] [D acc: (0.719)(0.656, 0.781)] [G loss: 0.981] [G acc: 0.156]\n",
      "563 [D loss: (0.654)(R 0.603, F 0.704)] [D acc: (0.602)(0.594, 0.609)] [G loss: 0.885] [G acc: 0.234]\n",
      "564 [D loss: (0.622)(R 0.501, F 0.743)] [D acc: (0.656)(0.766, 0.547)] [G loss: 0.900] [G acc: 0.141]\n",
      "565 [D loss: (0.647)(R 0.593, F 0.701)] [D acc: (0.633)(0.625, 0.641)] [G loss: 0.847] [G acc: 0.219]\n",
      "566 [D loss: (0.603)(R 0.558, F 0.648)] [D acc: (0.711)(0.688, 0.734)] [G loss: 0.845] [G acc: 0.281]\n",
      "567 [D loss: (0.629)(R 0.567, F 0.691)] [D acc: (0.609)(0.641, 0.578)] [G loss: 0.918] [G acc: 0.125]\n",
      "568 [D loss: (0.618)(R 0.507, F 0.729)] [D acc: (0.672)(0.750, 0.594)] [G loss: 0.909] [G acc: 0.141]\n",
      "569 [D loss: (0.591)(R 0.533, F 0.649)] [D acc: (0.688)(0.734, 0.641)] [G loss: 0.976] [G acc: 0.078]\n",
      "570 [D loss: (0.623)(R 0.562, F 0.683)] [D acc: (0.648)(0.656, 0.641)] [G loss: 0.910] [G acc: 0.156]\n",
      "571 [D loss: (0.641)(R 0.621, F 0.661)] [D acc: (0.641)(0.625, 0.656)] [G loss: 0.914] [G acc: 0.172]\n",
      "572 [D loss: (0.662)(R 0.587, F 0.737)] [D acc: (0.609)(0.688, 0.531)] [G loss: 0.906] [G acc: 0.203]\n",
      "573 [D loss: (0.673)(R 0.632, F 0.713)] [D acc: (0.594)(0.609, 0.578)] [G loss: 0.884] [G acc: 0.141]\n",
      "574 [D loss: (0.653)(R 0.625, F 0.682)] [D acc: (0.641)(0.609, 0.672)] [G loss: 0.879] [G acc: 0.125]\n",
      "575 [D loss: (0.692)(R 0.661, F 0.723)] [D acc: (0.586)(0.469, 0.703)] [G loss: 0.885] [G acc: 0.109]\n",
      "576 [D loss: (0.643)(R 0.583, F 0.704)] [D acc: (0.680)(0.688, 0.672)] [G loss: 0.901] [G acc: 0.109]\n",
      "577 [D loss: (0.624)(R 0.617, F 0.632)] [D acc: (0.672)(0.562, 0.781)] [G loss: 0.906] [G acc: 0.219]\n",
      "578 [D loss: (0.638)(R 0.560, F 0.716)] [D acc: (0.688)(0.688, 0.688)] [G loss: 0.876] [G acc: 0.156]\n",
      "579 [D loss: (0.609)(R 0.594, F 0.625)] [D acc: (0.633)(0.531, 0.734)] [G loss: 0.893] [G acc: 0.141]\n",
      "580 [D loss: (0.606)(R 0.545, F 0.667)] [D acc: (0.648)(0.625, 0.672)] [G loss: 0.928] [G acc: 0.172]\n",
      "581 [D loss: (0.663)(R 0.561, F 0.764)] [D acc: (0.586)(0.609, 0.562)] [G loss: 0.919] [G acc: 0.078]\n",
      "582 [D loss: (0.636)(R 0.639, F 0.633)] [D acc: (0.656)(0.594, 0.719)] [G loss: 0.848] [G acc: 0.250]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "583 [D loss: (0.637)(R 0.562, F 0.712)] [D acc: (0.664)(0.703, 0.625)] [G loss: 0.890] [G acc: 0.203]\n",
      "584 [D loss: (0.681)(R 0.605, F 0.756)] [D acc: (0.562)(0.578, 0.547)] [G loss: 0.885] [G acc: 0.141]\n",
      "585 [D loss: (0.607)(R 0.612, F 0.602)] [D acc: (0.648)(0.578, 0.719)] [G loss: 0.928] [G acc: 0.109]\n",
      "586 [D loss: (0.662)(R 0.572, F 0.751)] [D acc: (0.602)(0.625, 0.578)] [G loss: 0.851] [G acc: 0.172]\n",
      "587 [D loss: (0.676)(R 0.581, F 0.770)] [D acc: (0.594)(0.609, 0.578)] [G loss: 0.869] [G acc: 0.188]\n",
      "588 [D loss: (0.615)(R 0.573, F 0.656)] [D acc: (0.672)(0.672, 0.672)] [G loss: 0.895] [G acc: 0.125]\n",
      "589 [D loss: (0.638)(R 0.550, F 0.725)] [D acc: (0.602)(0.672, 0.531)] [G loss: 0.935] [G acc: 0.188]\n",
      "590 [D loss: (0.626)(R 0.585, F 0.667)] [D acc: (0.641)(0.594, 0.688)] [G loss: 0.892] [G acc: 0.250]\n",
      "591 [D loss: (0.639)(R 0.581, F 0.697)] [D acc: (0.625)(0.625, 0.625)] [G loss: 0.859] [G acc: 0.234]\n",
      "592 [D loss: (0.605)(R 0.614, F 0.596)] [D acc: (0.633)(0.547, 0.719)] [G loss: 0.949] [G acc: 0.078]\n",
      "593 [D loss: (0.613)(R 0.534, F 0.692)] [D acc: (0.695)(0.719, 0.672)] [G loss: 0.942] [G acc: 0.188]\n",
      "594 [D loss: (0.657)(R 0.631, F 0.682)] [D acc: (0.594)(0.516, 0.672)] [G loss: 0.927] [G acc: 0.172]\n",
      "595 [D loss: (0.690)(R 0.658, F 0.723)] [D acc: (0.586)(0.547, 0.625)] [G loss: 0.895] [G acc: 0.188]\n",
      "596 [D loss: (0.667)(R 0.628, F 0.705)] [D acc: (0.617)(0.594, 0.641)] [G loss: 0.913] [G acc: 0.109]\n",
      "597 [D loss: (0.621)(R 0.579, F 0.662)] [D acc: (0.656)(0.609, 0.703)] [G loss: 0.837] [G acc: 0.219]\n",
      "598 [D loss: (0.681)(R 0.652, F 0.709)] [D acc: (0.609)(0.594, 0.625)] [G loss: 0.920] [G acc: 0.141]\n",
      "599 [D loss: (0.666)(R 0.625, F 0.707)] [D acc: (0.641)(0.594, 0.688)] [G loss: 0.914] [G acc: 0.094]\n",
      "600 [D loss: (0.670)(R 0.674, F 0.667)] [D acc: (0.648)(0.547, 0.750)] [G loss: 0.890] [G acc: 0.141]\n",
      "601 [D loss: (0.638)(R 0.619, F 0.658)] [D acc: (0.672)(0.594, 0.750)] [G loss: 0.868] [G acc: 0.219]\n",
      "602 [D loss: (0.657)(R 0.658, F 0.655)] [D acc: (0.586)(0.500, 0.672)] [G loss: 0.856] [G acc: 0.188]\n",
      "603 [D loss: (0.611)(R 0.566, F 0.655)] [D acc: (0.742)(0.719, 0.766)] [G loss: 0.889] [G acc: 0.188]\n",
      "604 [D loss: (0.631)(R 0.592, F 0.671)] [D acc: (0.609)(0.594, 0.625)] [G loss: 0.916] [G acc: 0.188]\n",
      "605 [D loss: (0.658)(R 0.604, F 0.712)] [D acc: (0.617)(0.641, 0.594)] [G loss: 0.894] [G acc: 0.125]\n",
      "606 [D loss: (0.650)(R 0.630, F 0.669)] [D acc: (0.586)(0.578, 0.594)] [G loss: 0.900] [G acc: 0.203]\n",
      "607 [D loss: (0.630)(R 0.618, F 0.641)] [D acc: (0.672)(0.609, 0.734)] [G loss: 0.927] [G acc: 0.109]\n",
      "608 [D loss: (0.647)(R 0.624, F 0.671)] [D acc: (0.617)(0.578, 0.656)] [G loss: 0.917] [G acc: 0.125]\n",
      "609 [D loss: (0.621)(R 0.568, F 0.673)] [D acc: (0.648)(0.688, 0.609)] [G loss: 0.929] [G acc: 0.109]\n",
      "610 [D loss: (0.632)(R 0.636, F 0.628)] [D acc: (0.617)(0.500, 0.734)] [G loss: 0.881] [G acc: 0.188]\n",
      "611 [D loss: (0.618)(R 0.565, F 0.671)] [D acc: (0.625)(0.609, 0.641)] [G loss: 0.869] [G acc: 0.219]\n",
      "612 [D loss: (0.612)(R 0.540, F 0.683)] [D acc: (0.656)(0.656, 0.656)] [G loss: 0.861] [G acc: 0.234]\n",
      "613 [D loss: (0.655)(R 0.585, F 0.724)] [D acc: (0.594)(0.625, 0.562)] [G loss: 0.928] [G acc: 0.172]\n",
      "614 [D loss: (0.582)(R 0.487, F 0.676)] [D acc: (0.727)(0.844, 0.609)] [G loss: 0.951] [G acc: 0.156]\n",
      "615 [D loss: (0.700)(R 0.601, F 0.798)] [D acc: (0.570)(0.594, 0.547)] [G loss: 0.903] [G acc: 0.156]\n",
      "616 [D loss: (0.643)(R 0.598, F 0.688)] [D acc: (0.625)(0.625, 0.625)] [G loss: 0.871] [G acc: 0.203]\n",
      "617 [D loss: (0.678)(R 0.576, F 0.780)] [D acc: (0.594)(0.672, 0.516)] [G loss: 0.863] [G acc: 0.266]\n",
      "618 [D loss: (0.615)(R 0.601, F 0.630)] [D acc: (0.680)(0.641, 0.719)] [G loss: 0.977] [G acc: 0.094]\n",
      "619 [D loss: (0.654)(R 0.662, F 0.647)] [D acc: (0.617)(0.531, 0.703)] [G loss: 0.886] [G acc: 0.234]\n",
      "620 [D loss: (0.642)(R 0.642, F 0.642)] [D acc: (0.641)(0.625, 0.656)] [G loss: 0.880] [G acc: 0.250]\n",
      "621 [D loss: (0.671)(R 0.623, F 0.720)] [D acc: (0.562)(0.562, 0.562)] [G loss: 0.881] [G acc: 0.203]\n",
      "622 [D loss: (0.639)(R 0.599, F 0.679)] [D acc: (0.609)(0.625, 0.594)] [G loss: 0.893] [G acc: 0.188]\n",
      "623 [D loss: (0.622)(R 0.582, F 0.661)] [D acc: (0.680)(0.641, 0.719)] [G loss: 0.861] [G acc: 0.203]\n",
      "624 [D loss: (0.606)(R 0.545, F 0.667)] [D acc: (0.719)(0.734, 0.703)] [G loss: 0.943] [G acc: 0.125]\n",
      "625 [D loss: (0.637)(R 0.605, F 0.669)] [D acc: (0.625)(0.609, 0.641)] [G loss: 0.882] [G acc: 0.188]\n",
      "626 [D loss: (0.606)(R 0.612, F 0.599)] [D acc: (0.672)(0.594, 0.750)] [G loss: 0.952] [G acc: 0.156]\n",
      "627 [D loss: (0.621)(R 0.619, F 0.623)] [D acc: (0.633)(0.562, 0.703)] [G loss: 0.878] [G acc: 0.250]\n",
      "628 [D loss: (0.660)(R 0.664, F 0.657)] [D acc: (0.609)(0.562, 0.656)] [G loss: 0.918] [G acc: 0.141]\n",
      "629 [D loss: (0.691)(R 0.594, F 0.788)] [D acc: (0.570)(0.672, 0.469)] [G loss: 0.908] [G acc: 0.141]\n",
      "630 [D loss: (0.637)(R 0.538, F 0.736)] [D acc: (0.641)(0.719, 0.562)] [G loss: 0.920] [G acc: 0.172]\n",
      "631 [D loss: (0.665)(R 0.610, F 0.721)] [D acc: (0.617)(0.562, 0.672)] [G loss: 0.893] [G acc: 0.234]\n",
      "632 [D loss: (0.630)(R 0.563, F 0.697)] [D acc: (0.656)(0.672, 0.641)] [G loss: 0.861] [G acc: 0.188]\n",
      "633 [D loss: (0.602)(R 0.516, F 0.688)] [D acc: (0.625)(0.641, 0.609)] [G loss: 0.923] [G acc: 0.125]\n",
      "634 [D loss: (0.655)(R 0.605, F 0.706)] [D acc: (0.656)(0.641, 0.672)] [G loss: 0.864] [G acc: 0.203]\n",
      "635 [D loss: (0.643)(R 0.492, F 0.793)] [D acc: (0.617)(0.750, 0.484)] [G loss: 0.915] [G acc: 0.141]\n",
      "636 [D loss: (0.650)(R 0.541, F 0.759)] [D acc: (0.617)(0.641, 0.594)] [G loss: 0.933] [G acc: 0.172]\n",
      "637 [D loss: (0.674)(R 0.675, F 0.673)] [D acc: (0.531)(0.453, 0.609)] [G loss: 0.910] [G acc: 0.094]\n",
      "638 [D loss: (0.627)(R 0.561, F 0.692)] [D acc: (0.664)(0.688, 0.641)] [G loss: 0.980] [G acc: 0.047]\n",
      "639 [D loss: (0.621)(R 0.613, F 0.630)] [D acc: (0.602)(0.531, 0.672)] [G loss: 0.903] [G acc: 0.203]\n",
      "640 [D loss: (0.619)(R 0.592, F 0.646)] [D acc: (0.633)(0.594, 0.672)] [G loss: 1.021] [G acc: 0.141]\n",
      "641 [D loss: (0.652)(R 0.643, F 0.661)] [D acc: (0.625)(0.625, 0.625)] [G loss: 0.932] [G acc: 0.125]\n",
      "642 [D loss: (0.616)(R 0.574, F 0.659)] [D acc: (0.664)(0.625, 0.703)] [G loss: 0.932] [G acc: 0.219]\n",
      "643 [D loss: (0.634)(R 0.617, F 0.651)] [D acc: (0.648)(0.562, 0.734)] [G loss: 0.920] [G acc: 0.156]\n",
      "644 [D loss: (0.668)(R 0.635, F 0.701)] [D acc: (0.609)(0.578, 0.641)] [G loss: 0.916] [G acc: 0.141]\n",
      "645 [D loss: (0.674)(R 0.646, F 0.702)] [D acc: (0.617)(0.609, 0.625)] [G loss: 0.869] [G acc: 0.172]\n",
      "646 [D loss: (0.604)(R 0.517, F 0.691)] [D acc: (0.664)(0.703, 0.625)] [G loss: 0.869] [G acc: 0.219]\n",
      "647 [D loss: (0.663)(R 0.637, F 0.688)] [D acc: (0.547)(0.516, 0.578)] [G loss: 0.853] [G acc: 0.266]\n",
      "648 [D loss: (0.683)(R 0.598, F 0.769)] [D acc: (0.594)(0.594, 0.594)] [G loss: 0.877] [G acc: 0.125]\n",
      "649 [D loss: (0.603)(R 0.572, F 0.635)] [D acc: (0.727)(0.734, 0.719)] [G loss: 0.920] [G acc: 0.141]\n",
      "650 [D loss: (0.627)(R 0.575, F 0.679)] [D acc: (0.664)(0.625, 0.703)] [G loss: 0.882] [G acc: 0.188]\n",
      "651 [D loss: (0.585)(R 0.552, F 0.618)] [D acc: (0.711)(0.688, 0.734)] [G loss: 0.964] [G acc: 0.062]\n",
      "652 [D loss: (0.632)(R 0.498, F 0.766)] [D acc: (0.641)(0.719, 0.562)] [G loss: 0.919] [G acc: 0.109]\n",
      "653 [D loss: (0.658)(R 0.601, F 0.714)] [D acc: (0.586)(0.625, 0.547)] [G loss: 0.911] [G acc: 0.156]\n",
      "654 [D loss: (0.667)(R 0.644, F 0.690)] [D acc: (0.555)(0.500, 0.609)] [G loss: 0.909] [G acc: 0.125]\n",
      "655 [D loss: (0.591)(R 0.597, F 0.584)] [D acc: (0.734)(0.625, 0.844)] [G loss: 0.853] [G acc: 0.234]\n",
      "656 [D loss: (0.651)(R 0.554, F 0.747)] [D acc: (0.656)(0.688, 0.625)] [G loss: 0.892] [G acc: 0.188]\n",
      "657 [D loss: (0.696)(R 0.665, F 0.728)] [D acc: (0.555)(0.500, 0.609)] [G loss: 0.959] [G acc: 0.125]\n",
      "658 [D loss: (0.658)(R 0.600, F 0.716)] [D acc: (0.570)(0.562, 0.578)] [G loss: 0.900] [G acc: 0.156]\n",
      "659 [D loss: (0.615)(R 0.606, F 0.624)] [D acc: (0.648)(0.609, 0.688)] [G loss: 0.896] [G acc: 0.188]\n",
      "660 [D loss: (0.661)(R 0.633, F 0.688)] [D acc: (0.609)(0.594, 0.625)] [G loss: 0.884] [G acc: 0.188]\n",
      "661 [D loss: (0.648)(R 0.618, F 0.678)] [D acc: (0.625)(0.578, 0.672)] [G loss: 0.863] [G acc: 0.234]\n",
      "662 [D loss: (0.666)(R 0.642, F 0.689)] [D acc: (0.602)(0.562, 0.641)] [G loss: 0.922] [G acc: 0.219]\n",
      "663 [D loss: (0.641)(R 0.616, F 0.667)] [D acc: (0.609)(0.578, 0.641)] [G loss: 0.860] [G acc: 0.234]\n",
      "664 [D loss: (0.631)(R 0.606, F 0.657)] [D acc: (0.672)(0.672, 0.672)] [G loss: 0.905] [G acc: 0.172]\n",
      "665 [D loss: (0.645)(R 0.609, F 0.680)] [D acc: (0.609)(0.547, 0.672)] [G loss: 0.917] [G acc: 0.203]\n",
      "666 [D loss: (0.602)(R 0.591, F 0.613)] [D acc: (0.719)(0.641, 0.797)] [G loss: 0.923] [G acc: 0.141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "667 [D loss: (0.680)(R 0.603, F 0.756)] [D acc: (0.586)(0.625, 0.547)] [G loss: 0.904] [G acc: 0.203]\n",
      "668 [D loss: (0.631)(R 0.624, F 0.637)] [D acc: (0.680)(0.594, 0.766)] [G loss: 0.898] [G acc: 0.172]\n",
      "669 [D loss: (0.666)(R 0.669, F 0.664)] [D acc: (0.555)(0.438, 0.672)] [G loss: 0.909] [G acc: 0.156]\n",
      "670 [D loss: (0.665)(R 0.670, F 0.660)] [D acc: (0.562)(0.484, 0.641)] [G loss: 0.890] [G acc: 0.203]\n",
      "671 [D loss: (0.686)(R 0.658, F 0.715)] [D acc: (0.578)(0.547, 0.609)] [G loss: 0.915] [G acc: 0.109]\n",
      "672 [D loss: (0.609)(R 0.564, F 0.653)] [D acc: (0.680)(0.656, 0.703)] [G loss: 0.863] [G acc: 0.156]\n",
      "673 [D loss: (0.607)(R 0.553, F 0.660)] [D acc: (0.664)(0.625, 0.703)] [G loss: 0.871] [G acc: 0.203]\n",
      "674 [D loss: (0.614)(R 0.622, F 0.607)] [D acc: (0.656)(0.562, 0.750)] [G loss: 0.882] [G acc: 0.172]\n",
      "675 [D loss: (0.637)(R 0.546, F 0.728)] [D acc: (0.648)(0.688, 0.609)] [G loss: 0.870] [G acc: 0.203]\n",
      "676 [D loss: (0.632)(R 0.592, F 0.673)] [D acc: (0.703)(0.672, 0.734)] [G loss: 0.933] [G acc: 0.172]\n",
      "677 [D loss: (0.618)(R 0.582, F 0.655)] [D acc: (0.656)(0.578, 0.734)] [G loss: 0.875] [G acc: 0.219]\n",
      "678 [D loss: (0.619)(R 0.569, F 0.669)] [D acc: (0.688)(0.656, 0.719)] [G loss: 0.894] [G acc: 0.172]\n",
      "679 [D loss: (0.677)(R 0.632, F 0.722)] [D acc: (0.633)(0.609, 0.656)] [G loss: 0.967] [G acc: 0.172]\n",
      "680 [D loss: (0.625)(R 0.559, F 0.692)] [D acc: (0.688)(0.672, 0.703)] [G loss: 0.870] [G acc: 0.234]\n",
      "681 [D loss: (0.670)(R 0.596, F 0.744)] [D acc: (0.586)(0.609, 0.562)] [G loss: 0.933] [G acc: 0.078]\n",
      "682 [D loss: (0.635)(R 0.604, F 0.666)] [D acc: (0.633)(0.578, 0.688)] [G loss: 0.971] [G acc: 0.125]\n",
      "683 [D loss: (0.627)(R 0.660, F 0.594)] [D acc: (0.633)(0.516, 0.750)] [G loss: 1.004] [G acc: 0.109]\n",
      "684 [D loss: (0.616)(R 0.605, F 0.628)] [D acc: (0.641)(0.547, 0.734)] [G loss: 0.934] [G acc: 0.109]\n",
      "685 [D loss: (0.647)(R 0.594, F 0.701)] [D acc: (0.664)(0.672, 0.656)] [G loss: 0.919] [G acc: 0.109]\n",
      "686 [D loss: (0.646)(R 0.645, F 0.647)] [D acc: (0.586)(0.500, 0.672)] [G loss: 0.940] [G acc: 0.172]\n",
      "687 [D loss: (0.634)(R 0.619, F 0.650)] [D acc: (0.656)(0.609, 0.703)] [G loss: 0.932] [G acc: 0.141]\n",
      "688 [D loss: (0.615)(R 0.587, F 0.642)] [D acc: (0.641)(0.609, 0.672)] [G loss: 0.876] [G acc: 0.172]\n",
      "689 [D loss: (0.643)(R 0.522, F 0.764)] [D acc: (0.609)(0.672, 0.547)] [G loss: 0.926] [G acc: 0.141]\n",
      "690 [D loss: (0.597)(R 0.602, F 0.592)] [D acc: (0.672)(0.547, 0.797)] [G loss: 0.882] [G acc: 0.219]\n",
      "691 [D loss: (0.668)(R 0.596, F 0.741)] [D acc: (0.602)(0.609, 0.594)] [G loss: 0.838] [G acc: 0.250]\n",
      "692 [D loss: (0.630)(R 0.562, F 0.698)] [D acc: (0.648)(0.734, 0.562)] [G loss: 0.918] [G acc: 0.141]\n",
      "693 [D loss: (0.672)(R 0.609, F 0.735)] [D acc: (0.539)(0.547, 0.531)] [G loss: 0.904] [G acc: 0.188]\n",
      "694 [D loss: (0.705)(R 0.700, F 0.711)] [D acc: (0.539)(0.516, 0.562)] [G loss: 0.871] [G acc: 0.234]\n",
      "695 [D loss: (0.652)(R 0.596, F 0.709)] [D acc: (0.586)(0.562, 0.609)] [G loss: 0.913] [G acc: 0.156]\n",
      "696 [D loss: (0.649)(R 0.665, F 0.633)] [D acc: (0.570)(0.516, 0.625)] [G loss: 0.943] [G acc: 0.109]\n",
      "697 [D loss: (0.626)(R 0.610, F 0.641)] [D acc: (0.656)(0.609, 0.703)] [G loss: 0.883] [G acc: 0.172]\n",
      "698 [D loss: (0.606)(R 0.599, F 0.612)] [D acc: (0.672)(0.656, 0.688)] [G loss: 0.896] [G acc: 0.188]\n",
      "699 [D loss: (0.613)(R 0.571, F 0.655)] [D acc: (0.641)(0.656, 0.625)] [G loss: 0.853] [G acc: 0.188]\n",
      "700 [D loss: (0.614)(R 0.570, F 0.658)] [D acc: (0.648)(0.641, 0.656)] [G loss: 0.846] [G acc: 0.172]\n",
      "701 [D loss: (0.653)(R 0.564, F 0.742)] [D acc: (0.656)(0.688, 0.625)] [G loss: 0.916] [G acc: 0.203]\n",
      "702 [D loss: (0.653)(R 0.556, F 0.750)] [D acc: (0.625)(0.625, 0.625)] [G loss: 0.956] [G acc: 0.109]\n",
      "703 [D loss: (0.691)(R 0.656, F 0.726)] [D acc: (0.539)(0.516, 0.562)] [G loss: 0.908] [G acc: 0.141]\n",
      "704 [D loss: (0.621)(R 0.575, F 0.667)] [D acc: (0.680)(0.641, 0.719)] [G loss: 0.889] [G acc: 0.141]\n",
      "705 [D loss: (0.594)(R 0.520, F 0.667)] [D acc: (0.648)(0.703, 0.594)] [G loss: 0.906] [G acc: 0.219]\n",
      "706 [D loss: (0.659)(R 0.551, F 0.766)] [D acc: (0.633)(0.703, 0.562)] [G loss: 0.883] [G acc: 0.203]\n",
      "707 [D loss: (0.700)(R 0.675, F 0.724)] [D acc: (0.602)(0.531, 0.672)] [G loss: 0.912] [G acc: 0.172]\n",
      "708 [D loss: (0.676)(R 0.685, F 0.667)] [D acc: (0.578)(0.484, 0.672)] [G loss: 0.890] [G acc: 0.172]\n",
      "709 [D loss: (0.669)(R 0.651, F 0.688)] [D acc: (0.578)(0.516, 0.641)] [G loss: 0.880] [G acc: 0.141]\n",
      "710 [D loss: (0.682)(R 0.694, F 0.671)] [D acc: (0.586)(0.469, 0.703)] [G loss: 0.908] [G acc: 0.109]\n",
      "711 [D loss: (0.662)(R 0.630, F 0.693)] [D acc: (0.594)(0.547, 0.641)] [G loss: 0.873] [G acc: 0.203]\n",
      "712 [D loss: (0.618)(R 0.572, F 0.663)] [D acc: (0.688)(0.672, 0.703)] [G loss: 0.882] [G acc: 0.203]\n",
      "713 [D loss: (0.632)(R 0.631, F 0.632)] [D acc: (0.664)(0.562, 0.766)] [G loss: 0.888] [G acc: 0.188]\n",
      "714 [D loss: (0.657)(R 0.658, F 0.655)] [D acc: (0.625)(0.594, 0.656)] [G loss: 0.865] [G acc: 0.125]\n",
      "715 [D loss: (0.661)(R 0.647, F 0.675)] [D acc: (0.648)(0.562, 0.734)] [G loss: 0.891] [G acc: 0.125]\n",
      "716 [D loss: (0.645)(R 0.547, F 0.743)] [D acc: (0.617)(0.719, 0.516)] [G loss: 0.843] [G acc: 0.141]\n",
      "717 [D loss: (0.644)(R 0.616, F 0.672)] [D acc: (0.617)(0.609, 0.625)] [G loss: 0.937] [G acc: 0.125]\n",
      "718 [D loss: (0.585)(R 0.528, F 0.643)] [D acc: (0.711)(0.734, 0.688)] [G loss: 0.955] [G acc: 0.156]\n",
      "719 [D loss: (0.624)(R 0.651, F 0.597)] [D acc: (0.641)(0.484, 0.797)] [G loss: 0.953] [G acc: 0.156]\n",
      "720 [D loss: (0.590)(R 0.579, F 0.600)] [D acc: (0.711)(0.641, 0.781)] [G loss: 0.916] [G acc: 0.172]\n",
      "721 [D loss: (0.673)(R 0.583, F 0.762)] [D acc: (0.586)(0.625, 0.547)] [G loss: 1.004] [G acc: 0.172]\n",
      "722 [D loss: (0.627)(R 0.532, F 0.722)] [D acc: (0.609)(0.719, 0.500)] [G loss: 0.900] [G acc: 0.109]\n",
      "723 [D loss: (0.642)(R 0.604, F 0.681)] [D acc: (0.656)(0.641, 0.672)] [G loss: 0.932] [G acc: 0.188]\n",
      "724 [D loss: (0.607)(R 0.548, F 0.665)] [D acc: (0.703)(0.672, 0.734)] [G loss: 0.901] [G acc: 0.266]\n",
      "725 [D loss: (0.651)(R 0.624, F 0.678)] [D acc: (0.602)(0.625, 0.578)] [G loss: 0.927] [G acc: 0.141]\n",
      "726 [D loss: (0.602)(R 0.544, F 0.659)] [D acc: (0.641)(0.688, 0.594)] [G loss: 0.950] [G acc: 0.156]\n",
      "727 [D loss: (0.651)(R 0.663, F 0.640)] [D acc: (0.648)(0.500, 0.797)] [G loss: 0.917] [G acc: 0.156]\n",
      "728 [D loss: (0.657)(R 0.558, F 0.757)] [D acc: (0.633)(0.656, 0.609)] [G loss: 0.908] [G acc: 0.156]\n",
      "729 [D loss: (0.631)(R 0.593, F 0.668)] [D acc: (0.672)(0.641, 0.703)] [G loss: 0.890] [G acc: 0.156]\n",
      "730 [D loss: (0.649)(R 0.624, F 0.674)] [D acc: (0.664)(0.609, 0.719)] [G loss: 0.954] [G acc: 0.156]\n",
      "731 [D loss: (0.636)(R 0.617, F 0.654)] [D acc: (0.633)(0.531, 0.734)] [G loss: 0.890] [G acc: 0.188]\n",
      "732 [D loss: (0.622)(R 0.592, F 0.651)] [D acc: (0.656)(0.609, 0.703)] [G loss: 0.904] [G acc: 0.203]\n",
      "733 [D loss: (0.624)(R 0.573, F 0.674)] [D acc: (0.695)(0.672, 0.719)] [G loss: 0.951] [G acc: 0.141]\n",
      "734 [D loss: (0.643)(R 0.591, F 0.694)] [D acc: (0.609)(0.594, 0.625)] [G loss: 0.932] [G acc: 0.172]\n",
      "735 [D loss: (0.647)(R 0.623, F 0.672)] [D acc: (0.617)(0.578, 0.656)] [G loss: 0.974] [G acc: 0.109]\n",
      "736 [D loss: (0.641)(R 0.618, F 0.665)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.027] [G acc: 0.094]\n",
      "737 [D loss: (0.697)(R 0.677, F 0.717)] [D acc: (0.555)(0.484, 0.625)] [G loss: 0.908] [G acc: 0.172]\n",
      "738 [D loss: (0.660)(R 0.675, F 0.645)] [D acc: (0.641)(0.531, 0.750)] [G loss: 0.923] [G acc: 0.172]\n",
      "739 [D loss: (0.618)(R 0.595, F 0.642)] [D acc: (0.680)(0.672, 0.688)] [G loss: 0.890] [G acc: 0.281]\n",
      "740 [D loss: (0.602)(R 0.588, F 0.616)] [D acc: (0.648)(0.609, 0.688)] [G loss: 0.953] [G acc: 0.141]\n",
      "741 [D loss: (0.658)(R 0.627, F 0.689)] [D acc: (0.641)(0.578, 0.703)] [G loss: 0.956] [G acc: 0.156]\n",
      "742 [D loss: (0.659)(R 0.610, F 0.709)] [D acc: (0.586)(0.562, 0.609)] [G loss: 0.927] [G acc: 0.109]\n",
      "743 [D loss: (0.587)(R 0.575, F 0.598)] [D acc: (0.719)(0.672, 0.766)] [G loss: 0.878] [G acc: 0.219]\n",
      "744 [D loss: (0.631)(R 0.559, F 0.704)] [D acc: (0.672)(0.672, 0.672)] [G loss: 0.879] [G acc: 0.250]\n",
      "745 [D loss: (0.537)(R 0.506, F 0.568)] [D acc: (0.758)(0.734, 0.781)] [G loss: 0.874] [G acc: 0.266]\n",
      "746 [D loss: (0.696)(R 0.567, F 0.824)] [D acc: (0.594)(0.672, 0.516)] [G loss: 0.986] [G acc: 0.156]\n",
      "747 [D loss: (0.569)(R 0.567, F 0.571)] [D acc: (0.688)(0.609, 0.766)] [G loss: 0.955] [G acc: 0.188]\n",
      "748 [D loss: (0.627)(R 0.496, F 0.758)] [D acc: (0.680)(0.766, 0.594)] [G loss: 1.043] [G acc: 0.156]\n",
      "749 [D loss: (0.559)(R 0.545, F 0.574)] [D acc: (0.750)(0.734, 0.766)] [G loss: 1.097] [G acc: 0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750 [D loss: (0.685)(R 0.606, F 0.765)] [D acc: (0.617)(0.641, 0.594)] [G loss: 0.997] [G acc: 0.125]\n",
      "751 [D loss: (0.628)(R 0.602, F 0.654)] [D acc: (0.680)(0.641, 0.719)] [G loss: 0.975] [G acc: 0.188]\n",
      "752 [D loss: (0.594)(R 0.536, F 0.652)] [D acc: (0.656)(0.656, 0.656)] [G loss: 1.007] [G acc: 0.109]\n",
      "753 [D loss: (0.577)(R 0.570, F 0.584)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.021] [G acc: 0.188]\n",
      "754 [D loss: (0.634)(R 0.575, F 0.693)] [D acc: (0.680)(0.672, 0.688)] [G loss: 0.977] [G acc: 0.172]\n",
      "755 [D loss: (0.629)(R 0.558, F 0.700)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.013] [G acc: 0.141]\n",
      "756 [D loss: (0.712)(R 0.665, F 0.759)] [D acc: (0.562)(0.578, 0.547)] [G loss: 0.880] [G acc: 0.234]\n",
      "757 [D loss: (0.601)(R 0.539, F 0.663)] [D acc: (0.688)(0.750, 0.625)] [G loss: 0.955] [G acc: 0.250]\n",
      "758 [D loss: (0.621)(R 0.591, F 0.650)] [D acc: (0.711)(0.688, 0.734)] [G loss: 0.967] [G acc: 0.156]\n",
      "759 [D loss: (0.669)(R 0.604, F 0.733)] [D acc: (0.625)(0.594, 0.656)] [G loss: 1.021] [G acc: 0.125]\n",
      "760 [D loss: (0.637)(R 0.614, F 0.660)] [D acc: (0.594)(0.562, 0.625)] [G loss: 0.961] [G acc: 0.203]\n",
      "761 [D loss: (0.626)(R 0.629, F 0.624)] [D acc: (0.648)(0.578, 0.719)] [G loss: 0.960] [G acc: 0.109]\n",
      "762 [D loss: (0.677)(R 0.585, F 0.768)] [D acc: (0.617)(0.625, 0.609)] [G loss: 0.941] [G acc: 0.125]\n",
      "763 [D loss: (0.630)(R 0.611, F 0.648)] [D acc: (0.648)(0.609, 0.688)] [G loss: 1.052] [G acc: 0.141]\n",
      "764 [D loss: (0.602)(R 0.581, F 0.623)] [D acc: (0.688)(0.641, 0.734)] [G loss: 0.955] [G acc: 0.172]\n",
      "765 [D loss: (0.641)(R 0.549, F 0.733)] [D acc: (0.633)(0.688, 0.578)] [G loss: 1.045] [G acc: 0.109]\n",
      "766 [D loss: (0.604)(R 0.577, F 0.631)] [D acc: (0.633)(0.609, 0.656)] [G loss: 0.994] [G acc: 0.141]\n",
      "767 [D loss: (0.640)(R 0.646, F 0.634)] [D acc: (0.633)(0.562, 0.703)] [G loss: 0.912] [G acc: 0.172]\n",
      "768 [D loss: (0.632)(R 0.627, F 0.638)] [D acc: (0.594)(0.531, 0.656)] [G loss: 0.914] [G acc: 0.266]\n",
      "769 [D loss: (0.615)(R 0.547, F 0.683)] [D acc: (0.641)(0.672, 0.609)] [G loss: 1.014] [G acc: 0.250]\n",
      "770 [D loss: (0.608)(R 0.618, F 0.598)] [D acc: (0.648)(0.547, 0.750)] [G loss: 0.981] [G acc: 0.125]\n",
      "771 [D loss: (0.623)(R 0.616, F 0.629)] [D acc: (0.633)(0.562, 0.703)] [G loss: 0.969] [G acc: 0.156]\n",
      "772 [D loss: (0.571)(R 0.494, F 0.648)] [D acc: (0.727)(0.766, 0.688)] [G loss: 0.995] [G acc: 0.219]\n",
      "773 [D loss: (0.708)(R 0.664, F 0.753)] [D acc: (0.625)(0.578, 0.672)] [G loss: 0.887] [G acc: 0.188]\n",
      "774 [D loss: (0.635)(R 0.586, F 0.684)] [D acc: (0.617)(0.578, 0.656)] [G loss: 0.926] [G acc: 0.219]\n",
      "775 [D loss: (0.685)(R 0.662, F 0.707)] [D acc: (0.578)(0.516, 0.641)] [G loss: 0.903] [G acc: 0.234]\n",
      "776 [D loss: (0.675)(R 0.653, F 0.697)] [D acc: (0.555)(0.547, 0.562)] [G loss: 0.886] [G acc: 0.219]\n",
      "777 [D loss: (0.583)(R 0.587, F 0.579)] [D acc: (0.719)(0.641, 0.797)] [G loss: 0.880] [G acc: 0.203]\n",
      "778 [D loss: (0.578)(R 0.534, F 0.623)] [D acc: (0.688)(0.719, 0.656)] [G loss: 0.956] [G acc: 0.250]\n",
      "779 [D loss: (0.633)(R 0.598, F 0.668)] [D acc: (0.656)(0.672, 0.641)] [G loss: 0.998] [G acc: 0.172]\n",
      "780 [D loss: (0.605)(R 0.584, F 0.625)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.034] [G acc: 0.125]\n",
      "781 [D loss: (0.536)(R 0.495, F 0.577)] [D acc: (0.711)(0.703, 0.719)] [G loss: 0.937] [G acc: 0.281]\n",
      "782 [D loss: (0.568)(R 0.561, F 0.575)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.020] [G acc: 0.125]\n",
      "783 [D loss: (0.702)(R 0.589, F 0.815)] [D acc: (0.586)(0.578, 0.594)] [G loss: 1.017] [G acc: 0.172]\n",
      "784 [D loss: (0.586)(R 0.615, F 0.558)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.007] [G acc: 0.188]\n",
      "785 [D loss: (0.623)(R 0.603, F 0.642)] [D acc: (0.664)(0.656, 0.672)] [G loss: 0.916] [G acc: 0.156]\n",
      "786 [D loss: (0.634)(R 0.562, F 0.706)] [D acc: (0.656)(0.688, 0.625)] [G loss: 1.028] [G acc: 0.109]\n",
      "787 [D loss: (0.655)(R 0.618, F 0.691)] [D acc: (0.641)(0.578, 0.703)] [G loss: 0.922] [G acc: 0.188]\n",
      "788 [D loss: (0.631)(R 0.602, F 0.659)] [D acc: (0.641)(0.594, 0.688)] [G loss: 0.959] [G acc: 0.141]\n",
      "789 [D loss: (0.606)(R 0.582, F 0.631)] [D acc: (0.695)(0.641, 0.750)] [G loss: 0.958] [G acc: 0.188]\n",
      "790 [D loss: (0.613)(R 0.525, F 0.701)] [D acc: (0.641)(0.703, 0.578)] [G loss: 0.930] [G acc: 0.156]\n",
      "791 [D loss: (0.597)(R 0.612, F 0.583)] [D acc: (0.672)(0.594, 0.750)] [G loss: 0.949] [G acc: 0.172]\n",
      "792 [D loss: (0.640)(R 0.532, F 0.748)] [D acc: (0.672)(0.719, 0.625)] [G loss: 0.928] [G acc: 0.203]\n",
      "793 [D loss: (0.617)(R 0.560, F 0.674)] [D acc: (0.633)(0.578, 0.688)] [G loss: 1.017] [G acc: 0.156]\n",
      "794 [D loss: (0.616)(R 0.529, F 0.702)] [D acc: (0.680)(0.703, 0.656)] [G loss: 0.975] [G acc: 0.188]\n",
      "795 [D loss: (0.646)(R 0.634, F 0.658)] [D acc: (0.656)(0.609, 0.703)] [G loss: 0.999] [G acc: 0.125]\n",
      "796 [D loss: (0.593)(R 0.508, F 0.678)] [D acc: (0.711)(0.734, 0.688)] [G loss: 1.025] [G acc: 0.094]\n",
      "797 [D loss: (0.569)(R 0.559, F 0.579)] [D acc: (0.695)(0.609, 0.781)] [G loss: 0.882] [G acc: 0.188]\n",
      "798 [D loss: (0.669)(R 0.607, F 0.732)] [D acc: (0.594)(0.625, 0.562)] [G loss: 0.996] [G acc: 0.125]\n",
      "799 [D loss: (0.638)(R 0.645, F 0.632)] [D acc: (0.648)(0.531, 0.766)] [G loss: 0.901] [G acc: 0.234]\n",
      "800 [D loss: (0.598)(R 0.571, F 0.625)] [D acc: (0.688)(0.625, 0.750)] [G loss: 0.977] [G acc: 0.219]\n",
      "801 [D loss: (0.615)(R 0.583, F 0.646)] [D acc: (0.625)(0.609, 0.641)] [G loss: 0.966] [G acc: 0.156]\n",
      "802 [D loss: (0.652)(R 0.673, F 0.632)] [D acc: (0.648)(0.578, 0.719)] [G loss: 0.888] [G acc: 0.188]\n",
      "803 [D loss: (0.662)(R 0.636, F 0.688)] [D acc: (0.641)(0.578, 0.703)] [G loss: 0.876] [G acc: 0.219]\n",
      "804 [D loss: (0.608)(R 0.574, F 0.642)] [D acc: (0.656)(0.672, 0.641)] [G loss: 1.028] [G acc: 0.188]\n",
      "805 [D loss: (0.627)(R 0.626, F 0.627)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.031] [G acc: 0.125]\n",
      "806 [D loss: (0.628)(R 0.550, F 0.706)] [D acc: (0.547)(0.578, 0.516)] [G loss: 0.938] [G acc: 0.219]\n",
      "807 [D loss: (0.654)(R 0.644, F 0.665)] [D acc: (0.617)(0.578, 0.656)] [G loss: 0.929] [G acc: 0.234]\n",
      "808 [D loss: (0.663)(R 0.609, F 0.717)] [D acc: (0.602)(0.641, 0.562)] [G loss: 0.988] [G acc: 0.109]\n",
      "809 [D loss: (0.609)(R 0.552, F 0.665)] [D acc: (0.703)(0.688, 0.719)] [G loss: 0.932] [G acc: 0.188]\n",
      "810 [D loss: (0.654)(R 0.647, F 0.662)] [D acc: (0.602)(0.516, 0.688)] [G loss: 1.105] [G acc: 0.062]\n",
      "811 [D loss: (0.644)(R 0.633, F 0.654)] [D acc: (0.633)(0.578, 0.688)] [G loss: 0.946] [G acc: 0.188]\n",
      "812 [D loss: (0.594)(R 0.582, F 0.606)] [D acc: (0.727)(0.656, 0.797)] [G loss: 0.955] [G acc: 0.141]\n",
      "813 [D loss: (0.588)(R 0.577, F 0.598)] [D acc: (0.688)(0.641, 0.734)] [G loss: 0.969] [G acc: 0.078]\n",
      "814 [D loss: (0.636)(R 0.600, F 0.672)] [D acc: (0.609)(0.562, 0.656)] [G loss: 0.916] [G acc: 0.156]\n",
      "815 [D loss: (0.548)(R 0.531, F 0.565)] [D acc: (0.703)(0.656, 0.750)] [G loss: 0.928] [G acc: 0.219]\n",
      "816 [D loss: (0.720)(R 0.602, F 0.838)] [D acc: (0.609)(0.594, 0.625)] [G loss: 0.979] [G acc: 0.094]\n",
      "817 [D loss: (0.670)(R 0.626, F 0.714)] [D acc: (0.625)(0.609, 0.641)] [G loss: 0.907] [G acc: 0.188]\n",
      "818 [D loss: (0.596)(R 0.546, F 0.647)] [D acc: (0.695)(0.672, 0.719)] [G loss: 0.922] [G acc: 0.234]\n",
      "819 [D loss: (0.659)(R 0.664, F 0.654)] [D acc: (0.586)(0.516, 0.656)] [G loss: 0.873] [G acc: 0.266]\n",
      "820 [D loss: (0.639)(R 0.610, F 0.667)] [D acc: (0.672)(0.609, 0.734)] [G loss: 0.949] [G acc: 0.203]\n",
      "821 [D loss: (0.663)(R 0.571, F 0.756)] [D acc: (0.633)(0.594, 0.672)] [G loss: 0.937] [G acc: 0.203]\n",
      "822 [D loss: (0.595)(R 0.600, F 0.590)] [D acc: (0.672)(0.578, 0.766)] [G loss: 0.994] [G acc: 0.141]\n",
      "823 [D loss: (0.674)(R 0.653, F 0.696)] [D acc: (0.625)(0.578, 0.672)] [G loss: 0.868] [G acc: 0.266]\n",
      "824 [D loss: (0.627)(R 0.567, F 0.687)] [D acc: (0.633)(0.656, 0.609)] [G loss: 1.046] [G acc: 0.125]\n",
      "825 [D loss: (0.641)(R 0.686, F 0.596)] [D acc: (0.594)(0.469, 0.719)] [G loss: 0.953] [G acc: 0.219]\n",
      "826 [D loss: (0.631)(R 0.647, F 0.614)] [D acc: (0.617)(0.500, 0.734)] [G loss: 1.014] [G acc: 0.125]\n",
      "827 [D loss: (0.586)(R 0.613, F 0.559)] [D acc: (0.719)(0.656, 0.781)] [G loss: 0.918] [G acc: 0.219]\n",
      "828 [D loss: (0.658)(R 0.646, F 0.669)] [D acc: (0.617)(0.562, 0.672)] [G loss: 0.922] [G acc: 0.141]\n",
      "829 [D loss: (0.631)(R 0.586, F 0.677)] [D acc: (0.703)(0.688, 0.719)] [G loss: 1.062] [G acc: 0.125]\n",
      "830 [D loss: (0.608)(R 0.588, F 0.628)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.042] [G acc: 0.125]\n",
      "831 [D loss: (0.607)(R 0.593, F 0.622)] [D acc: (0.695)(0.672, 0.719)] [G loss: 0.953] [G acc: 0.172]\n",
      "832 [D loss: (0.614)(R 0.589, F 0.639)] [D acc: (0.625)(0.625, 0.625)] [G loss: 1.003] [G acc: 0.125]\n",
      "833 [D loss: (0.661)(R 0.589, F 0.733)] [D acc: (0.594)(0.609, 0.578)] [G loss: 0.993] [G acc: 0.141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "834 [D loss: (0.664)(R 0.626, F 0.701)] [D acc: (0.578)(0.562, 0.594)] [G loss: 0.967] [G acc: 0.172]\n",
      "835 [D loss: (0.634)(R 0.605, F 0.663)] [D acc: (0.617)(0.578, 0.656)] [G loss: 1.038] [G acc: 0.078]\n",
      "836 [D loss: (0.523)(R 0.478, F 0.567)] [D acc: (0.766)(0.719, 0.812)] [G loss: 1.067] [G acc: 0.109]\n",
      "837 [D loss: (0.553)(R 0.485, F 0.621)] [D acc: (0.750)(0.750, 0.750)] [G loss: 1.096] [G acc: 0.109]\n",
      "838 [D loss: (0.673)(R 0.561, F 0.784)] [D acc: (0.625)(0.703, 0.547)] [G loss: 1.083] [G acc: 0.156]\n",
      "839 [D loss: (0.594)(R 0.515, F 0.674)] [D acc: (0.695)(0.656, 0.734)] [G loss: 0.987] [G acc: 0.172]\n",
      "840 [D loss: (0.608)(R 0.569, F 0.648)] [D acc: (0.656)(0.656, 0.656)] [G loss: 1.127] [G acc: 0.203]\n",
      "841 [D loss: (0.643)(R 0.688, F 0.598)] [D acc: (0.672)(0.531, 0.812)] [G loss: 0.954] [G acc: 0.125]\n",
      "842 [D loss: (0.618)(R 0.574, F 0.662)] [D acc: (0.633)(0.594, 0.672)] [G loss: 0.984] [G acc: 0.188]\n",
      "843 [D loss: (0.686)(R 0.598, F 0.773)] [D acc: (0.648)(0.625, 0.672)] [G loss: 0.920] [G acc: 0.203]\n",
      "844 [D loss: (0.645)(R 0.622, F 0.669)] [D acc: (0.664)(0.609, 0.719)] [G loss: 0.979] [G acc: 0.141]\n",
      "845 [D loss: (0.686)(R 0.698, F 0.674)] [D acc: (0.539)(0.453, 0.625)] [G loss: 1.002] [G acc: 0.094]\n",
      "846 [D loss: (0.606)(R 0.638, F 0.575)] [D acc: (0.680)(0.594, 0.766)] [G loss: 0.976] [G acc: 0.141]\n",
      "847 [D loss: (0.664)(R 0.627, F 0.702)] [D acc: (0.625)(0.594, 0.656)] [G loss: 1.031] [G acc: 0.125]\n",
      "848 [D loss: (0.659)(R 0.669, F 0.649)] [D acc: (0.586)(0.484, 0.688)] [G loss: 0.934] [G acc: 0.109]\n",
      "849 [D loss: (0.615)(R 0.574, F 0.655)] [D acc: (0.633)(0.625, 0.641)] [G loss: 1.000] [G acc: 0.078]\n",
      "850 [D loss: (0.638)(R 0.584, F 0.692)] [D acc: (0.609)(0.625, 0.594)] [G loss: 0.979] [G acc: 0.156]\n",
      "851 [D loss: (0.599)(R 0.575, F 0.623)] [D acc: (0.664)(0.609, 0.719)] [G loss: 0.939] [G acc: 0.234]\n",
      "852 [D loss: (0.679)(R 0.622, F 0.736)] [D acc: (0.602)(0.578, 0.625)] [G loss: 0.994] [G acc: 0.125]\n",
      "853 [D loss: (0.642)(R 0.641, F 0.643)] [D acc: (0.617)(0.516, 0.719)] [G loss: 0.902] [G acc: 0.203]\n",
      "854 [D loss: (0.623)(R 0.587, F 0.658)] [D acc: (0.664)(0.625, 0.703)] [G loss: 0.992] [G acc: 0.188]\n",
      "855 [D loss: (0.585)(R 0.580, F 0.590)] [D acc: (0.703)(0.641, 0.766)] [G loss: 0.981] [G acc: 0.125]\n",
      "856 [D loss: (0.642)(R 0.573, F 0.711)] [D acc: (0.609)(0.656, 0.562)] [G loss: 0.931] [G acc: 0.125]\n",
      "857 [D loss: (0.606)(R 0.613, F 0.599)] [D acc: (0.695)(0.609, 0.781)] [G loss: 0.927] [G acc: 0.172]\n",
      "858 [D loss: (0.593)(R 0.592, F 0.594)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.026] [G acc: 0.219]\n",
      "859 [D loss: (0.608)(R 0.592, F 0.624)] [D acc: (0.672)(0.594, 0.750)] [G loss: 0.879] [G acc: 0.328]\n",
      "860 [D loss: (0.640)(R 0.615, F 0.665)] [D acc: (0.609)(0.562, 0.656)] [G loss: 0.957] [G acc: 0.188]\n",
      "861 [D loss: (0.648)(R 0.552, F 0.744)] [D acc: (0.633)(0.641, 0.625)] [G loss: 0.961] [G acc: 0.219]\n",
      "862 [D loss: (0.676)(R 0.621, F 0.730)] [D acc: (0.602)(0.547, 0.656)] [G loss: 0.999] [G acc: 0.109]\n",
      "863 [D loss: (0.641)(R 0.649, F 0.633)] [D acc: (0.688)(0.594, 0.781)] [G loss: 0.931] [G acc: 0.141]\n",
      "864 [D loss: (0.593)(R 0.586, F 0.600)] [D acc: (0.695)(0.656, 0.734)] [G loss: 0.936] [G acc: 0.141]\n",
      "865 [D loss: (0.572)(R 0.608, F 0.536)] [D acc: (0.734)(0.625, 0.844)] [G loss: 0.951] [G acc: 0.203]\n",
      "866 [D loss: (0.583)(R 0.520, F 0.646)] [D acc: (0.719)(0.703, 0.734)] [G loss: 0.953] [G acc: 0.188]\n",
      "867 [D loss: (0.658)(R 0.608, F 0.708)] [D acc: (0.594)(0.594, 0.594)] [G loss: 0.904] [G acc: 0.219]\n",
      "868 [D loss: (0.622)(R 0.604, F 0.641)] [D acc: (0.648)(0.656, 0.641)] [G loss: 1.093] [G acc: 0.062]\n",
      "869 [D loss: (0.677)(R 0.677, F 0.677)] [D acc: (0.578)(0.500, 0.656)] [G loss: 0.961] [G acc: 0.094]\n",
      "870 [D loss: (0.644)(R 0.663, F 0.625)] [D acc: (0.594)(0.438, 0.750)] [G loss: 0.983] [G acc: 0.172]\n",
      "871 [D loss: (0.567)(R 0.517, F 0.616)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.007] [G acc: 0.141]\n",
      "872 [D loss: (0.629)(R 0.557, F 0.702)] [D acc: (0.594)(0.609, 0.578)] [G loss: 0.982] [G acc: 0.125]\n",
      "873 [D loss: (0.652)(R 0.634, F 0.670)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.008] [G acc: 0.109]\n",
      "874 [D loss: (0.566)(R 0.553, F 0.579)] [D acc: (0.695)(0.703, 0.688)] [G loss: 1.041] [G acc: 0.156]\n",
      "875 [D loss: (0.606)(R 0.539, F 0.673)] [D acc: (0.719)(0.688, 0.750)] [G loss: 0.969] [G acc: 0.234]\n",
      "876 [D loss: (0.625)(R 0.625, F 0.625)] [D acc: (0.570)(0.516, 0.625)] [G loss: 1.010] [G acc: 0.125]\n",
      "877 [D loss: (0.638)(R 0.536, F 0.740)] [D acc: (0.688)(0.766, 0.609)] [G loss: 0.993] [G acc: 0.172]\n",
      "878 [D loss: (0.651)(R 0.601, F 0.701)] [D acc: (0.648)(0.641, 0.656)] [G loss: 1.019] [G acc: 0.156]\n",
      "879 [D loss: (0.626)(R 0.550, F 0.703)] [D acc: (0.594)(0.594, 0.594)] [G loss: 1.034] [G acc: 0.109]\n",
      "880 [D loss: (0.617)(R 0.525, F 0.708)] [D acc: (0.648)(0.656, 0.641)] [G loss: 1.091] [G acc: 0.078]\n",
      "881 [D loss: (0.603)(R 0.626, F 0.580)] [D acc: (0.648)(0.531, 0.766)] [G loss: 1.061] [G acc: 0.141]\n",
      "882 [D loss: (0.594)(R 0.634, F 0.555)] [D acc: (0.680)(0.562, 0.797)] [G loss: 0.992] [G acc: 0.188]\n",
      "883 [D loss: (0.627)(R 0.630, F 0.625)] [D acc: (0.664)(0.609, 0.719)] [G loss: 0.938] [G acc: 0.219]\n",
      "884 [D loss: (0.576)(R 0.496, F 0.655)] [D acc: (0.719)(0.750, 0.688)] [G loss: 0.981] [G acc: 0.203]\n",
      "885 [D loss: (0.638)(R 0.558, F 0.718)] [D acc: (0.609)(0.625, 0.594)] [G loss: 0.928] [G acc: 0.250]\n",
      "886 [D loss: (0.651)(R 0.640, F 0.662)] [D acc: (0.648)(0.594, 0.703)] [G loss: 0.975] [G acc: 0.188]\n",
      "887 [D loss: (0.637)(R 0.633, F 0.641)] [D acc: (0.586)(0.531, 0.641)] [G loss: 0.894] [G acc: 0.172]\n",
      "888 [D loss: (0.631)(R 0.642, F 0.619)] [D acc: (0.648)(0.516, 0.781)] [G loss: 1.092] [G acc: 0.141]\n",
      "889 [D loss: (0.634)(R 0.648, F 0.621)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.029] [G acc: 0.156]\n",
      "890 [D loss: (0.625)(R 0.630, F 0.621)] [D acc: (0.633)(0.547, 0.719)] [G loss: 1.004] [G acc: 0.125]\n",
      "891 [D loss: (0.639)(R 0.676, F 0.602)] [D acc: (0.617)(0.484, 0.750)] [G loss: 0.990] [G acc: 0.188]\n",
      "892 [D loss: (0.618)(R 0.616, F 0.619)] [D acc: (0.664)(0.625, 0.703)] [G loss: 0.971] [G acc: 0.203]\n",
      "893 [D loss: (0.614)(R 0.602, F 0.625)] [D acc: (0.695)(0.688, 0.703)] [G loss: 0.999] [G acc: 0.156]\n",
      "894 [D loss: (0.659)(R 0.617, F 0.702)] [D acc: (0.594)(0.562, 0.625)] [G loss: 0.926] [G acc: 0.188]\n",
      "895 [D loss: (0.628)(R 0.643, F 0.614)] [D acc: (0.602)(0.500, 0.703)] [G loss: 0.957] [G acc: 0.125]\n",
      "896 [D loss: (0.618)(R 0.599, F 0.636)] [D acc: (0.641)(0.578, 0.703)] [G loss: 0.938] [G acc: 0.156]\n",
      "897 [D loss: (0.621)(R 0.573, F 0.669)] [D acc: (0.656)(0.625, 0.688)] [G loss: 0.947] [G acc: 0.172]\n",
      "898 [D loss: (0.559)(R 0.525, F 0.593)] [D acc: (0.727)(0.734, 0.719)] [G loss: 0.973] [G acc: 0.125]\n",
      "899 [D loss: (0.634)(R 0.670, F 0.598)] [D acc: (0.656)(0.484, 0.828)] [G loss: 0.958] [G acc: 0.172]\n",
      "900 [D loss: (0.649)(R 0.573, F 0.726)] [D acc: (0.641)(0.641, 0.641)] [G loss: 0.970] [G acc: 0.156]\n",
      "901 [D loss: (0.660)(R 0.629, F 0.691)] [D acc: (0.617)(0.547, 0.688)] [G loss: 1.036] [G acc: 0.125]\n",
      "902 [D loss: (0.690)(R 0.682, F 0.699)] [D acc: (0.625)(0.578, 0.672)] [G loss: 0.940] [G acc: 0.141]\n",
      "903 [D loss: (0.593)(R 0.580, F 0.605)] [D acc: (0.711)(0.672, 0.750)] [G loss: 0.997] [G acc: 0.047]\n",
      "904 [D loss: (0.621)(R 0.594, F 0.648)] [D acc: (0.617)(0.531, 0.703)] [G loss: 0.977] [G acc: 0.156]\n",
      "905 [D loss: (0.640)(R 0.615, F 0.665)] [D acc: (0.586)(0.547, 0.625)] [G loss: 0.934] [G acc: 0.234]\n",
      "906 [D loss: (0.576)(R 0.588, F 0.565)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.005] [G acc: 0.188]\n",
      "907 [D loss: (0.678)(R 0.559, F 0.796)] [D acc: (0.594)(0.656, 0.531)] [G loss: 1.002] [G acc: 0.141]\n",
      "908 [D loss: (0.647)(R 0.675, F 0.619)] [D acc: (0.656)(0.562, 0.750)] [G loss: 0.847] [G acc: 0.312]\n",
      "909 [D loss: (0.642)(R 0.648, F 0.635)] [D acc: (0.570)(0.453, 0.688)] [G loss: 0.894] [G acc: 0.203]\n",
      "910 [D loss: (0.620)(R 0.615, F 0.626)] [D acc: (0.633)(0.531, 0.734)] [G loss: 0.910] [G acc: 0.172]\n",
      "911 [D loss: (0.622)(R 0.602, F 0.643)] [D acc: (0.609)(0.531, 0.688)] [G loss: 0.932] [G acc: 0.172]\n",
      "912 [D loss: (0.595)(R 0.536, F 0.654)] [D acc: (0.680)(0.641, 0.719)] [G loss: 0.996] [G acc: 0.188]\n",
      "913 [D loss: (0.670)(R 0.628, F 0.711)] [D acc: (0.602)(0.562, 0.641)] [G loss: 0.965] [G acc: 0.125]\n",
      "914 [D loss: (0.612)(R 0.660, F 0.564)] [D acc: (0.625)(0.469, 0.781)] [G loss: 0.898] [G acc: 0.203]\n",
      "915 [D loss: (0.601)(R 0.579, F 0.622)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.034] [G acc: 0.125]\n",
      "916 [D loss: (0.653)(R 0.641, F 0.666)] [D acc: (0.625)(0.547, 0.703)] [G loss: 0.885] [G acc: 0.234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "917 [D loss: (0.675)(R 0.580, F 0.769)] [D acc: (0.680)(0.734, 0.625)] [G loss: 0.926] [G acc: 0.219]\n",
      "918 [D loss: (0.649)(R 0.574, F 0.723)] [D acc: (0.664)(0.656, 0.672)] [G loss: 0.932] [G acc: 0.109]\n",
      "919 [D loss: (0.608)(R 0.584, F 0.631)] [D acc: (0.664)(0.609, 0.719)] [G loss: 0.915] [G acc: 0.109]\n",
      "920 [D loss: (0.587)(R 0.618, F 0.556)] [D acc: (0.703)(0.562, 0.844)] [G loss: 0.915] [G acc: 0.281]\n",
      "921 [D loss: (0.547)(R 0.473, F 0.622)] [D acc: (0.719)(0.719, 0.719)] [G loss: 0.961] [G acc: 0.156]\n",
      "922 [D loss: (0.676)(R 0.667, F 0.684)] [D acc: (0.648)(0.578, 0.719)] [G loss: 0.936] [G acc: 0.188]\n",
      "923 [D loss: (0.678)(R 0.595, F 0.760)] [D acc: (0.609)(0.641, 0.578)] [G loss: 0.998] [G acc: 0.156]\n",
      "924 [D loss: (0.568)(R 0.567, F 0.570)] [D acc: (0.703)(0.641, 0.766)] [G loss: 0.978] [G acc: 0.156]\n",
      "925 [D loss: (0.661)(R 0.602, F 0.721)] [D acc: (0.578)(0.625, 0.531)] [G loss: 0.974] [G acc: 0.172]\n",
      "926 [D loss: (0.665)(R 0.642, F 0.689)] [D acc: (0.648)(0.578, 0.719)] [G loss: 0.949] [G acc: 0.250]\n",
      "927 [D loss: (0.615)(R 0.605, F 0.624)] [D acc: (0.703)(0.641, 0.766)] [G loss: 0.995] [G acc: 0.188]\n",
      "928 [D loss: (0.605)(R 0.606, F 0.605)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.036] [G acc: 0.156]\n",
      "929 [D loss: (0.636)(R 0.604, F 0.669)] [D acc: (0.633)(0.609, 0.656)] [G loss: 1.015] [G acc: 0.188]\n",
      "930 [D loss: (0.588)(R 0.599, F 0.576)] [D acc: (0.711)(0.656, 0.766)] [G loss: 0.952] [G acc: 0.219]\n",
      "931 [D loss: (0.611)(R 0.525, F 0.696)] [D acc: (0.656)(0.703, 0.609)] [G loss: 1.047] [G acc: 0.203]\n",
      "932 [D loss: (0.668)(R 0.686, F 0.651)] [D acc: (0.586)(0.531, 0.641)] [G loss: 1.024] [G acc: 0.141]\n",
      "933 [D loss: (0.604)(R 0.600, F 0.608)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.015] [G acc: 0.125]\n",
      "934 [D loss: (0.617)(R 0.607, F 0.628)] [D acc: (0.664)(0.609, 0.719)] [G loss: 0.923] [G acc: 0.266]\n",
      "935 [D loss: (0.627)(R 0.625, F 0.629)] [D acc: (0.648)(0.562, 0.734)] [G loss: 0.968] [G acc: 0.141]\n",
      "936 [D loss: (0.593)(R 0.544, F 0.642)] [D acc: (0.711)(0.656, 0.766)] [G loss: 0.911] [G acc: 0.234]\n",
      "937 [D loss: (0.613)(R 0.550, F 0.675)] [D acc: (0.656)(0.547, 0.766)] [G loss: 0.968] [G acc: 0.234]\n",
      "938 [D loss: (0.618)(R 0.558, F 0.677)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.011] [G acc: 0.250]\n",
      "939 [D loss: (0.631)(R 0.603, F 0.660)] [D acc: (0.656)(0.625, 0.688)] [G loss: 0.962] [G acc: 0.172]\n",
      "940 [D loss: (0.698)(R 0.688, F 0.708)] [D acc: (0.570)(0.516, 0.625)] [G loss: 1.003] [G acc: 0.125]\n",
      "941 [D loss: (0.704)(R 0.670, F 0.737)] [D acc: (0.555)(0.484, 0.625)] [G loss: 0.941] [G acc: 0.188]\n",
      "942 [D loss: (0.646)(R 0.658, F 0.633)] [D acc: (0.641)(0.547, 0.734)] [G loss: 0.983] [G acc: 0.078]\n",
      "943 [D loss: (0.637)(R 0.615, F 0.660)] [D acc: (0.680)(0.625, 0.734)] [G loss: 0.976] [G acc: 0.125]\n",
      "944 [D loss: (0.618)(R 0.630, F 0.607)] [D acc: (0.625)(0.516, 0.734)] [G loss: 0.948] [G acc: 0.141]\n",
      "945 [D loss: (0.581)(R 0.596, F 0.565)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.044] [G acc: 0.109]\n",
      "946 [D loss: (0.593)(R 0.585, F 0.601)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.096] [G acc: 0.141]\n",
      "947 [D loss: (0.627)(R 0.573, F 0.682)] [D acc: (0.641)(0.641, 0.641)] [G loss: 0.991] [G acc: 0.156]\n",
      "948 [D loss: (0.648)(R 0.600, F 0.696)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.080] [G acc: 0.141]\n",
      "949 [D loss: (0.620)(R 0.620, F 0.620)] [D acc: (0.641)(0.594, 0.688)] [G loss: 0.911] [G acc: 0.281]\n",
      "950 [D loss: (0.646)(R 0.594, F 0.698)] [D acc: (0.648)(0.609, 0.688)] [G loss: 0.931] [G acc: 0.250]\n",
      "951 [D loss: (0.651)(R 0.626, F 0.675)] [D acc: (0.656)(0.594, 0.719)] [G loss: 0.982] [G acc: 0.188]\n",
      "952 [D loss: (0.637)(R 0.599, F 0.675)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.054] [G acc: 0.109]\n",
      "953 [D loss: (0.639)(R 0.615, F 0.662)] [D acc: (0.648)(0.594, 0.703)] [G loss: 1.036] [G acc: 0.094]\n",
      "954 [D loss: (0.621)(R 0.611, F 0.631)] [D acc: (0.664)(0.625, 0.703)] [G loss: 0.991] [G acc: 0.141]\n",
      "955 [D loss: (0.622)(R 0.586, F 0.657)] [D acc: (0.609)(0.641, 0.578)] [G loss: 1.037] [G acc: 0.125]\n",
      "956 [D loss: (0.630)(R 0.601, F 0.659)] [D acc: (0.641)(0.609, 0.672)] [G loss: 1.032] [G acc: 0.156]\n",
      "957 [D loss: (0.631)(R 0.619, F 0.642)] [D acc: (0.633)(0.562, 0.703)] [G loss: 1.131] [G acc: 0.062]\n",
      "958 [D loss: (0.623)(R 0.637, F 0.610)] [D acc: (0.594)(0.531, 0.656)] [G loss: 0.937] [G acc: 0.203]\n",
      "959 [D loss: (0.651)(R 0.648, F 0.654)] [D acc: (0.656)(0.609, 0.703)] [G loss: 0.954] [G acc: 0.188]\n",
      "960 [D loss: (0.648)(R 0.610, F 0.685)] [D acc: (0.609)(0.562, 0.656)] [G loss: 0.968] [G acc: 0.141]\n",
      "961 [D loss: (0.597)(R 0.619, F 0.575)] [D acc: (0.656)(0.516, 0.797)] [G loss: 0.952] [G acc: 0.109]\n",
      "962 [D loss: (0.610)(R 0.584, F 0.637)] [D acc: (0.664)(0.641, 0.688)] [G loss: 1.007] [G acc: 0.109]\n",
      "963 [D loss: (0.571)(R 0.632, F 0.510)] [D acc: (0.664)(0.484, 0.844)] [G loss: 1.033] [G acc: 0.172]\n",
      "964 [D loss: (0.565)(R 0.576, F 0.553)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.030] [G acc: 0.141]\n",
      "965 [D loss: (0.586)(R 0.524, F 0.648)] [D acc: (0.633)(0.641, 0.625)] [G loss: 1.088] [G acc: 0.156]\n",
      "966 [D loss: (0.631)(R 0.561, F 0.700)] [D acc: (0.617)(0.625, 0.609)] [G loss: 1.117] [G acc: 0.109]\n",
      "967 [D loss: (0.670)(R 0.727, F 0.612)] [D acc: (0.562)(0.438, 0.688)] [G loss: 1.019] [G acc: 0.141]\n",
      "968 [D loss: (0.567)(R 0.549, F 0.586)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.026] [G acc: 0.172]\n",
      "969 [D loss: (0.585)(R 0.538, F 0.631)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.115] [G acc: 0.156]\n",
      "970 [D loss: (0.610)(R 0.603, F 0.617)] [D acc: (0.602)(0.547, 0.656)] [G loss: 1.063] [G acc: 0.078]\n",
      "971 [D loss: (0.690)(R 0.674, F 0.707)] [D acc: (0.586)(0.516, 0.656)] [G loss: 0.988] [G acc: 0.188]\n",
      "972 [D loss: (0.532)(R 0.467, F 0.597)] [D acc: (0.781)(0.781, 0.781)] [G loss: 1.016] [G acc: 0.156]\n",
      "973 [D loss: (0.595)(R 0.606, F 0.584)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.102] [G acc: 0.125]\n",
      "974 [D loss: (0.574)(R 0.554, F 0.594)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.018] [G acc: 0.141]\n",
      "975 [D loss: (0.661)(R 0.666, F 0.655)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.042] [G acc: 0.125]\n",
      "976 [D loss: (0.617)(R 0.617, F 0.616)] [D acc: (0.625)(0.547, 0.703)] [G loss: 0.953] [G acc: 0.203]\n",
      "977 [D loss: (0.708)(R 0.644, F 0.772)] [D acc: (0.578)(0.594, 0.562)] [G loss: 0.987] [G acc: 0.172]\n",
      "978 [D loss: (0.603)(R 0.587, F 0.619)] [D acc: (0.664)(0.641, 0.688)] [G loss: 0.967] [G acc: 0.250]\n",
      "979 [D loss: (0.590)(R 0.565, F 0.614)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.042] [G acc: 0.156]\n",
      "980 [D loss: (0.618)(R 0.651, F 0.585)] [D acc: (0.703)(0.562, 0.844)] [G loss: 1.012] [G acc: 0.203]\n",
      "981 [D loss: (0.635)(R 0.556, F 0.715)] [D acc: (0.648)(0.688, 0.609)] [G loss: 0.931] [G acc: 0.281]\n",
      "982 [D loss: (0.651)(R 0.597, F 0.705)] [D acc: (0.609)(0.594, 0.625)] [G loss: 1.038] [G acc: 0.109]\n",
      "983 [D loss: (0.667)(R 0.697, F 0.637)] [D acc: (0.578)(0.484, 0.672)] [G loss: 0.914] [G acc: 0.219]\n",
      "984 [D loss: (0.602)(R 0.616, F 0.588)] [D acc: (0.695)(0.625, 0.766)] [G loss: 0.956] [G acc: 0.188]\n",
      "985 [D loss: (0.603)(R 0.604, F 0.602)] [D acc: (0.727)(0.672, 0.781)] [G loss: 0.947] [G acc: 0.219]\n",
      "986 [D loss: (0.662)(R 0.632, F 0.692)] [D acc: (0.586)(0.484, 0.688)] [G loss: 0.985] [G acc: 0.125]\n",
      "987 [D loss: (0.593)(R 0.565, F 0.621)] [D acc: (0.672)(0.656, 0.688)] [G loss: 0.992] [G acc: 0.219]\n",
      "988 [D loss: (0.610)(R 0.591, F 0.629)] [D acc: (0.625)(0.609, 0.641)] [G loss: 0.995] [G acc: 0.156]\n",
      "989 [D loss: (0.623)(R 0.640, F 0.606)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.004] [G acc: 0.141]\n",
      "990 [D loss: (0.606)(R 0.585, F 0.627)] [D acc: (0.656)(0.656, 0.656)] [G loss: 1.035] [G acc: 0.094]\n",
      "991 [D loss: (0.555)(R 0.537, F 0.574)] [D acc: (0.727)(0.656, 0.797)] [G loss: 0.983] [G acc: 0.250]\n",
      "992 [D loss: (0.586)(R 0.531, F 0.642)] [D acc: (0.688)(0.703, 0.672)] [G loss: 1.027] [G acc: 0.250]\n",
      "993 [D loss: (0.591)(R 0.446, F 0.737)] [D acc: (0.680)(0.750, 0.609)] [G loss: 1.083] [G acc: 0.172]\n",
      "994 [D loss: (0.547)(R 0.516, F 0.578)] [D acc: (0.711)(0.703, 0.719)] [G loss: 1.207] [G acc: 0.109]\n",
      "995 [D loss: (0.578)(R 0.575, F 0.581)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.142] [G acc: 0.109]\n",
      "996 [D loss: (0.590)(R 0.535, F 0.645)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.123] [G acc: 0.172]\n",
      "997 [D loss: (0.694)(R 0.678, F 0.711)] [D acc: (0.570)(0.531, 0.609)] [G loss: 1.025] [G acc: 0.188]\n",
      "998 [D loss: (0.659)(R 0.581, F 0.737)] [D acc: (0.625)(0.562, 0.688)] [G loss: 1.211] [G acc: 0.062]\n",
      "999 [D loss: (0.580)(R 0.603, F 0.558)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.112] [G acc: 0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 [D loss: (0.654)(R 0.606, F 0.701)] [D acc: (0.633)(0.562, 0.703)] [G loss: 1.018] [G acc: 0.156]\n",
      "1001 [D loss: (0.678)(R 0.671, F 0.686)] [D acc: (0.641)(0.594, 0.688)] [G loss: 1.070] [G acc: 0.078]\n",
      "1002 [D loss: (0.604)(R 0.603, F 0.605)] [D acc: (0.648)(0.594, 0.703)] [G loss: 1.070] [G acc: 0.125]\n",
      "1003 [D loss: (0.593)(R 0.559, F 0.627)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.053] [G acc: 0.125]\n",
      "1004 [D loss: (0.569)(R 0.546, F 0.592)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.091] [G acc: 0.125]\n",
      "1005 [D loss: (0.553)(R 0.589, F 0.516)] [D acc: (0.703)(0.562, 0.844)] [G loss: 1.066] [G acc: 0.109]\n",
      "1006 [D loss: (0.605)(R 0.553, F 0.657)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.091] [G acc: 0.109]\n",
      "1007 [D loss: (0.554)(R 0.485, F 0.623)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.176] [G acc: 0.156]\n",
      "1008 [D loss: (0.600)(R 0.541, F 0.659)] [D acc: (0.664)(0.641, 0.688)] [G loss: 1.153] [G acc: 0.047]\n",
      "1009 [D loss: (0.634)(R 0.706, F 0.563)] [D acc: (0.688)(0.516, 0.859)] [G loss: 1.041] [G acc: 0.125]\n",
      "1010 [D loss: (0.631)(R 0.569, F 0.693)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.158] [G acc: 0.078]\n",
      "1011 [D loss: (0.582)(R 0.575, F 0.588)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.055] [G acc: 0.125]\n",
      "1012 [D loss: (0.548)(R 0.549, F 0.548)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.081] [G acc: 0.141]\n",
      "1013 [D loss: (0.557)(R 0.532, F 0.583)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.046] [G acc: 0.141]\n",
      "1014 [D loss: (0.582)(R 0.516, F 0.649)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.048] [G acc: 0.125]\n",
      "1015 [D loss: (0.516)(R 0.448, F 0.585)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.192] [G acc: 0.125]\n",
      "1016 [D loss: (0.550)(R 0.502, F 0.597)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.174] [G acc: 0.109]\n",
      "1017 [D loss: (0.698)(R 0.640, F 0.757)] [D acc: (0.602)(0.562, 0.641)] [G loss: 1.085] [G acc: 0.078]\n",
      "1018 [D loss: (0.573)(R 0.576, F 0.570)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.140] [G acc: 0.141]\n",
      "1019 [D loss: (0.706)(R 0.532, F 0.880)] [D acc: (0.609)(0.672, 0.547)] [G loss: 1.136] [G acc: 0.094]\n",
      "1020 [D loss: (0.657)(R 0.619, F 0.695)] [D acc: (0.602)(0.594, 0.609)] [G loss: 0.988] [G acc: 0.109]\n",
      "1021 [D loss: (0.659)(R 0.594, F 0.723)] [D acc: (0.617)(0.625, 0.609)] [G loss: 1.120] [G acc: 0.156]\n",
      "1022 [D loss: (0.687)(R 0.633, F 0.740)] [D acc: (0.594)(0.594, 0.594)] [G loss: 1.044] [G acc: 0.156]\n",
      "1023 [D loss: (0.658)(R 0.734, F 0.583)] [D acc: (0.617)(0.500, 0.734)] [G loss: 1.066] [G acc: 0.078]\n",
      "1024 [D loss: (0.588)(R 0.545, F 0.631)] [D acc: (0.695)(0.688, 0.703)] [G loss: 1.067] [G acc: 0.109]\n",
      "1025 [D loss: (0.548)(R 0.527, F 0.570)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.140] [G acc: 0.172]\n",
      "1026 [D loss: (0.602)(R 0.565, F 0.639)] [D acc: (0.672)(0.688, 0.656)] [G loss: 1.057] [G acc: 0.141]\n",
      "1027 [D loss: (0.595)(R 0.536, F 0.655)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.114] [G acc: 0.125]\n",
      "1028 [D loss: (0.606)(R 0.559, F 0.652)] [D acc: (0.664)(0.656, 0.672)] [G loss: 1.063] [G acc: 0.109]\n",
      "1029 [D loss: (0.644)(R 0.633, F 0.654)] [D acc: (0.641)(0.562, 0.719)] [G loss: 1.019] [G acc: 0.172]\n",
      "1030 [D loss: (0.576)(R 0.594, F 0.557)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.030] [G acc: 0.109]\n",
      "1031 [D loss: (0.631)(R 0.625, F 0.637)] [D acc: (0.586)(0.500, 0.672)] [G loss: 1.031] [G acc: 0.141]\n",
      "1032 [D loss: (0.667)(R 0.598, F 0.735)] [D acc: (0.617)(0.625, 0.609)] [G loss: 0.961] [G acc: 0.188]\n",
      "1033 [D loss: (0.640)(R 0.628, F 0.652)] [D acc: (0.602)(0.531, 0.672)] [G loss: 1.083] [G acc: 0.125]\n",
      "1034 [D loss: (0.600)(R 0.632, F 0.568)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.163] [G acc: 0.094]\n",
      "1035 [D loss: (0.631)(R 0.621, F 0.640)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.058] [G acc: 0.141]\n",
      "1036 [D loss: (0.597)(R 0.571, F 0.623)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.031] [G acc: 0.141]\n",
      "1037 [D loss: (0.578)(R 0.569, F 0.587)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.175] [G acc: 0.062]\n",
      "1038 [D loss: (0.569)(R 0.606, F 0.532)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.168] [G acc: 0.125]\n",
      "1039 [D loss: (0.594)(R 0.565, F 0.623)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.130] [G acc: 0.125]\n",
      "1040 [D loss: (0.671)(R 0.588, F 0.754)] [D acc: (0.586)(0.578, 0.594)] [G loss: 1.153] [G acc: 0.141]\n",
      "1041 [D loss: (0.647)(R 0.616, F 0.678)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.193] [G acc: 0.000]\n",
      "1042 [D loss: (0.602)(R 0.636, F 0.567)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.083] [G acc: 0.125]\n",
      "1043 [D loss: (0.554)(R 0.573, F 0.534)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.184] [G acc: 0.094]\n",
      "1044 [D loss: (0.610)(R 0.603, F 0.616)] [D acc: (0.648)(0.609, 0.688)] [G loss: 1.101] [G acc: 0.125]\n",
      "1045 [D loss: (0.675)(R 0.674, F 0.676)] [D acc: (0.594)(0.531, 0.656)] [G loss: 1.095] [G acc: 0.156]\n",
      "1046 [D loss: (0.623)(R 0.607, F 0.640)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.079] [G acc: 0.172]\n",
      "1047 [D loss: (0.634)(R 0.622, F 0.647)] [D acc: (0.625)(0.609, 0.641)] [G loss: 1.046] [G acc: 0.141]\n",
      "1048 [D loss: (0.626)(R 0.663, F 0.589)] [D acc: (0.656)(0.547, 0.766)] [G loss: 1.028] [G acc: 0.141]\n",
      "1049 [D loss: (0.552)(R 0.515, F 0.589)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.117] [G acc: 0.109]\n",
      "1050 [D loss: (0.631)(R 0.578, F 0.685)] [D acc: (0.625)(0.625, 0.625)] [G loss: 1.041] [G acc: 0.219]\n",
      "1051 [D loss: (0.601)(R 0.615, F 0.586)] [D acc: (0.625)(0.578, 0.672)] [G loss: 0.999] [G acc: 0.172]\n",
      "1052 [D loss: (0.611)(R 0.535, F 0.688)] [D acc: (0.656)(0.688, 0.625)] [G loss: 0.927] [G acc: 0.219]\n",
      "1053 [D loss: (0.581)(R 0.540, F 0.622)] [D acc: (0.656)(0.641, 0.672)] [G loss: 0.960] [G acc: 0.219]\n",
      "1054 [D loss: (0.594)(R 0.549, F 0.639)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.008] [G acc: 0.250]\n",
      "1055 [D loss: (0.637)(R 0.679, F 0.595)] [D acc: (0.617)(0.484, 0.750)] [G loss: 1.086] [G acc: 0.125]\n",
      "1056 [D loss: (0.638)(R 0.564, F 0.711)] [D acc: (0.680)(0.703, 0.656)] [G loss: 1.067] [G acc: 0.156]\n",
      "1057 [D loss: (0.580)(R 0.550, F 0.610)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.034] [G acc: 0.203]\n",
      "1058 [D loss: (0.658)(R 0.660, F 0.655)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.037] [G acc: 0.156]\n",
      "1059 [D loss: (0.590)(R 0.567, F 0.613)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.090] [G acc: 0.109]\n",
      "1060 [D loss: (0.643)(R 0.538, F 0.749)] [D acc: (0.625)(0.672, 0.578)] [G loss: 1.063] [G acc: 0.172]\n",
      "1061 [D loss: (0.642)(R 0.712, F 0.573)] [D acc: (0.656)(0.531, 0.781)] [G loss: 0.982] [G acc: 0.188]\n",
      "1062 [D loss: (0.591)(R 0.557, F 0.625)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.125] [G acc: 0.078]\n",
      "1063 [D loss: (0.551)(R 0.563, F 0.540)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.106] [G acc: 0.078]\n",
      "1064 [D loss: (0.610)(R 0.608, F 0.611)] [D acc: (0.648)(0.594, 0.703)] [G loss: 1.092] [G acc: 0.172]\n",
      "1065 [D loss: (0.601)(R 0.591, F 0.611)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.074] [G acc: 0.109]\n",
      "1066 [D loss: (0.561)(R 0.519, F 0.603)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.043] [G acc: 0.156]\n",
      "1067 [D loss: (0.628)(R 0.610, F 0.645)] [D acc: (0.633)(0.609, 0.656)] [G loss: 1.029] [G acc: 0.219]\n",
      "1068 [D loss: (0.704)(R 0.599, F 0.809)] [D acc: (0.617)(0.578, 0.656)] [G loss: 1.038] [G acc: 0.109]\n",
      "1069 [D loss: (0.580)(R 0.553, F 0.608)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.082] [G acc: 0.141]\n",
      "1070 [D loss: (0.606)(R 0.601, F 0.611)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.095] [G acc: 0.172]\n",
      "1071 [D loss: (0.592)(R 0.520, F 0.665)] [D acc: (0.664)(0.672, 0.656)] [G loss: 1.071] [G acc: 0.156]\n",
      "1072 [D loss: (0.562)(R 0.601, F 0.522)] [D acc: (0.734)(0.594, 0.875)] [G loss: 1.051] [G acc: 0.156]\n",
      "1073 [D loss: (0.640)(R 0.630, F 0.651)] [D acc: (0.625)(0.562, 0.688)] [G loss: 1.026] [G acc: 0.188]\n",
      "1074 [D loss: (0.660)(R 0.621, F 0.699)] [D acc: (0.617)(0.547, 0.688)] [G loss: 1.049] [G acc: 0.172]\n",
      "1075 [D loss: (0.598)(R 0.606, F 0.590)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.139] [G acc: 0.125]\n",
      "1076 [D loss: (0.602)(R 0.536, F 0.667)] [D acc: (0.664)(0.672, 0.656)] [G loss: 0.956] [G acc: 0.219]\n",
      "1077 [D loss: (0.654)(R 0.694, F 0.614)] [D acc: (0.625)(0.531, 0.719)] [G loss: 1.089] [G acc: 0.109]\n",
      "1078 [D loss: (0.618)(R 0.658, F 0.578)] [D acc: (0.656)(0.531, 0.781)] [G loss: 1.002] [G acc: 0.109]\n",
      "1079 [D loss: (0.518)(R 0.514, F 0.522)] [D acc: (0.781)(0.703, 0.859)] [G loss: 1.043] [G acc: 0.188]\n",
      "1080 [D loss: (0.571)(R 0.521, F 0.621)] [D acc: (0.703)(0.688, 0.719)] [G loss: 1.119] [G acc: 0.172]\n",
      "1081 [D loss: (0.620)(R 0.498, F 0.742)] [D acc: (0.672)(0.703, 0.641)] [G loss: 0.966] [G acc: 0.219]\n",
      "1082 [D loss: (0.662)(R 0.625, F 0.699)] [D acc: (0.625)(0.625, 0.625)] [G loss: 0.966] [G acc: 0.172]\n",
      "1083 [D loss: (0.634)(R 0.582, F 0.685)] [D acc: (0.641)(0.656, 0.625)] [G loss: 1.016] [G acc: 0.141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1084 [D loss: (0.548)(R 0.544, F 0.553)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.088] [G acc: 0.172]\n",
      "1085 [D loss: (0.587)(R 0.468, F 0.706)] [D acc: (0.711)(0.781, 0.641)] [G loss: 1.028] [G acc: 0.141]\n",
      "1086 [D loss: (0.599)(R 0.585, F 0.613)] [D acc: (0.648)(0.625, 0.672)] [G loss: 1.160] [G acc: 0.156]\n",
      "1087 [D loss: (0.625)(R 0.642, F 0.608)] [D acc: (0.648)(0.500, 0.797)] [G loss: 1.058] [G acc: 0.141]\n",
      "1088 [D loss: (0.577)(R 0.522, F 0.631)] [D acc: (0.719)(0.719, 0.719)] [G loss: 1.284] [G acc: 0.078]\n",
      "1089 [D loss: (0.612)(R 0.614, F 0.610)] [D acc: (0.641)(0.641, 0.641)] [G loss: 1.123] [G acc: 0.141]\n",
      "1090 [D loss: (0.589)(R 0.537, F 0.640)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.120] [G acc: 0.094]\n",
      "1091 [D loss: (0.639)(R 0.614, F 0.663)] [D acc: (0.586)(0.578, 0.594)] [G loss: 1.101] [G acc: 0.094]\n",
      "1092 [D loss: (0.613)(R 0.559, F 0.667)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.195] [G acc: 0.125]\n",
      "1093 [D loss: (0.639)(R 0.713, F 0.566)] [D acc: (0.664)(0.516, 0.812)] [G loss: 1.110] [G acc: 0.016]\n",
      "1094 [D loss: (0.579)(R 0.608, F 0.550)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.092] [G acc: 0.109]\n",
      "1095 [D loss: (0.602)(R 0.553, F 0.651)] [D acc: (0.656)(0.625, 0.688)] [G loss: 1.075] [G acc: 0.125]\n",
      "1096 [D loss: (0.635)(R 0.532, F 0.737)] [D acc: (0.734)(0.734, 0.734)] [G loss: 1.195] [G acc: 0.047]\n",
      "1097 [D loss: (0.598)(R 0.668, F 0.528)] [D acc: (0.656)(0.516, 0.797)] [G loss: 1.167] [G acc: 0.047]\n",
      "1098 [D loss: (0.649)(R 0.733, F 0.565)] [D acc: (0.617)(0.484, 0.750)] [G loss: 1.049] [G acc: 0.109]\n",
      "1099 [D loss: (0.634)(R 0.646, F 0.622)] [D acc: (0.602)(0.484, 0.719)] [G loss: 1.044] [G acc: 0.203]\n",
      "1100 [D loss: (0.563)(R 0.559, F 0.566)] [D acc: (0.680)(0.531, 0.828)] [G loss: 1.118] [G acc: 0.141]\n",
      "1101 [D loss: (0.571)(R 0.546, F 0.597)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.053] [G acc: 0.125]\n",
      "1102 [D loss: (0.616)(R 0.530, F 0.702)] [D acc: (0.617)(0.656, 0.578)] [G loss: 1.051] [G acc: 0.125]\n",
      "1103 [D loss: (0.656)(R 0.562, F 0.750)] [D acc: (0.695)(0.703, 0.688)] [G loss: 1.077] [G acc: 0.125]\n",
      "1104 [D loss: (0.590)(R 0.687, F 0.493)] [D acc: (0.680)(0.484, 0.875)] [G loss: 1.134] [G acc: 0.078]\n",
      "1105 [D loss: (0.625)(R 0.642, F 0.609)] [D acc: (0.641)(0.625, 0.656)] [G loss: 1.057] [G acc: 0.125]\n",
      "1106 [D loss: (0.539)(R 0.487, F 0.590)] [D acc: (0.727)(0.750, 0.703)] [G loss: 1.114] [G acc: 0.078]\n",
      "1107 [D loss: (0.584)(R 0.584, F 0.583)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.087] [G acc: 0.156]\n",
      "1108 [D loss: (0.532)(R 0.480, F 0.583)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.100] [G acc: 0.188]\n",
      "1109 [D loss: (0.613)(R 0.608, F 0.618)] [D acc: (0.656)(0.531, 0.781)] [G loss: 1.083] [G acc: 0.125]\n",
      "1110 [D loss: (0.544)(R 0.483, F 0.604)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.099] [G acc: 0.125]\n",
      "1111 [D loss: (0.639)(R 0.575, F 0.704)] [D acc: (0.680)(0.688, 0.672)] [G loss: 1.081] [G acc: 0.109]\n",
      "1112 [D loss: (0.638)(R 0.618, F 0.659)] [D acc: (0.602)(0.594, 0.609)] [G loss: 1.110] [G acc: 0.141]\n",
      "1113 [D loss: (0.593)(R 0.580, F 0.606)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.046] [G acc: 0.188]\n",
      "1114 [D loss: (0.606)(R 0.508, F 0.704)] [D acc: (0.664)(0.703, 0.625)] [G loss: 1.214] [G acc: 0.109]\n",
      "1115 [D loss: (0.598)(R 0.664, F 0.532)] [D acc: (0.680)(0.531, 0.828)] [G loss: 1.103] [G acc: 0.047]\n",
      "1116 [D loss: (0.638)(R 0.569, F 0.707)] [D acc: (0.617)(0.656, 0.578)] [G loss: 1.195] [G acc: 0.094]\n",
      "1117 [D loss: (0.618)(R 0.627, F 0.608)] [D acc: (0.648)(0.641, 0.656)] [G loss: 1.032] [G acc: 0.188]\n",
      "1118 [D loss: (0.595)(R 0.591, F 0.599)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.023] [G acc: 0.172]\n",
      "1119 [D loss: (0.604)(R 0.645, F 0.564)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.156] [G acc: 0.141]\n",
      "1120 [D loss: (0.630)(R 0.645, F 0.616)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.061] [G acc: 0.047]\n",
      "1121 [D loss: (0.554)(R 0.538, F 0.570)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.050] [G acc: 0.188]\n",
      "1122 [D loss: (0.599)(R 0.586, F 0.612)] [D acc: (0.641)(0.594, 0.688)] [G loss: 1.086] [G acc: 0.188]\n",
      "1123 [D loss: (0.671)(R 0.621, F 0.722)] [D acc: (0.617)(0.562, 0.672)] [G loss: 0.994] [G acc: 0.219]\n",
      "1124 [D loss: (0.587)(R 0.619, F 0.554)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.084] [G acc: 0.078]\n",
      "1125 [D loss: (0.593)(R 0.576, F 0.610)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.015] [G acc: 0.141]\n",
      "1126 [D loss: (0.624)(R 0.673, F 0.575)] [D acc: (0.688)(0.594, 0.781)] [G loss: 0.993] [G acc: 0.125]\n",
      "1127 [D loss: (0.662)(R 0.611, F 0.714)] [D acc: (0.648)(0.625, 0.672)] [G loss: 1.078] [G acc: 0.109]\n",
      "1128 [D loss: (0.631)(R 0.703, F 0.560)] [D acc: (0.648)(0.516, 0.781)] [G loss: 1.038] [G acc: 0.141]\n",
      "1129 [D loss: (0.584)(R 0.574, F 0.593)] [D acc: (0.672)(0.578, 0.766)] [G loss: 0.993] [G acc: 0.125]\n",
      "1130 [D loss: (0.576)(R 0.564, F 0.587)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.083] [G acc: 0.109]\n",
      "1131 [D loss: (0.570)(R 0.616, F 0.525)] [D acc: (0.727)(0.594, 0.859)] [G loss: 1.015] [G acc: 0.188]\n",
      "1132 [D loss: (0.641)(R 0.561, F 0.721)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.099] [G acc: 0.125]\n",
      "1133 [D loss: (0.557)(R 0.524, F 0.590)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.147] [G acc: 0.062]\n",
      "1134 [D loss: (0.646)(R 0.679, F 0.613)] [D acc: (0.633)(0.516, 0.750)] [G loss: 1.097] [G acc: 0.031]\n",
      "1135 [D loss: (0.560)(R 0.599, F 0.522)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.090] [G acc: 0.094]\n",
      "1136 [D loss: (0.584)(R 0.520, F 0.648)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.038] [G acc: 0.203]\n",
      "1137 [D loss: (0.620)(R 0.473, F 0.768)] [D acc: (0.680)(0.719, 0.641)] [G loss: 1.090] [G acc: 0.203]\n",
      "1138 [D loss: (0.589)(R 0.600, F 0.578)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.102] [G acc: 0.109]\n",
      "1139 [D loss: (0.545)(R 0.494, F 0.595)] [D acc: (0.758)(0.750, 0.766)] [G loss: 1.112] [G acc: 0.156]\n",
      "1140 [D loss: (0.602)(R 0.629, F 0.575)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.129] [G acc: 0.141]\n",
      "1141 [D loss: (0.612)(R 0.605, F 0.619)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.018] [G acc: 0.172]\n",
      "1142 [D loss: (0.550)(R 0.509, F 0.592)] [D acc: (0.750)(0.734, 0.766)] [G loss: 1.104] [G acc: 0.172]\n",
      "1143 [D loss: (0.559)(R 0.553, F 0.566)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.131] [G acc: 0.125]\n",
      "1144 [D loss: (0.641)(R 0.604, F 0.677)] [D acc: (0.625)(0.625, 0.625)] [G loss: 1.170] [G acc: 0.141]\n",
      "1145 [D loss: (0.570)(R 0.585, F 0.555)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.072] [G acc: 0.125]\n",
      "1146 [D loss: (0.606)(R 0.569, F 0.644)] [D acc: (0.641)(0.609, 0.672)] [G loss: 1.125] [G acc: 0.125]\n",
      "1147 [D loss: (0.587)(R 0.512, F 0.662)] [D acc: (0.680)(0.688, 0.672)] [G loss: 1.157] [G acc: 0.125]\n",
      "1148 [D loss: (0.585)(R 0.562, F 0.608)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.216] [G acc: 0.062]\n",
      "1149 [D loss: (0.623)(R 0.660, F 0.586)] [D acc: (0.656)(0.547, 0.766)] [G loss: 1.049] [G acc: 0.172]\n",
      "1150 [D loss: (0.592)(R 0.564, F 0.621)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.052] [G acc: 0.172]\n",
      "1151 [D loss: (0.613)(R 0.652, F 0.575)] [D acc: (0.609)(0.516, 0.703)] [G loss: 1.099] [G acc: 0.172]\n",
      "1152 [D loss: (0.567)(R 0.554, F 0.581)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.129] [G acc: 0.109]\n",
      "1153 [D loss: (0.615)(R 0.640, F 0.589)] [D acc: (0.648)(0.562, 0.734)] [G loss: 1.113] [G acc: 0.078]\n",
      "1154 [D loss: (0.650)(R 0.711, F 0.588)] [D acc: (0.641)(0.531, 0.750)] [G loss: 0.974] [G acc: 0.156]\n",
      "1155 [D loss: (0.537)(R 0.481, F 0.592)] [D acc: (0.734)(0.750, 0.719)] [G loss: 1.087] [G acc: 0.188]\n",
      "1156 [D loss: (0.501)(R 0.439, F 0.563)] [D acc: (0.758)(0.781, 0.734)] [G loss: 1.208] [G acc: 0.188]\n",
      "1157 [D loss: (0.683)(R 0.661, F 0.705)] [D acc: (0.602)(0.531, 0.672)] [G loss: 1.134] [G acc: 0.188]\n",
      "1158 [D loss: (0.601)(R 0.602, F 0.600)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.151] [G acc: 0.141]\n",
      "1159 [D loss: (0.559)(R 0.549, F 0.570)] [D acc: (0.719)(0.719, 0.719)] [G loss: 1.103] [G acc: 0.172]\n",
      "1160 [D loss: (0.589)(R 0.574, F 0.604)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.073] [G acc: 0.109]\n",
      "1161 [D loss: (0.575)(R 0.515, F 0.636)] [D acc: (0.711)(0.719, 0.703)] [G loss: 1.091] [G acc: 0.172]\n",
      "1162 [D loss: (0.576)(R 0.618, F 0.533)] [D acc: (0.742)(0.625, 0.859)] [G loss: 1.158] [G acc: 0.094]\n",
      "1163 [D loss: (0.567)(R 0.512, F 0.622)] [D acc: (0.688)(0.688, 0.688)] [G loss: 1.043] [G acc: 0.266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164 [D loss: (0.693)(R 0.663, F 0.722)] [D acc: (0.562)(0.500, 0.625)] [G loss: 1.059] [G acc: 0.188]\n",
      "1165 [D loss: (0.619)(R 0.602, F 0.637)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.224] [G acc: 0.047]\n",
      "1166 [D loss: (0.567)(R 0.513, F 0.621)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.085] [G acc: 0.203]\n",
      "1167 [D loss: (0.632)(R 0.550, F 0.714)] [D acc: (0.641)(0.609, 0.672)] [G loss: 1.018] [G acc: 0.188]\n",
      "1168 [D loss: (0.610)(R 0.653, F 0.567)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.056] [G acc: 0.172]\n",
      "1169 [D loss: (0.503)(R 0.494, F 0.512)] [D acc: (0.781)(0.703, 0.859)] [G loss: 1.108] [G acc: 0.141]\n",
      "1170 [D loss: (0.553)(R 0.592, F 0.514)] [D acc: (0.742)(0.609, 0.875)] [G loss: 1.202] [G acc: 0.125]\n",
      "1171 [D loss: (0.601)(R 0.553, F 0.650)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.145] [G acc: 0.109]\n",
      "1172 [D loss: (0.677)(R 0.735, F 0.619)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.218] [G acc: 0.141]\n",
      "1173 [D loss: (0.609)(R 0.632, F 0.586)] [D acc: (0.641)(0.562, 0.719)] [G loss: 1.111] [G acc: 0.078]\n",
      "1174 [D loss: (0.604)(R 0.604, F 0.604)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.110] [G acc: 0.094]\n",
      "1175 [D loss: (0.586)(R 0.580, F 0.592)] [D acc: (0.664)(0.547, 0.781)] [G loss: 1.064] [G acc: 0.094]\n",
      "1176 [D loss: (0.615)(R 0.620, F 0.609)] [D acc: (0.625)(0.578, 0.672)] [G loss: 1.097] [G acc: 0.172]\n",
      "1177 [D loss: (0.564)(R 0.515, F 0.613)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.084] [G acc: 0.172]\n",
      "1178 [D loss: (0.563)(R 0.468, F 0.659)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.088] [G acc: 0.172]\n",
      "1179 [D loss: (0.588)(R 0.607, F 0.568)] [D acc: (0.664)(0.578, 0.750)] [G loss: 1.124] [G acc: 0.141]\n",
      "1180 [D loss: (0.592)(R 0.600, F 0.584)] [D acc: (0.656)(0.625, 0.688)] [G loss: 1.060] [G acc: 0.156]\n",
      "1181 [D loss: (0.533)(R 0.504, F 0.561)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.181] [G acc: 0.125]\n",
      "1182 [D loss: (0.585)(R 0.486, F 0.685)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.221] [G acc: 0.172]\n",
      "1183 [D loss: (0.517)(R 0.558, F 0.476)] [D acc: (0.781)(0.672, 0.891)] [G loss: 1.227] [G acc: 0.172]\n",
      "1184 [D loss: (0.601)(R 0.554, F 0.649)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.195] [G acc: 0.109]\n",
      "1185 [D loss: (0.504)(R 0.517, F 0.492)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.042] [G acc: 0.203]\n",
      "1186 [D loss: (0.586)(R 0.494, F 0.679)] [D acc: (0.680)(0.719, 0.641)] [G loss: 1.236] [G acc: 0.125]\n",
      "1187 [D loss: (0.622)(R 0.539, F 0.704)] [D acc: (0.648)(0.672, 0.625)] [G loss: 1.202] [G acc: 0.156]\n",
      "1188 [D loss: (0.649)(R 0.595, F 0.703)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.175] [G acc: 0.125]\n",
      "1189 [D loss: (0.694)(R 0.779, F 0.608)] [D acc: (0.648)(0.516, 0.781)] [G loss: 1.103] [G acc: 0.172]\n",
      "1190 [D loss: (0.508)(R 0.517, F 0.499)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.180] [G acc: 0.141]\n",
      "1191 [D loss: (0.656)(R 0.531, F 0.782)] [D acc: (0.648)(0.719, 0.578)] [G loss: 1.064] [G acc: 0.234]\n",
      "1192 [D loss: (0.610)(R 0.638, F 0.582)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.167] [G acc: 0.094]\n",
      "1193 [D loss: (0.560)(R 0.613, F 0.508)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.153] [G acc: 0.125]\n",
      "1194 [D loss: (0.606)(R 0.622, F 0.590)] [D acc: (0.625)(0.547, 0.703)] [G loss: 1.143] [G acc: 0.125]\n",
      "1195 [D loss: (0.618)(R 0.592, F 0.643)] [D acc: (0.641)(0.609, 0.672)] [G loss: 1.061] [G acc: 0.234]\n",
      "1196 [D loss: (0.600)(R 0.577, F 0.623)] [D acc: (0.656)(0.594, 0.719)] [G loss: 1.113] [G acc: 0.094]\n",
      "1197 [D loss: (0.671)(R 0.657, F 0.685)] [D acc: (0.609)(0.562, 0.656)] [G loss: 1.096] [G acc: 0.109]\n",
      "1198 [D loss: (0.567)(R 0.560, F 0.575)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.019] [G acc: 0.203]\n",
      "1199 [D loss: (0.582)(R 0.479, F 0.684)] [D acc: (0.680)(0.719, 0.641)] [G loss: 1.209] [G acc: 0.078]\n",
      "1200 [D loss: (0.619)(R 0.626, F 0.611)] [D acc: (0.641)(0.547, 0.734)] [G loss: 1.046] [G acc: 0.156]\n",
      "1201 [D loss: (0.513)(R 0.526, F 0.500)] [D acc: (0.781)(0.734, 0.828)] [G loss: 1.181] [G acc: 0.172]\n",
      "1202 [D loss: (0.560)(R 0.529, F 0.592)] [D acc: (0.711)(0.719, 0.703)] [G loss: 1.179] [G acc: 0.047]\n",
      "1203 [D loss: (0.580)(R 0.545, F 0.615)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.169] [G acc: 0.156]\n",
      "1204 [D loss: (0.693)(R 0.709, F 0.677)] [D acc: (0.656)(0.547, 0.766)] [G loss: 1.058] [G acc: 0.141]\n",
      "1205 [D loss: (0.536)(R 0.468, F 0.603)] [D acc: (0.734)(0.734, 0.734)] [G loss: 1.156] [G acc: 0.109]\n",
      "1206 [D loss: (0.606)(R 0.621, F 0.591)] [D acc: (0.688)(0.578, 0.797)] [G loss: 1.175] [G acc: 0.109]\n",
      "1207 [D loss: (0.518)(R 0.519, F 0.517)] [D acc: (0.781)(0.719, 0.844)] [G loss: 1.149] [G acc: 0.188]\n",
      "1208 [D loss: (0.557)(R 0.567, F 0.547)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.234] [G acc: 0.094]\n",
      "1209 [D loss: (0.606)(R 0.503, F 0.709)] [D acc: (0.711)(0.719, 0.703)] [G loss: 1.118] [G acc: 0.109]\n",
      "1210 [D loss: (0.594)(R 0.628, F 0.561)] [D acc: (0.648)(0.516, 0.781)] [G loss: 1.099] [G acc: 0.125]\n",
      "1211 [D loss: (0.531)(R 0.518, F 0.543)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.190] [G acc: 0.109]\n",
      "1212 [D loss: (0.591)(R 0.595, F 0.586)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.194] [G acc: 0.078]\n",
      "1213 [D loss: (0.535)(R 0.621, F 0.449)] [D acc: (0.758)(0.609, 0.906)] [G loss: 1.272] [G acc: 0.094]\n",
      "1214 [D loss: (0.567)(R 0.496, F 0.638)] [D acc: (0.664)(0.672, 0.656)] [G loss: 1.077] [G acc: 0.125]\n",
      "1215 [D loss: (0.574)(R 0.520, F 0.627)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.167] [G acc: 0.172]\n",
      "1216 [D loss: (0.580)(R 0.551, F 0.608)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.113] [G acc: 0.094]\n",
      "1217 [D loss: (0.593)(R 0.585, F 0.601)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.216] [G acc: 0.078]\n",
      "1218 [D loss: (0.630)(R 0.613, F 0.647)] [D acc: (0.641)(0.594, 0.688)] [G loss: 1.075] [G acc: 0.156]\n",
      "1219 [D loss: (0.543)(R 0.516, F 0.571)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.092] [G acc: 0.141]\n",
      "1220 [D loss: (0.561)(R 0.543, F 0.580)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.260] [G acc: 0.109]\n",
      "1221 [D loss: (0.564)(R 0.578, F 0.549)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.170] [G acc: 0.125]\n",
      "1222 [D loss: (0.609)(R 0.588, F 0.631)] [D acc: (0.695)(0.719, 0.672)] [G loss: 1.207] [G acc: 0.109]\n",
      "1223 [D loss: (0.567)(R 0.505, F 0.629)] [D acc: (0.664)(0.656, 0.672)] [G loss: 1.201] [G acc: 0.094]\n",
      "1224 [D loss: (0.530)(R 0.549, F 0.512)] [D acc: (0.773)(0.734, 0.812)] [G loss: 1.042] [G acc: 0.250]\n",
      "1225 [D loss: (0.593)(R 0.466, F 0.720)] [D acc: (0.703)(0.719, 0.688)] [G loss: 1.202] [G acc: 0.141]\n",
      "1226 [D loss: (0.565)(R 0.452, F 0.678)] [D acc: (0.719)(0.766, 0.672)] [G loss: 1.153] [G acc: 0.109]\n",
      "1227 [D loss: (0.535)(R 0.540, F 0.530)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.215] [G acc: 0.156]\n",
      "1228 [D loss: (0.632)(R 0.651, F 0.614)] [D acc: (0.625)(0.578, 0.672)] [G loss: 1.096] [G acc: 0.172]\n",
      "1229 [D loss: (0.509)(R 0.534, F 0.484)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.285] [G acc: 0.109]\n",
      "1230 [D loss: (0.641)(R 0.527, F 0.755)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.092] [G acc: 0.172]\n",
      "1231 [D loss: (0.615)(R 0.643, F 0.587)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.122] [G acc: 0.125]\n",
      "1232 [D loss: (0.608)(R 0.491, F 0.725)] [D acc: (0.672)(0.672, 0.672)] [G loss: 1.135] [G acc: 0.188]\n",
      "1233 [D loss: (0.591)(R 0.580, F 0.601)] [D acc: (0.703)(0.641, 0.766)] [G loss: 0.986] [G acc: 0.250]\n",
      "1234 [D loss: (0.614)(R 0.586, F 0.642)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.219] [G acc: 0.094]\n",
      "1235 [D loss: (0.606)(R 0.675, F 0.536)] [D acc: (0.641)(0.500, 0.781)] [G loss: 1.113] [G acc: 0.125]\n",
      "1236 [D loss: (0.561)(R 0.609, F 0.513)] [D acc: (0.664)(0.531, 0.797)] [G loss: 1.204] [G acc: 0.188]\n",
      "1237 [D loss: (0.575)(R 0.563, F 0.587)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.147] [G acc: 0.078]\n",
      "1238 [D loss: (0.619)(R 0.627, F 0.611)] [D acc: (0.648)(0.625, 0.672)] [G loss: 1.161] [G acc: 0.094]\n",
      "1239 [D loss: (0.564)(R 0.572, F 0.555)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.110] [G acc: 0.047]\n",
      "1240 [D loss: (0.599)(R 0.518, F 0.681)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.089] [G acc: 0.094]\n",
      "1241 [D loss: (0.499)(R 0.474, F 0.524)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.186] [G acc: 0.062]\n",
      "1242 [D loss: (0.585)(R 0.454, F 0.716)] [D acc: (0.688)(0.688, 0.688)] [G loss: 1.351] [G acc: 0.094]\n",
      "1243 [D loss: (0.533)(R 0.513, F 0.553)] [D acc: (0.766)(0.672, 0.859)] [G loss: 1.285] [G acc: 0.047]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1244 [D loss: (0.540)(R 0.555, F 0.525)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.205] [G acc: 0.172]\n",
      "1245 [D loss: (0.543)(R 0.533, F 0.553)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.270] [G acc: 0.078]\n",
      "1246 [D loss: (0.620)(R 0.585, F 0.655)] [D acc: (0.633)(0.562, 0.703)] [G loss: 1.195] [G acc: 0.109]\n",
      "1247 [D loss: (0.580)(R 0.464, F 0.695)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.265] [G acc: 0.141]\n",
      "1248 [D loss: (0.629)(R 0.619, F 0.639)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.255] [G acc: 0.062]\n",
      "1249 [D loss: (0.639)(R 0.701, F 0.578)] [D acc: (0.617)(0.500, 0.734)] [G loss: 1.125] [G acc: 0.078]\n",
      "1250 [D loss: (0.589)(R 0.597, F 0.582)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.095] [G acc: 0.188]\n",
      "1251 [D loss: (0.567)(R 0.494, F 0.641)] [D acc: (0.664)(0.688, 0.641)] [G loss: 1.126] [G acc: 0.094]\n",
      "1252 [D loss: (0.598)(R 0.650, F 0.546)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.209] [G acc: 0.109]\n",
      "1253 [D loss: (0.611)(R 0.620, F 0.601)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.168] [G acc: 0.094]\n",
      "1254 [D loss: (0.556)(R 0.563, F 0.550)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.111] [G acc: 0.094]\n",
      "1255 [D loss: (0.591)(R 0.602, F 0.580)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.054] [G acc: 0.141]\n",
      "1256 [D loss: (0.548)(R 0.490, F 0.605)] [D acc: (0.742)(0.750, 0.734)] [G loss: 1.113] [G acc: 0.094]\n",
      "1257 [D loss: (0.533)(R 0.578, F 0.489)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.094] [G acc: 0.219]\n",
      "1258 [D loss: (0.582)(R 0.498, F 0.666)] [D acc: (0.695)(0.703, 0.688)] [G loss: 1.149] [G acc: 0.156]\n",
      "1259 [D loss: (0.636)(R 0.625, F 0.647)] [D acc: (0.664)(0.641, 0.688)] [G loss: 1.052] [G acc: 0.172]\n",
      "1260 [D loss: (0.618)(R 0.668, F 0.569)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.127] [G acc: 0.094]\n",
      "1261 [D loss: (0.618)(R 0.670, F 0.567)] [D acc: (0.625)(0.500, 0.750)] [G loss: 1.477] [G acc: 0.109]\n",
      "1262 [D loss: (0.623)(R 0.745, F 0.501)] [D acc: (0.656)(0.438, 0.875)] [G loss: 1.127] [G acc: 0.109]\n",
      "1263 [D loss: (0.569)(R 0.509, F 0.629)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.155] [G acc: 0.109]\n",
      "1264 [D loss: (0.546)(R 0.585, F 0.507)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.196] [G acc: 0.047]\n",
      "1265 [D loss: (0.560)(R 0.446, F 0.675)] [D acc: (0.703)(0.750, 0.656)] [G loss: 1.017] [G acc: 0.234]\n",
      "1266 [D loss: (0.642)(R 0.559, F 0.725)] [D acc: (0.633)(0.688, 0.578)] [G loss: 1.247] [G acc: 0.094]\n",
      "1267 [D loss: (0.573)(R 0.667, F 0.478)] [D acc: (0.695)(0.516, 0.875)] [G loss: 1.206] [G acc: 0.047]\n",
      "1268 [D loss: (0.525)(R 0.510, F 0.540)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.188] [G acc: 0.203]\n",
      "1269 [D loss: (0.633)(R 0.556, F 0.710)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.148] [G acc: 0.156]\n",
      "1270 [D loss: (0.577)(R 0.581, F 0.572)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.216] [G acc: 0.109]\n",
      "1271 [D loss: (0.619)(R 0.610, F 0.628)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.041] [G acc: 0.219]\n",
      "1272 [D loss: (0.529)(R 0.501, F 0.557)] [D acc: (0.711)(0.703, 0.719)] [G loss: 1.200] [G acc: 0.141]\n",
      "1273 [D loss: (0.560)(R 0.551, F 0.569)] [D acc: (0.734)(0.719, 0.750)] [G loss: 1.239] [G acc: 0.125]\n",
      "1274 [D loss: (0.639)(R 0.691, F 0.587)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.108] [G acc: 0.172]\n",
      "1275 [D loss: (0.644)(R 0.681, F 0.607)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.081] [G acc: 0.141]\n",
      "1276 [D loss: (0.581)(R 0.601, F 0.560)] [D acc: (0.680)(0.531, 0.828)] [G loss: 1.121] [G acc: 0.125]\n",
      "1277 [D loss: (0.572)(R 0.558, F 0.585)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.131] [G acc: 0.125]\n",
      "1278 [D loss: (0.495)(R 0.476, F 0.514)] [D acc: (0.773)(0.719, 0.828)] [G loss: 1.133] [G acc: 0.172]\n",
      "1279 [D loss: (0.650)(R 0.684, F 0.617)] [D acc: (0.609)(0.500, 0.719)] [G loss: 1.052] [G acc: 0.141]\n",
      "1280 [D loss: (0.639)(R 0.535, F 0.742)] [D acc: (0.672)(0.672, 0.672)] [G loss: 1.144] [G acc: 0.078]\n",
      "1281 [D loss: (0.594)(R 0.591, F 0.596)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.160] [G acc: 0.062]\n",
      "1282 [D loss: (0.511)(R 0.544, F 0.478)] [D acc: (0.766)(0.656, 0.875)] [G loss: 1.164] [G acc: 0.109]\n",
      "1283 [D loss: (0.562)(R 0.516, F 0.609)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.152] [G acc: 0.156]\n",
      "1284 [D loss: (0.578)(R 0.550, F 0.605)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.187] [G acc: 0.094]\n",
      "1285 [D loss: (0.553)(R 0.550, F 0.556)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.238] [G acc: 0.047]\n",
      "1286 [D loss: (0.573)(R 0.502, F 0.645)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.256] [G acc: 0.047]\n",
      "1287 [D loss: (0.580)(R 0.695, F 0.465)] [D acc: (0.727)(0.562, 0.891)] [G loss: 1.113] [G acc: 0.125]\n",
      "1288 [D loss: (0.516)(R 0.437, F 0.595)] [D acc: (0.711)(0.734, 0.688)] [G loss: 1.327] [G acc: 0.125]\n",
      "1289 [D loss: (0.623)(R 0.648, F 0.597)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.268] [G acc: 0.125]\n",
      "1290 [D loss: (0.511)(R 0.521, F 0.501)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.191] [G acc: 0.188]\n",
      "1291 [D loss: (0.606)(R 0.596, F 0.616)] [D acc: (0.664)(0.641, 0.688)] [G loss: 1.194] [G acc: 0.109]\n",
      "1292 [D loss: (0.598)(R 0.587, F 0.608)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.083] [G acc: 0.188]\n",
      "1293 [D loss: (0.543)(R 0.476, F 0.611)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.222] [G acc: 0.062]\n",
      "1294 [D loss: (0.560)(R 0.567, F 0.552)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.123] [G acc: 0.109]\n",
      "1295 [D loss: (0.631)(R 0.630, F 0.633)] [D acc: (0.633)(0.562, 0.703)] [G loss: 1.268] [G acc: 0.078]\n",
      "1296 [D loss: (0.611)(R 0.659, F 0.562)] [D acc: (0.664)(0.531, 0.797)] [G loss: 1.197] [G acc: 0.109]\n",
      "1297 [D loss: (0.577)(R 0.515, F 0.640)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.158] [G acc: 0.141]\n",
      "1298 [D loss: (0.627)(R 0.616, F 0.638)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.135] [G acc: 0.094]\n",
      "1299 [D loss: (0.567)(R 0.526, F 0.607)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.276] [G acc: 0.094]\n",
      "1300 [D loss: (0.592)(R 0.579, F 0.605)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.202] [G acc: 0.125]\n",
      "1301 [D loss: (0.525)(R 0.527, F 0.523)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.278] [G acc: 0.094]\n",
      "1302 [D loss: (0.607)(R 0.575, F 0.640)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.136] [G acc: 0.062]\n",
      "1303 [D loss: (0.625)(R 0.667, F 0.582)] [D acc: (0.641)(0.547, 0.734)] [G loss: 1.180] [G acc: 0.141]\n",
      "1304 [D loss: (0.523)(R 0.567, F 0.478)] [D acc: (0.758)(0.688, 0.828)] [G loss: 1.215] [G acc: 0.125]\n",
      "1305 [D loss: (0.570)(R 0.525, F 0.615)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.125] [G acc: 0.156]\n",
      "1306 [D loss: (0.615)(R 0.492, F 0.738)] [D acc: (0.719)(0.750, 0.688)] [G loss: 1.127] [G acc: 0.062]\n",
      "1307 [D loss: (0.587)(R 0.562, F 0.611)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.171] [G acc: 0.172]\n",
      "1308 [D loss: (0.566)(R 0.678, F 0.454)] [D acc: (0.711)(0.531, 0.891)] [G loss: 1.063] [G acc: 0.172]\n",
      "1309 [D loss: (0.579)(R 0.598, F 0.559)] [D acc: (0.727)(0.594, 0.859)] [G loss: 1.212] [G acc: 0.062]\n",
      "1310 [D loss: (0.567)(R 0.525, F 0.610)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.151] [G acc: 0.078]\n",
      "1311 [D loss: (0.556)(R 0.531, F 0.582)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.216] [G acc: 0.109]\n",
      "1312 [D loss: (0.510)(R 0.454, F 0.566)] [D acc: (0.742)(0.781, 0.703)] [G loss: 1.164] [G acc: 0.047]\n",
      "1313 [D loss: (0.538)(R 0.565, F 0.511)] [D acc: (0.766)(0.656, 0.875)] [G loss: 1.251] [G acc: 0.062]\n",
      "1314 [D loss: (0.601)(R 0.571, F 0.631)] [D acc: (0.625)(0.594, 0.656)] [G loss: 1.163] [G acc: 0.125]\n",
      "1315 [D loss: (0.573)(R 0.576, F 0.569)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.146] [G acc: 0.109]\n",
      "1316 [D loss: (0.548)(R 0.578, F 0.518)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.243] [G acc: 0.141]\n",
      "1317 [D loss: (0.498)(R 0.448, F 0.548)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.378] [G acc: 0.078]\n",
      "1318 [D loss: (0.575)(R 0.560, F 0.589)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.225] [G acc: 0.078]\n",
      "1319 [D loss: (0.591)(R 0.534, F 0.649)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.239] [G acc: 0.141]\n",
      "1320 [D loss: (0.532)(R 0.640, F 0.423)] [D acc: (0.727)(0.562, 0.891)] [G loss: 1.277] [G acc: 0.094]\n",
      "1321 [D loss: (0.548)(R 0.537, F 0.559)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.200] [G acc: 0.094]\n",
      "1322 [D loss: (0.611)(R 0.563, F 0.658)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.300] [G acc: 0.141]\n",
      "1323 [D loss: (0.555)(R 0.597, F 0.512)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.233] [G acc: 0.141]\n",
      "1324 [D loss: (0.604)(R 0.642, F 0.567)] [D acc: (0.664)(0.531, 0.797)] [G loss: 1.286] [G acc: 0.078]\n",
      "1325 [D loss: (0.585)(R 0.577, F 0.593)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.154] [G acc: 0.109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1326 [D loss: (0.571)(R 0.493, F 0.648)] [D acc: (0.680)(0.719, 0.641)] [G loss: 1.234] [G acc: 0.047]\n",
      "1327 [D loss: (0.493)(R 0.482, F 0.504)] [D acc: (0.773)(0.734, 0.812)] [G loss: 1.136] [G acc: 0.125]\n",
      "1328 [D loss: (0.529)(R 0.449, F 0.608)] [D acc: (0.734)(0.750, 0.719)] [G loss: 1.268] [G acc: 0.172]\n",
      "1329 [D loss: (0.584)(R 0.641, F 0.526)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.253] [G acc: 0.156]\n",
      "1330 [D loss: (0.573)(R 0.653, F 0.493)] [D acc: (0.703)(0.562, 0.844)] [G loss: 1.313] [G acc: 0.094]\n",
      "1331 [D loss: (0.562)(R 0.501, F 0.623)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.166] [G acc: 0.109]\n",
      "1332 [D loss: (0.541)(R 0.531, F 0.551)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.162] [G acc: 0.125]\n",
      "1333 [D loss: (0.610)(R 0.522, F 0.697)] [D acc: (0.680)(0.719, 0.641)] [G loss: 1.204] [G acc: 0.141]\n",
      "1334 [D loss: (0.602)(R 0.653, F 0.551)] [D acc: (0.672)(0.547, 0.797)] [G loss: 1.206] [G acc: 0.109]\n",
      "1335 [D loss: (0.599)(R 0.554, F 0.644)] [D acc: (0.664)(0.641, 0.688)] [G loss: 1.237] [G acc: 0.062]\n",
      "1336 [D loss: (0.627)(R 0.706, F 0.547)] [D acc: (0.695)(0.547, 0.844)] [G loss: 1.178] [G acc: 0.047]\n",
      "1337 [D loss: (0.534)(R 0.526, F 0.541)] [D acc: (0.789)(0.750, 0.828)] [G loss: 1.200] [G acc: 0.188]\n",
      "1338 [D loss: (0.520)(R 0.530, F 0.510)] [D acc: (0.758)(0.641, 0.875)] [G loss: 1.216] [G acc: 0.125]\n",
      "1339 [D loss: (0.526)(R 0.492, F 0.560)] [D acc: (0.711)(0.703, 0.719)] [G loss: 1.271] [G acc: 0.109]\n",
      "1340 [D loss: (0.576)(R 0.559, F 0.594)] [D acc: (0.703)(0.703, 0.703)] [G loss: 1.223] [G acc: 0.156]\n",
      "1341 [D loss: (0.627)(R 0.597, F 0.657)] [D acc: (0.672)(0.625, 0.719)] [G loss: 1.299] [G acc: 0.094]\n",
      "1342 [D loss: (0.641)(R 0.682, F 0.601)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.242] [G acc: 0.078]\n",
      "1343 [D loss: (0.623)(R 0.616, F 0.631)] [D acc: (0.625)(0.547, 0.703)] [G loss: 1.219] [G acc: 0.047]\n",
      "1344 [D loss: (0.574)(R 0.525, F 0.624)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.227] [G acc: 0.094]\n",
      "1345 [D loss: (0.615)(R 0.752, F 0.477)] [D acc: (0.703)(0.516, 0.891)] [G loss: 1.238] [G acc: 0.062]\n",
      "1346 [D loss: (0.567)(R 0.497, F 0.637)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.124] [G acc: 0.156]\n",
      "1347 [D loss: (0.547)(R 0.540, F 0.554)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.226] [G acc: 0.141]\n",
      "1348 [D loss: (0.560)(R 0.556, F 0.564)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.154] [G acc: 0.219]\n",
      "1349 [D loss: (0.614)(R 0.558, F 0.670)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.188] [G acc: 0.078]\n",
      "1350 [D loss: (0.591)(R 0.562, F 0.619)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.092] [G acc: 0.141]\n",
      "1351 [D loss: (0.587)(R 0.562, F 0.612)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.258] [G acc: 0.094]\n",
      "1352 [D loss: (0.585)(R 0.659, F 0.512)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.166] [G acc: 0.078]\n",
      "1353 [D loss: (0.541)(R 0.526, F 0.556)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.205] [G acc: 0.078]\n",
      "1354 [D loss: (0.572)(R 0.527, F 0.617)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.220] [G acc: 0.094]\n",
      "1355 [D loss: (0.611)(R 0.639, F 0.582)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.240] [G acc: 0.109]\n",
      "1356 [D loss: (0.606)(R 0.599, F 0.614)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.264] [G acc: 0.078]\n",
      "1357 [D loss: (0.581)(R 0.600, F 0.563)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.114] [G acc: 0.141]\n",
      "1358 [D loss: (0.552)(R 0.604, F 0.500)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.126] [G acc: 0.172]\n",
      "1359 [D loss: (0.622)(R 0.678, F 0.565)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.231] [G acc: 0.062]\n",
      "1360 [D loss: (0.568)(R 0.601, F 0.534)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.167] [G acc: 0.141]\n",
      "1361 [D loss: (0.613)(R 0.630, F 0.595)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.263] [G acc: 0.000]\n",
      "1362 [D loss: (0.616)(R 0.604, F 0.627)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.148] [G acc: 0.078]\n",
      "1363 [D loss: (0.621)(R 0.631, F 0.611)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.075] [G acc: 0.125]\n",
      "1364 [D loss: (0.593)(R 0.605, F 0.581)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.257] [G acc: 0.047]\n",
      "1365 [D loss: (0.560)(R 0.578, F 0.543)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.125] [G acc: 0.125]\n",
      "1366 [D loss: (0.551)(R 0.532, F 0.571)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.114] [G acc: 0.109]\n",
      "1367 [D loss: (0.538)(R 0.533, F 0.543)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.083] [G acc: 0.172]\n",
      "1368 [D loss: (0.568)(R 0.537, F 0.598)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.185] [G acc: 0.094]\n",
      "1369 [D loss: (0.504)(R 0.461, F 0.547)] [D acc: (0.758)(0.719, 0.797)] [G loss: 1.161] [G acc: 0.094]\n",
      "1370 [D loss: (0.484)(R 0.530, F 0.438)] [D acc: (0.781)(0.641, 0.922)] [G loss: 1.229] [G acc: 0.141]\n",
      "1371 [D loss: (0.517)(R 0.376, F 0.658)] [D acc: (0.734)(0.781, 0.688)] [G loss: 1.461] [G acc: 0.062]\n",
      "1372 [D loss: (0.468)(R 0.464, F 0.471)] [D acc: (0.789)(0.750, 0.828)] [G loss: 1.412] [G acc: 0.203]\n",
      "1373 [D loss: (0.506)(R 0.445, F 0.566)] [D acc: (0.742)(0.750, 0.734)] [G loss: 1.304] [G acc: 0.109]\n",
      "1374 [D loss: (0.499)(R 0.465, F 0.532)] [D acc: (0.766)(0.734, 0.797)] [G loss: 1.286] [G acc: 0.172]\n",
      "1375 [D loss: (0.603)(R 0.544, F 0.663)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.420] [G acc: 0.094]\n",
      "1376 [D loss: (0.602)(R 0.673, F 0.531)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.342] [G acc: 0.062]\n",
      "1377 [D loss: (0.540)(R 0.567, F 0.513)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.373] [G acc: 0.094]\n",
      "1378 [D loss: (0.520)(R 0.526, F 0.513)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.326] [G acc: 0.141]\n",
      "1379 [D loss: (0.503)(R 0.501, F 0.505)] [D acc: (0.773)(0.688, 0.859)] [G loss: 1.336] [G acc: 0.125]\n",
      "1380 [D loss: (0.563)(R 0.537, F 0.589)] [D acc: (0.711)(0.703, 0.719)] [G loss: 1.152] [G acc: 0.172]\n",
      "1381 [D loss: (0.539)(R 0.538, F 0.540)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.309] [G acc: 0.094]\n",
      "1382 [D loss: (0.543)(R 0.527, F 0.558)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.294] [G acc: 0.094]\n",
      "1383 [D loss: (0.565)(R 0.471, F 0.659)] [D acc: (0.711)(0.703, 0.719)] [G loss: 1.297] [G acc: 0.062]\n",
      "1384 [D loss: (0.582)(R 0.573, F 0.592)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.298] [G acc: 0.094]\n",
      "1385 [D loss: (0.567)(R 0.606, F 0.528)] [D acc: (0.680)(0.531, 0.828)] [G loss: 1.236] [G acc: 0.094]\n",
      "1386 [D loss: (0.534)(R 0.443, F 0.626)] [D acc: (0.766)(0.766, 0.766)] [G loss: 1.328] [G acc: 0.125]\n",
      "1387 [D loss: (0.646)(R 0.706, F 0.586)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.234] [G acc: 0.062]\n",
      "1388 [D loss: (0.518)(R 0.481, F 0.555)] [D acc: (0.773)(0.719, 0.828)] [G loss: 1.187] [G acc: 0.125]\n",
      "1389 [D loss: (0.574)(R 0.458, F 0.690)] [D acc: (0.711)(0.719, 0.703)] [G loss: 1.419] [G acc: 0.078]\n",
      "1390 [D loss: (0.501)(R 0.537, F 0.465)] [D acc: (0.789)(0.688, 0.891)] [G loss: 1.328] [G acc: 0.078]\n",
      "1391 [D loss: (0.615)(R 0.597, F 0.633)] [D acc: (0.648)(0.609, 0.688)] [G loss: 1.229] [G acc: 0.141]\n",
      "1392 [D loss: (0.588)(R 0.611, F 0.566)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.290] [G acc: 0.141]\n",
      "1393 [D loss: (0.571)(R 0.590, F 0.553)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.351] [G acc: 0.031]\n",
      "1394 [D loss: (0.620)(R 0.594, F 0.645)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.214] [G acc: 0.094]\n",
      "1395 [D loss: (0.691)(R 0.638, F 0.745)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.247] [G acc: 0.078]\n",
      "1396 [D loss: (0.604)(R 0.722, F 0.486)] [D acc: (0.727)(0.531, 0.922)] [G loss: 1.187] [G acc: 0.047]\n",
      "1397 [D loss: (0.563)(R 0.648, F 0.477)] [D acc: (0.695)(0.547, 0.844)] [G loss: 1.113] [G acc: 0.156]\n",
      "1398 [D loss: (0.479)(R 0.460, F 0.497)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.327] [G acc: 0.078]\n",
      "1399 [D loss: (0.530)(R 0.550, F 0.509)] [D acc: (0.750)(0.641, 0.859)] [G loss: 1.132] [G acc: 0.125]\n",
      "1400 [D loss: (0.573)(R 0.553, F 0.593)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.335] [G acc: 0.109]\n",
      "1401 [D loss: (0.612)(R 0.709, F 0.515)] [D acc: (0.656)(0.469, 0.844)] [G loss: 1.226] [G acc: 0.094]\n",
      "1402 [D loss: (0.571)(R 0.542, F 0.599)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.280] [G acc: 0.094]\n",
      "1403 [D loss: (0.609)(R 0.745, F 0.474)] [D acc: (0.672)(0.438, 0.906)] [G loss: 1.131] [G acc: 0.156]\n",
      "1404 [D loss: (0.685)(R 0.565, F 0.806)] [D acc: (0.602)(0.578, 0.625)] [G loss: 1.087] [G acc: 0.172]\n",
      "1405 [D loss: (0.589)(R 0.631, F 0.548)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.104] [G acc: 0.156]\n",
      "1406 [D loss: (0.578)(R 0.609, F 0.547)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.073] [G acc: 0.109]\n",
      "1407 [D loss: (0.560)(R 0.485, F 0.636)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.119] [G acc: 0.156]\n",
      "1408 [D loss: (0.612)(R 0.600, F 0.625)] [D acc: (0.625)(0.547, 0.703)] [G loss: 1.083] [G acc: 0.109]\n",
      "1409 [D loss: (0.621)(R 0.665, F 0.577)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.024] [G acc: 0.141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1410 [D loss: (0.591)(R 0.583, F 0.599)] [D acc: (0.664)(0.625, 0.703)] [G loss: 0.993] [G acc: 0.188]\n",
      "1411 [D loss: (0.621)(R 0.540, F 0.703)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.112] [G acc: 0.031]\n",
      "1412 [D loss: (0.591)(R 0.659, F 0.522)] [D acc: (0.648)(0.516, 0.781)] [G loss: 1.075] [G acc: 0.125]\n",
      "1413 [D loss: (0.574)(R 0.592, F 0.556)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.117] [G acc: 0.094]\n",
      "1414 [D loss: (0.599)(R 0.529, F 0.669)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.200] [G acc: 0.062]\n",
      "1415 [D loss: (0.553)(R 0.568, F 0.538)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.107] [G acc: 0.141]\n",
      "1416 [D loss: (0.564)(R 0.575, F 0.552)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.068] [G acc: 0.156]\n",
      "1417 [D loss: (0.507)(R 0.505, F 0.509)] [D acc: (0.773)(0.688, 0.859)] [G loss: 1.175] [G acc: 0.047]\n",
      "1418 [D loss: (0.547)(R 0.520, F 0.573)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.224] [G acc: 0.109]\n",
      "1419 [D loss: (0.630)(R 0.611, F 0.649)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.091] [G acc: 0.141]\n",
      "1420 [D loss: (0.555)(R 0.485, F 0.626)] [D acc: (0.680)(0.703, 0.656)] [G loss: 1.201] [G acc: 0.094]\n",
      "1421 [D loss: (0.557)(R 0.574, F 0.540)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.188] [G acc: 0.094]\n",
      "1422 [D loss: (0.523)(R 0.428, F 0.617)] [D acc: (0.742)(0.750, 0.734)] [G loss: 1.348] [G acc: 0.062]\n",
      "1423 [D loss: (0.628)(R 0.707, F 0.549)] [D acc: (0.695)(0.516, 0.875)] [G loss: 1.358] [G acc: 0.031]\n",
      "1424 [D loss: (0.552)(R 0.577, F 0.528)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.243] [G acc: 0.094]\n",
      "1425 [D loss: (0.632)(R 0.571, F 0.693)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.226] [G acc: 0.109]\n",
      "1426 [D loss: (0.547)(R 0.575, F 0.519)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.262] [G acc: 0.125]\n",
      "1427 [D loss: (0.606)(R 0.541, F 0.671)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.174] [G acc: 0.172]\n",
      "1428 [D loss: (0.515)(R 0.540, F 0.490)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.294] [G acc: 0.125]\n",
      "1429 [D loss: (0.546)(R 0.560, F 0.532)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.334] [G acc: 0.031]\n",
      "1430 [D loss: (0.593)(R 0.477, F 0.709)] [D acc: (0.688)(0.703, 0.672)] [G loss: 1.232] [G acc: 0.156]\n",
      "1431 [D loss: (0.541)(R 0.530, F 0.553)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.248] [G acc: 0.172]\n",
      "1432 [D loss: (0.592)(R 0.645, F 0.539)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.211] [G acc: 0.109]\n",
      "1433 [D loss: (0.536)(R 0.573, F 0.498)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.199] [G acc: 0.156]\n",
      "1434 [D loss: (0.564)(R 0.595, F 0.533)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.252] [G acc: 0.156]\n",
      "1435 [D loss: (0.574)(R 0.557, F 0.590)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.204] [G acc: 0.109]\n",
      "1436 [D loss: (0.502)(R 0.516, F 0.489)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.219] [G acc: 0.078]\n",
      "1437 [D loss: (0.501)(R 0.452, F 0.550)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.200] [G acc: 0.109]\n",
      "1438 [D loss: (0.639)(R 0.581, F 0.697)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.309] [G acc: 0.109]\n",
      "1439 [D loss: (0.675)(R 0.660, F 0.689)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.261] [G acc: 0.156]\n",
      "1440 [D loss: (0.543)(R 0.609, F 0.478)] [D acc: (0.742)(0.609, 0.875)] [G loss: 1.204] [G acc: 0.094]\n",
      "1441 [D loss: (0.575)(R 0.577, F 0.572)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.257] [G acc: 0.078]\n",
      "1442 [D loss: (0.517)(R 0.453, F 0.581)] [D acc: (0.766)(0.734, 0.797)] [G loss: 1.311] [G acc: 0.109]\n",
      "1443 [D loss: (0.609)(R 0.608, F 0.610)] [D acc: (0.633)(0.609, 0.656)] [G loss: 1.346] [G acc: 0.078]\n",
      "1444 [D loss: (0.573)(R 0.562, F 0.583)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.352] [G acc: 0.062]\n",
      "1445 [D loss: (0.577)(R 0.543, F 0.612)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.226] [G acc: 0.031]\n",
      "1446 [D loss: (0.550)(R 0.582, F 0.519)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.318] [G acc: 0.047]\n",
      "1447 [D loss: (0.578)(R 0.525, F 0.631)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.275] [G acc: 0.078]\n",
      "1448 [D loss: (0.542)(R 0.578, F 0.506)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.232] [G acc: 0.141]\n",
      "1449 [D loss: (0.536)(R 0.538, F 0.535)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.062] [G acc: 0.203]\n",
      "1450 [D loss: (0.559)(R 0.524, F 0.593)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.310] [G acc: 0.078]\n",
      "1451 [D loss: (0.547)(R 0.610, F 0.484)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.232] [G acc: 0.188]\n",
      "1452 [D loss: (0.553)(R 0.531, F 0.575)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.289] [G acc: 0.125]\n",
      "1453 [D loss: (0.563)(R 0.557, F 0.568)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.377] [G acc: 0.125]\n",
      "1454 [D loss: (0.638)(R 0.712, F 0.564)] [D acc: (0.656)(0.500, 0.812)] [G loss: 1.248] [G acc: 0.125]\n",
      "1455 [D loss: (0.574)(R 0.504, F 0.645)] [D acc: (0.672)(0.688, 0.656)] [G loss: 1.462] [G acc: 0.109]\n",
      "1456 [D loss: (0.563)(R 0.583, F 0.543)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.293] [G acc: 0.094]\n",
      "1457 [D loss: (0.545)(R 0.588, F 0.502)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.291] [G acc: 0.125]\n",
      "1458 [D loss: (0.603)(R 0.605, F 0.601)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.121] [G acc: 0.125]\n",
      "1459 [D loss: (0.621)(R 0.602, F 0.640)] [D acc: (0.609)(0.562, 0.656)] [G loss: 1.163] [G acc: 0.141]\n",
      "1460 [D loss: (0.616)(R 0.685, F 0.546)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.163] [G acc: 0.125]\n",
      "1461 [D loss: (0.644)(R 0.627, F 0.661)] [D acc: (0.641)(0.562, 0.719)] [G loss: 1.192] [G acc: 0.125]\n",
      "1462 [D loss: (0.602)(R 0.662, F 0.543)] [D acc: (0.633)(0.500, 0.766)] [G loss: 1.102] [G acc: 0.125]\n",
      "1463 [D loss: (0.564)(R 0.490, F 0.639)] [D acc: (0.641)(0.641, 0.641)] [G loss: 1.200] [G acc: 0.062]\n",
      "1464 [D loss: (0.604)(R 0.623, F 0.585)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.237] [G acc: 0.047]\n",
      "1465 [D loss: (0.555)(R 0.612, F 0.499)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.189] [G acc: 0.141]\n",
      "1466 [D loss: (0.537)(R 0.595, F 0.480)] [D acc: (0.766)(0.656, 0.875)] [G loss: 1.163] [G acc: 0.078]\n",
      "1467 [D loss: (0.541)(R 0.514, F 0.568)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.231] [G acc: 0.109]\n",
      "1468 [D loss: (0.571)(R 0.508, F 0.633)] [D acc: (0.664)(0.656, 0.672)] [G loss: 1.138] [G acc: 0.109]\n",
      "1469 [D loss: (0.625)(R 0.530, F 0.720)] [D acc: (0.664)(0.688, 0.641)] [G loss: 1.141] [G acc: 0.203]\n",
      "1470 [D loss: (0.584)(R 0.656, F 0.513)] [D acc: (0.680)(0.500, 0.859)] [G loss: 1.242] [G acc: 0.078]\n",
      "1471 [D loss: (0.579)(R 0.588, F 0.570)] [D acc: (0.664)(0.562, 0.766)] [G loss: 1.249] [G acc: 0.094]\n",
      "1472 [D loss: (0.626)(R 0.701, F 0.552)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.240] [G acc: 0.031]\n",
      "1473 [D loss: (0.503)(R 0.483, F 0.523)] [D acc: (0.805)(0.734, 0.875)] [G loss: 1.190] [G acc: 0.094]\n",
      "1474 [D loss: (0.566)(R 0.534, F 0.598)] [D acc: (0.758)(0.688, 0.828)] [G loss: 1.187] [G acc: 0.125]\n",
      "1475 [D loss: (0.505)(R 0.542, F 0.469)] [D acc: (0.711)(0.578, 0.844)] [G loss: 1.240] [G acc: 0.047]\n",
      "1476 [D loss: (0.577)(R 0.523, F 0.632)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.231] [G acc: 0.062]\n",
      "1477 [D loss: (0.535)(R 0.621, F 0.450)] [D acc: (0.719)(0.547, 0.891)] [G loss: 1.290] [G acc: 0.062]\n",
      "1478 [D loss: (0.586)(R 0.499, F 0.674)] [D acc: (0.766)(0.734, 0.797)] [G loss: 1.311] [G acc: 0.156]\n",
      "1479 [D loss: (0.474)(R 0.505, F 0.443)] [D acc: (0.766)(0.672, 0.859)] [G loss: 1.295] [G acc: 0.094]\n",
      "1480 [D loss: (0.599)(R 0.562, F 0.637)] [D acc: (0.664)(0.656, 0.672)] [G loss: 1.328] [G acc: 0.062]\n",
      "1481 [D loss: (0.576)(R 0.586, F 0.566)] [D acc: (0.625)(0.547, 0.703)] [G loss: 1.318] [G acc: 0.094]\n",
      "1482 [D loss: (0.594)(R 0.663, F 0.525)] [D acc: (0.711)(0.562, 0.859)] [G loss: 1.321] [G acc: 0.047]\n",
      "1483 [D loss: (0.640)(R 0.657, F 0.623)] [D acc: (0.594)(0.547, 0.641)] [G loss: 1.089] [G acc: 0.156]\n",
      "1484 [D loss: (0.554)(R 0.508, F 0.600)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.215] [G acc: 0.094]\n",
      "1485 [D loss: (0.628)(R 0.665, F 0.591)] [D acc: (0.609)(0.484, 0.734)] [G loss: 1.136] [G acc: 0.125]\n",
      "1486 [D loss: (0.556)(R 0.517, F 0.594)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.220] [G acc: 0.047]\n",
      "1487 [D loss: (0.532)(R 0.484, F 0.581)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.143] [G acc: 0.141]\n",
      "1488 [D loss: (0.536)(R 0.518, F 0.554)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.209] [G acc: 0.188]\n",
      "1489 [D loss: (0.540)(R 0.531, F 0.550)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.160] [G acc: 0.078]\n",
      "1490 [D loss: (0.528)(R 0.523, F 0.533)] [D acc: (0.719)(0.656, 0.781)] [G loss: 1.209] [G acc: 0.109]\n",
      "1491 [D loss: (0.596)(R 0.623, F 0.570)] [D acc: (0.648)(0.531, 0.766)] [G loss: 1.194] [G acc: 0.203]\n",
      "1492 [D loss: (0.530)(R 0.533, F 0.527)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.043] [G acc: 0.219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1493 [D loss: (0.502)(R 0.415, F 0.589)] [D acc: (0.719)(0.734, 0.703)] [G loss: 1.132] [G acc: 0.141]\n",
      "1494 [D loss: (0.503)(R 0.451, F 0.554)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.251] [G acc: 0.062]\n",
      "1495 [D loss: (0.559)(R 0.631, F 0.487)] [D acc: (0.727)(0.594, 0.859)] [G loss: 1.257] [G acc: 0.094]\n",
      "1496 [D loss: (0.503)(R 0.444, F 0.562)] [D acc: (0.766)(0.750, 0.781)] [G loss: 1.363] [G acc: 0.109]\n",
      "1497 [D loss: (0.573)(R 0.537, F 0.608)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.260] [G acc: 0.094]\n",
      "1498 [D loss: (0.528)(R 0.544, F 0.511)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.232] [G acc: 0.188]\n",
      "1499 [D loss: (0.516)(R 0.389, F 0.643)] [D acc: (0.781)(0.797, 0.766)] [G loss: 1.318] [G acc: 0.062]\n",
      "1500 [D loss: (0.596)(R 0.663, F 0.530)] [D acc: (0.680)(0.531, 0.828)] [G loss: 1.154] [G acc: 0.094]\n",
      "1501 [D loss: (0.526)(R 0.467, F 0.584)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.234] [G acc: 0.062]\n",
      "1502 [D loss: (0.566)(R 0.565, F 0.567)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.273] [G acc: 0.078]\n",
      "1503 [D loss: (0.575)(R 0.634, F 0.515)] [D acc: (0.664)(0.500, 0.828)] [G loss: 1.245] [G acc: 0.141]\n",
      "1504 [D loss: (0.510)(R 0.499, F 0.522)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.302] [G acc: 0.109]\n",
      "1505 [D loss: (0.604)(R 0.560, F 0.647)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.150] [G acc: 0.109]\n",
      "1506 [D loss: (0.562)(R 0.636, F 0.487)] [D acc: (0.711)(0.578, 0.844)] [G loss: 1.092] [G acc: 0.219]\n",
      "1507 [D loss: (0.616)(R 0.473, F 0.759)] [D acc: (0.648)(0.656, 0.641)] [G loss: 1.343] [G acc: 0.172]\n",
      "1508 [D loss: (0.581)(R 0.603, F 0.559)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.186] [G acc: 0.172]\n",
      "1509 [D loss: (0.594)(R 0.614, F 0.574)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.181] [G acc: 0.078]\n",
      "1510 [D loss: (0.462)(R 0.435, F 0.490)] [D acc: (0.789)(0.750, 0.828)] [G loss: 1.284] [G acc: 0.172]\n",
      "1511 [D loss: (0.652)(R 0.564, F 0.740)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.133] [G acc: 0.125]\n",
      "1512 [D loss: (0.516)(R 0.447, F 0.585)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.423] [G acc: 0.078]\n",
      "1513 [D loss: (0.598)(R 0.704, F 0.492)] [D acc: (0.695)(0.516, 0.875)] [G loss: 1.222] [G acc: 0.109]\n",
      "1514 [D loss: (0.546)(R 0.505, F 0.587)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.258] [G acc: 0.109]\n",
      "1515 [D loss: (0.555)(R 0.455, F 0.655)] [D acc: (0.711)(0.703, 0.719)] [G loss: 1.268] [G acc: 0.094]\n",
      "1516 [D loss: (0.553)(R 0.587, F 0.519)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.246] [G acc: 0.078]\n",
      "1517 [D loss: (0.573)(R 0.555, F 0.591)] [D acc: (0.695)(0.688, 0.703)] [G loss: 1.306] [G acc: 0.078]\n",
      "1518 [D loss: (0.550)(R 0.657, F 0.444)] [D acc: (0.711)(0.562, 0.859)] [G loss: 1.246] [G acc: 0.078]\n",
      "1519 [D loss: (0.531)(R 0.568, F 0.494)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.244] [G acc: 0.156]\n",
      "1520 [D loss: (0.554)(R 0.508, F 0.600)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.286] [G acc: 0.125]\n",
      "1521 [D loss: (0.541)(R 0.522, F 0.561)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.404] [G acc: 0.047]\n",
      "1522 [D loss: (0.473)(R 0.502, F 0.445)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.244] [G acc: 0.125]\n",
      "1523 [D loss: (0.535)(R 0.598, F 0.472)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.204] [G acc: 0.062]\n",
      "1524 [D loss: (0.517)(R 0.540, F 0.495)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.341] [G acc: 0.062]\n",
      "1525 [D loss: (0.506)(R 0.505, F 0.508)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.324] [G acc: 0.109]\n",
      "1526 [D loss: (0.528)(R 0.500, F 0.556)] [D acc: (0.695)(0.688, 0.703)] [G loss: 1.456] [G acc: 0.094]\n",
      "1527 [D loss: (0.489)(R 0.565, F 0.414)] [D acc: (0.789)(0.625, 0.953)] [G loss: 1.294] [G acc: 0.094]\n",
      "1528 [D loss: (0.564)(R 0.402, F 0.726)] [D acc: (0.773)(0.781, 0.766)] [G loss: 1.408] [G acc: 0.172]\n",
      "1529 [D loss: (0.533)(R 0.506, F 0.559)] [D acc: (0.766)(0.750, 0.781)] [G loss: 1.441] [G acc: 0.047]\n",
      "1530 [D loss: (0.512)(R 0.572, F 0.451)] [D acc: (0.766)(0.672, 0.859)] [G loss: 1.421] [G acc: 0.109]\n",
      "1531 [D loss: (0.529)(R 0.529, F 0.530)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.336] [G acc: 0.125]\n",
      "1532 [D loss: (0.561)(R 0.587, F 0.535)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.205] [G acc: 0.109]\n",
      "1533 [D loss: (0.548)(R 0.613, F 0.483)] [D acc: (0.695)(0.547, 0.844)] [G loss: 1.423] [G acc: 0.062]\n",
      "1534 [D loss: (0.498)(R 0.420, F 0.576)] [D acc: (0.703)(0.719, 0.688)] [G loss: 1.274] [G acc: 0.125]\n",
      "1535 [D loss: (0.524)(R 0.553, F 0.495)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.261] [G acc: 0.078]\n",
      "1536 [D loss: (0.597)(R 0.640, F 0.554)] [D acc: (0.656)(0.531, 0.781)] [G loss: 1.170] [G acc: 0.125]\n",
      "1537 [D loss: (0.499)(R 0.419, F 0.578)] [D acc: (0.781)(0.781, 0.781)] [G loss: 1.272] [G acc: 0.078]\n",
      "1538 [D loss: (0.499)(R 0.491, F 0.506)] [D acc: (0.781)(0.750, 0.812)] [G loss: 1.238] [G acc: 0.125]\n",
      "1539 [D loss: (0.472)(R 0.453, F 0.492)] [D acc: (0.773)(0.734, 0.812)] [G loss: 1.311] [G acc: 0.203]\n",
      "1540 [D loss: (0.635)(R 0.596, F 0.675)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.514] [G acc: 0.094]\n",
      "1541 [D loss: (0.522)(R 0.514, F 0.531)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.340] [G acc: 0.078]\n",
      "1542 [D loss: (0.469)(R 0.427, F 0.512)] [D acc: (0.750)(0.734, 0.766)] [G loss: 1.420] [G acc: 0.078]\n",
      "1543 [D loss: (0.564)(R 0.628, F 0.501)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.356] [G acc: 0.078]\n",
      "1544 [D loss: (0.605)(R 0.609, F 0.600)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.156] [G acc: 0.094]\n",
      "1545 [D loss: (0.522)(R 0.505, F 0.540)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.277] [G acc: 0.094]\n",
      "1546 [D loss: (0.692)(R 0.710, F 0.673)] [D acc: (0.617)(0.484, 0.750)] [G loss: 1.217] [G acc: 0.031]\n",
      "1547 [D loss: (0.564)(R 0.667, F 0.460)] [D acc: (0.695)(0.531, 0.859)] [G loss: 1.019] [G acc: 0.188]\n",
      "1548 [D loss: (0.535)(R 0.513, F 0.558)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.254] [G acc: 0.078]\n",
      "1549 [D loss: (0.544)(R 0.518, F 0.569)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.308] [G acc: 0.094]\n",
      "1550 [D loss: (0.526)(R 0.488, F 0.564)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.321] [G acc: 0.125]\n",
      "1551 [D loss: (0.552)(R 0.581, F 0.523)] [D acc: (0.680)(0.609, 0.750)] [G loss: 1.299] [G acc: 0.062]\n",
      "1552 [D loss: (0.569)(R 0.585, F 0.552)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.319] [G acc: 0.125]\n",
      "1553 [D loss: (0.586)(R 0.592, F 0.580)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.238] [G acc: 0.078]\n",
      "1554 [D loss: (0.560)(R 0.549, F 0.571)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.238] [G acc: 0.078]\n",
      "1555 [D loss: (0.494)(R 0.407, F 0.580)] [D acc: (0.750)(0.734, 0.766)] [G loss: 1.408] [G acc: 0.000]\n",
      "1556 [D loss: (0.670)(R 0.554, F 0.785)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.373] [G acc: 0.094]\n",
      "1557 [D loss: (0.500)(R 0.564, F 0.435)] [D acc: (0.750)(0.594, 0.906)] [G loss: 1.409] [G acc: 0.000]\n",
      "1558 [D loss: (0.578)(R 0.598, F 0.557)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.308] [G acc: 0.062]\n",
      "1559 [D loss: (0.511)(R 0.561, F 0.462)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.290] [G acc: 0.047]\n",
      "1560 [D loss: (0.534)(R 0.578, F 0.491)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.472] [G acc: 0.062]\n",
      "1561 [D loss: (0.441)(R 0.456, F 0.425)] [D acc: (0.773)(0.703, 0.844)] [G loss: 1.384] [G acc: 0.047]\n",
      "1562 [D loss: (0.511)(R 0.407, F 0.615)] [D acc: (0.797)(0.828, 0.766)] [G loss: 1.354] [G acc: 0.172]\n",
      "1563 [D loss: (0.508)(R 0.493, F 0.522)] [D acc: (0.758)(0.719, 0.797)] [G loss: 1.411] [G acc: 0.125]\n",
      "1564 [D loss: (0.505)(R 0.539, F 0.471)] [D acc: (0.711)(0.578, 0.844)] [G loss: 1.294] [G acc: 0.109]\n",
      "1565 [D loss: (0.524)(R 0.522, F 0.525)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.317] [G acc: 0.078]\n",
      "1566 [D loss: (0.529)(R 0.449, F 0.608)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.331] [G acc: 0.078]\n",
      "1567 [D loss: (0.526)(R 0.538, F 0.514)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.456] [G acc: 0.047]\n",
      "1568 [D loss: (0.476)(R 0.490, F 0.462)] [D acc: (0.789)(0.734, 0.844)] [G loss: 1.387] [G acc: 0.172]\n",
      "1569 [D loss: (0.585)(R 0.543, F 0.628)] [D acc: (0.719)(0.703, 0.734)] [G loss: 1.404] [G acc: 0.109]\n",
      "1570 [D loss: (0.506)(R 0.478, F 0.534)] [D acc: (0.727)(0.703, 0.750)] [G loss: 1.315] [G acc: 0.156]\n",
      "1571 [D loss: (0.569)(R 0.543, F 0.594)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.193] [G acc: 0.203]\n",
      "1572 [D loss: (0.474)(R 0.475, F 0.474)] [D acc: (0.797)(0.766, 0.828)] [G loss: 1.368] [G acc: 0.125]\n",
      "1573 [D loss: (0.584)(R 0.502, F 0.666)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.415] [G acc: 0.062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1574 [D loss: (0.556)(R 0.586, F 0.525)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.379] [G acc: 0.016]\n",
      "1575 [D loss: (0.544)(R 0.532, F 0.557)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.504] [G acc: 0.094]\n",
      "1576 [D loss: (0.595)(R 0.663, F 0.527)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.330] [G acc: 0.078]\n",
      "1577 [D loss: (0.688)(R 0.752, F 0.624)] [D acc: (0.641)(0.500, 0.781)] [G loss: 1.245] [G acc: 0.062]\n",
      "1578 [D loss: (0.480)(R 0.462, F 0.499)] [D acc: (0.773)(0.766, 0.781)] [G loss: 1.198] [G acc: 0.172]\n",
      "1579 [D loss: (0.621)(R 0.639, F 0.604)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.251] [G acc: 0.094]\n",
      "1580 [D loss: (0.602)(R 0.604, F 0.601)] [D acc: (0.672)(0.656, 0.688)] [G loss: 1.282] [G acc: 0.125]\n",
      "1581 [D loss: (0.687)(R 0.665, F 0.710)] [D acc: (0.648)(0.547, 0.750)] [G loss: 1.165] [G acc: 0.078]\n",
      "1582 [D loss: (0.507)(R 0.514, F 0.501)] [D acc: (0.758)(0.656, 0.859)] [G loss: 1.204] [G acc: 0.047]\n",
      "1583 [D loss: (0.570)(R 0.530, F 0.609)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.098] [G acc: 0.109]\n",
      "1584 [D loss: (0.562)(R 0.650, F 0.474)] [D acc: (0.727)(0.562, 0.891)] [G loss: 1.144] [G acc: 0.141]\n",
      "1585 [D loss: (0.541)(R 0.507, F 0.575)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.114] [G acc: 0.109]\n",
      "1586 [D loss: (0.510)(R 0.520, F 0.500)] [D acc: (0.766)(0.672, 0.859)] [G loss: 1.131] [G acc: 0.203]\n",
      "1587 [D loss: (0.537)(R 0.439, F 0.636)] [D acc: (0.703)(0.734, 0.672)] [G loss: 1.349] [G acc: 0.094]\n",
      "1588 [D loss: (0.528)(R 0.545, F 0.511)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.371] [G acc: 0.141]\n",
      "1589 [D loss: (0.482)(R 0.419, F 0.544)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.380] [G acc: 0.109]\n",
      "1590 [D loss: (0.564)(R 0.594, F 0.534)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.271] [G acc: 0.109]\n",
      "1591 [D loss: (0.529)(R 0.489, F 0.569)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.219] [G acc: 0.109]\n",
      "1592 [D loss: (0.516)(R 0.450, F 0.581)] [D acc: (0.773)(0.766, 0.781)] [G loss: 1.505] [G acc: 0.062]\n",
      "1593 [D loss: (0.621)(R 0.699, F 0.543)] [D acc: (0.680)(0.531, 0.828)] [G loss: 1.223] [G acc: 0.094]\n",
      "1594 [D loss: (0.487)(R 0.453, F 0.521)] [D acc: (0.758)(0.719, 0.797)] [G loss: 1.335] [G acc: 0.078]\n",
      "1595 [D loss: (0.512)(R 0.557, F 0.467)] [D acc: (0.789)(0.703, 0.875)] [G loss: 1.308] [G acc: 0.078]\n",
      "1596 [D loss: (0.606)(R 0.669, F 0.543)] [D acc: (0.680)(0.562, 0.797)] [G loss: 1.202] [G acc: 0.156]\n",
      "1597 [D loss: (0.542)(R 0.564, F 0.520)] [D acc: (0.789)(0.734, 0.844)] [G loss: 1.287] [G acc: 0.141]\n",
      "1598 [D loss: (0.480)(R 0.412, F 0.547)] [D acc: (0.758)(0.781, 0.734)] [G loss: 1.227] [G acc: 0.125]\n",
      "1599 [D loss: (0.556)(R 0.431, F 0.681)] [D acc: (0.711)(0.750, 0.672)] [G loss: 1.229] [G acc: 0.156]\n",
      "1600 [D loss: (0.586)(R 0.609, F 0.562)] [D acc: (0.680)(0.672, 0.688)] [G loss: 1.444] [G acc: 0.031]\n",
      "1601 [D loss: (0.487)(R 0.543, F 0.430)] [D acc: (0.773)(0.703, 0.844)] [G loss: 1.329] [G acc: 0.109]\n",
      "1602 [D loss: (0.565)(R 0.470, F 0.661)] [D acc: (0.711)(0.750, 0.672)] [G loss: 1.251] [G acc: 0.156]\n",
      "1603 [D loss: (0.550)(R 0.584, F 0.516)] [D acc: (0.734)(0.609, 0.859)] [G loss: 1.363] [G acc: 0.109]\n",
      "1604 [D loss: (0.549)(R 0.567, F 0.531)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.311] [G acc: 0.094]\n",
      "1605 [D loss: (0.419)(R 0.360, F 0.478)] [D acc: (0.820)(0.812, 0.828)] [G loss: 1.320] [G acc: 0.141]\n",
      "1606 [D loss: (0.630)(R 0.526, F 0.735)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.381] [G acc: 0.109]\n",
      "1607 [D loss: (0.514)(R 0.565, F 0.463)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.329] [G acc: 0.141]\n",
      "1608 [D loss: (0.598)(R 0.659, F 0.538)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.243] [G acc: 0.250]\n",
      "1609 [D loss: (0.513)(R 0.487, F 0.539)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.419] [G acc: 0.125]\n",
      "1610 [D loss: (0.560)(R 0.580, F 0.541)] [D acc: (0.727)(0.719, 0.734)] [G loss: 1.289] [G acc: 0.141]\n",
      "1611 [D loss: (0.589)(R 0.553, F 0.625)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.476] [G acc: 0.125]\n",
      "1612 [D loss: (0.566)(R 0.607, F 0.526)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.247] [G acc: 0.094]\n",
      "1613 [D loss: (0.553)(R 0.507, F 0.598)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.290] [G acc: 0.094]\n",
      "1614 [D loss: (0.596)(R 0.534, F 0.658)] [D acc: (0.711)(0.703, 0.719)] [G loss: 1.260] [G acc: 0.094]\n",
      "1615 [D loss: (0.574)(R 0.699, F 0.448)] [D acc: (0.727)(0.547, 0.906)] [G loss: 1.393] [G acc: 0.047]\n",
      "1616 [D loss: (0.575)(R 0.619, F 0.532)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.246] [G acc: 0.156]\n",
      "1617 [D loss: (0.569)(R 0.538, F 0.601)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.480] [G acc: 0.062]\n",
      "1618 [D loss: (0.578)(R 0.548, F 0.609)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.408] [G acc: 0.062]\n",
      "1619 [D loss: (0.599)(R 0.610, F 0.588)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.445] [G acc: 0.062]\n",
      "1620 [D loss: (0.506)(R 0.568, F 0.444)] [D acc: (0.789)(0.688, 0.891)] [G loss: 1.233] [G acc: 0.062]\n",
      "1621 [D loss: (0.567)(R 0.514, F 0.620)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.365] [G acc: 0.047]\n",
      "1622 [D loss: (0.522)(R 0.490, F 0.553)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.306] [G acc: 0.062]\n",
      "1623 [D loss: (0.570)(R 0.563, F 0.577)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.326] [G acc: 0.078]\n",
      "1624 [D loss: (0.483)(R 0.575, F 0.392)] [D acc: (0.789)(0.688, 0.891)] [G loss: 1.321] [G acc: 0.109]\n",
      "1625 [D loss: (0.507)(R 0.493, F 0.522)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.344] [G acc: 0.078]\n",
      "1626 [D loss: (0.543)(R 0.553, F 0.532)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.138] [G acc: 0.156]\n",
      "1627 [D loss: (0.522)(R 0.560, F 0.483)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.291] [G acc: 0.141]\n",
      "1628 [D loss: (0.473)(R 0.409, F 0.536)] [D acc: (0.742)(0.750, 0.734)] [G loss: 1.351] [G acc: 0.062]\n",
      "1629 [D loss: (0.573)(R 0.561, F 0.585)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.290] [G acc: 0.125]\n",
      "1630 [D loss: (0.581)(R 0.580, F 0.581)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.404] [G acc: 0.125]\n",
      "1631 [D loss: (0.618)(R 0.628, F 0.609)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.291] [G acc: 0.031]\n",
      "1632 [D loss: (0.537)(R 0.646, F 0.428)] [D acc: (0.742)(0.578, 0.906)] [G loss: 1.270] [G acc: 0.094]\n",
      "1633 [D loss: (0.574)(R 0.563, F 0.584)] [D acc: (0.648)(0.594, 0.703)] [G loss: 1.241] [G acc: 0.094]\n",
      "1634 [D loss: (0.522)(R 0.503, F 0.540)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.074] [G acc: 0.234]\n",
      "1635 [D loss: (0.549)(R 0.551, F 0.548)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.299] [G acc: 0.109]\n",
      "1636 [D loss: (0.596)(R 0.623, F 0.568)] [D acc: (0.672)(0.594, 0.750)] [G loss: 1.123] [G acc: 0.141]\n",
      "1637 [D loss: (0.560)(R 0.600, F 0.519)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.332] [G acc: 0.078]\n",
      "1638 [D loss: (0.529)(R 0.478, F 0.579)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.325] [G acc: 0.047]\n",
      "1639 [D loss: (0.547)(R 0.592, F 0.501)] [D acc: (0.711)(0.578, 0.844)] [G loss: 1.357] [G acc: 0.062]\n",
      "1640 [D loss: (0.476)(R 0.431, F 0.521)] [D acc: (0.797)(0.766, 0.828)] [G loss: 1.329] [G acc: 0.141]\n",
      "1641 [D loss: (0.556)(R 0.631, F 0.481)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.396] [G acc: 0.141]\n",
      "1642 [D loss: (0.513)(R 0.508, F 0.518)] [D acc: (0.758)(0.688, 0.828)] [G loss: 1.429] [G acc: 0.094]\n",
      "1643 [D loss: (0.547)(R 0.560, F 0.533)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.291] [G acc: 0.094]\n",
      "1644 [D loss: (0.514)(R 0.523, F 0.505)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.288] [G acc: 0.094]\n",
      "1645 [D loss: (0.559)(R 0.596, F 0.523)] [D acc: (0.758)(0.656, 0.859)] [G loss: 1.229] [G acc: 0.141]\n",
      "1646 [D loss: (0.514)(R 0.561, F 0.466)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.258] [G acc: 0.109]\n",
      "1647 [D loss: (0.544)(R 0.485, F 0.604)] [D acc: (0.742)(0.734, 0.750)] [G loss: 1.291] [G acc: 0.062]\n",
      "1648 [D loss: (0.514)(R 0.502, F 0.526)] [D acc: (0.781)(0.750, 0.812)] [G loss: 1.334] [G acc: 0.188]\n",
      "1649 [D loss: (0.523)(R 0.510, F 0.536)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.272] [G acc: 0.141]\n",
      "1650 [D loss: (0.622)(R 0.528, F 0.715)] [D acc: (0.711)(0.719, 0.703)] [G loss: 1.278] [G acc: 0.141]\n",
      "1651 [D loss: (0.519)(R 0.584, F 0.453)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.404] [G acc: 0.125]\n",
      "1652 [D loss: (0.478)(R 0.373, F 0.584)] [D acc: (0.789)(0.766, 0.812)] [G loss: 1.289] [G acc: 0.172]\n",
      "1653 [D loss: (0.527)(R 0.569, F 0.485)] [D acc: (0.773)(0.688, 0.859)] [G loss: 1.321] [G acc: 0.094]\n",
      "1654 [D loss: (0.594)(R 0.526, F 0.662)] [D acc: (0.703)(0.641, 0.766)] [G loss: 1.277] [G acc: 0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1655 [D loss: (0.553)(R 0.578, F 0.529)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.420] [G acc: 0.125]\n",
      "1656 [D loss: (0.558)(R 0.574, F 0.542)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.459] [G acc: 0.062]\n",
      "1657 [D loss: (0.567)(R 0.528, F 0.606)] [D acc: (0.734)(0.734, 0.734)] [G loss: 1.306] [G acc: 0.141]\n",
      "1658 [D loss: (0.536)(R 0.558, F 0.514)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.340] [G acc: 0.094]\n",
      "1659 [D loss: (0.499)(R 0.515, F 0.482)] [D acc: (0.750)(0.641, 0.859)] [G loss: 1.214] [G acc: 0.156]\n",
      "1660 [D loss: (0.497)(R 0.479, F 0.515)] [D acc: (0.773)(0.734, 0.812)] [G loss: 1.369] [G acc: 0.078]\n",
      "1661 [D loss: (0.472)(R 0.428, F 0.516)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.440] [G acc: 0.062]\n",
      "1662 [D loss: (0.461)(R 0.500, F 0.421)] [D acc: (0.781)(0.672, 0.891)] [G loss: 1.352] [G acc: 0.156]\n",
      "1663 [D loss: (0.428)(R 0.383, F 0.474)] [D acc: (0.789)(0.781, 0.797)] [G loss: 1.578] [G acc: 0.156]\n",
      "1664 [D loss: (0.547)(R 0.583, F 0.511)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.602] [G acc: 0.109]\n",
      "1665 [D loss: (0.531)(R 0.553, F 0.510)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.476] [G acc: 0.078]\n",
      "1666 [D loss: (0.476)(R 0.475, F 0.478)] [D acc: (0.773)(0.719, 0.828)] [G loss: 1.433] [G acc: 0.109]\n",
      "1667 [D loss: (0.582)(R 0.589, F 0.575)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.396] [G acc: 0.094]\n",
      "1668 [D loss: (0.623)(R 0.582, F 0.665)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.480] [G acc: 0.031]\n",
      "1669 [D loss: (0.498)(R 0.548, F 0.448)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.522] [G acc: 0.062]\n",
      "1670 [D loss: (0.491)(R 0.475, F 0.506)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.352] [G acc: 0.062]\n",
      "1671 [D loss: (0.576)(R 0.624, F 0.528)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.387] [G acc: 0.094]\n",
      "1672 [D loss: (0.520)(R 0.550, F 0.490)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.423] [G acc: 0.078]\n",
      "1673 [D loss: (0.544)(R 0.543, F 0.546)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.305] [G acc: 0.125]\n",
      "1674 [D loss: (0.593)(R 0.603, F 0.582)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.230] [G acc: 0.109]\n",
      "1675 [D loss: (0.556)(R 0.563, F 0.550)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.330] [G acc: 0.109]\n",
      "1676 [D loss: (0.611)(R 0.628, F 0.594)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.242] [G acc: 0.078]\n",
      "1677 [D loss: (0.590)(R 0.610, F 0.571)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.200] [G acc: 0.172]\n",
      "1678 [D loss: (0.557)(R 0.560, F 0.554)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.255] [G acc: 0.094]\n",
      "1679 [D loss: (0.539)(R 0.499, F 0.580)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.345] [G acc: 0.094]\n",
      "1680 [D loss: (0.565)(R 0.528, F 0.603)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.357] [G acc: 0.078]\n",
      "1681 [D loss: (0.546)(R 0.587, F 0.505)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.387] [G acc: 0.141]\n",
      "1682 [D loss: (0.612)(R 0.626, F 0.598)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.289] [G acc: 0.094]\n",
      "1683 [D loss: (0.532)(R 0.517, F 0.548)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.215] [G acc: 0.125]\n",
      "1684 [D loss: (0.562)(R 0.553, F 0.570)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.370] [G acc: 0.109]\n",
      "1685 [D loss: (0.563)(R 0.566, F 0.559)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.395] [G acc: 0.078]\n",
      "1686 [D loss: (0.513)(R 0.595, F 0.430)] [D acc: (0.742)(0.609, 0.875)] [G loss: 1.329] [G acc: 0.078]\n",
      "1687 [D loss: (0.676)(R 0.591, F 0.760)] [D acc: (0.641)(0.656, 0.625)] [G loss: 1.359] [G acc: 0.078]\n",
      "1688 [D loss: (0.589)(R 0.705, F 0.473)] [D acc: (0.656)(0.453, 0.859)] [G loss: 1.198] [G acc: 0.094]\n",
      "1689 [D loss: (0.529)(R 0.565, F 0.494)] [D acc: (0.727)(0.641, 0.812)] [G loss: 1.334] [G acc: 0.156]\n",
      "1690 [D loss: (0.537)(R 0.577, F 0.497)] [D acc: (0.742)(0.625, 0.859)] [G loss: 1.267] [G acc: 0.094]\n",
      "1691 [D loss: (0.657)(R 0.504, F 0.810)] [D acc: (0.672)(0.672, 0.672)] [G loss: 1.350] [G acc: 0.094]\n",
      "1692 [D loss: (0.597)(R 0.650, F 0.544)] [D acc: (0.695)(0.625, 0.766)] [G loss: 1.234] [G acc: 0.125]\n",
      "1693 [D loss: (0.552)(R 0.617, F 0.488)] [D acc: (0.703)(0.562, 0.844)] [G loss: 1.344] [G acc: 0.031]\n",
      "1694 [D loss: (0.523)(R 0.471, F 0.575)] [D acc: (0.789)(0.766, 0.812)] [G loss: 1.266] [G acc: 0.078]\n",
      "1695 [D loss: (0.591)(R 0.609, F 0.572)] [D acc: (0.641)(0.547, 0.734)] [G loss: 1.336] [G acc: 0.031]\n",
      "1696 [D loss: (0.542)(R 0.571, F 0.512)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.155] [G acc: 0.094]\n",
      "1697 [D loss: (0.464)(R 0.408, F 0.521)] [D acc: (0.781)(0.750, 0.812)] [G loss: 1.350] [G acc: 0.047]\n",
      "1698 [D loss: (0.508)(R 0.479, F 0.537)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.418] [G acc: 0.047]\n",
      "1699 [D loss: (0.456)(R 0.418, F 0.495)] [D acc: (0.773)(0.766, 0.781)] [G loss: 1.521] [G acc: 0.062]\n",
      "1700 [D loss: (0.568)(R 0.627, F 0.510)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.322] [G acc: 0.109]\n",
      "1701 [D loss: (0.491)(R 0.466, F 0.516)] [D acc: (0.773)(0.719, 0.828)] [G loss: 1.494] [G acc: 0.094]\n",
      "1702 [D loss: (0.615)(R 0.622, F 0.608)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.508] [G acc: 0.062]\n",
      "1703 [D loss: (0.538)(R 0.528, F 0.548)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.462] [G acc: 0.062]\n",
      "1704 [D loss: (0.590)(R 0.633, F 0.547)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.202] [G acc: 0.234]\n",
      "1705 [D loss: (0.488)(R 0.475, F 0.502)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.302] [G acc: 0.141]\n",
      "1706 [D loss: (0.542)(R 0.541, F 0.542)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.302] [G acc: 0.078]\n",
      "1707 [D loss: (0.589)(R 0.516, F 0.662)] [D acc: (0.664)(0.688, 0.641)] [G loss: 1.265] [G acc: 0.141]\n",
      "1708 [D loss: (0.502)(R 0.548, F 0.456)] [D acc: (0.758)(0.656, 0.859)] [G loss: 1.364] [G acc: 0.141]\n",
      "1709 [D loss: (0.609)(R 0.552, F 0.667)] [D acc: (0.695)(0.703, 0.688)] [G loss: 1.394] [G acc: 0.062]\n",
      "1710 [D loss: (0.551)(R 0.646, F 0.456)] [D acc: (0.734)(0.562, 0.906)] [G loss: 1.213] [G acc: 0.094]\n",
      "1711 [D loss: (0.538)(R 0.457, F 0.618)] [D acc: (0.734)(0.750, 0.719)] [G loss: 1.470] [G acc: 0.109]\n",
      "1712 [D loss: (0.603)(R 0.631, F 0.575)] [D acc: (0.688)(0.609, 0.766)] [G loss: 1.221] [G acc: 0.172]\n",
      "1713 [D loss: (0.576)(R 0.548, F 0.604)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.286] [G acc: 0.094]\n",
      "1714 [D loss: (0.450)(R 0.455, F 0.445)] [D acc: (0.812)(0.766, 0.859)] [G loss: 1.350] [G acc: 0.109]\n",
      "1715 [D loss: (0.593)(R 0.530, F 0.657)] [D acc: (0.672)(0.703, 0.641)] [G loss: 1.366] [G acc: 0.047]\n",
      "1716 [D loss: (0.612)(R 0.636, F 0.587)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.318] [G acc: 0.078]\n",
      "1717 [D loss: (0.615)(R 0.628, F 0.602)] [D acc: (0.562)(0.500, 0.625)] [G loss: 1.189] [G acc: 0.141]\n",
      "1718 [D loss: (0.536)(R 0.584, F 0.489)] [D acc: (0.711)(0.562, 0.859)] [G loss: 1.248] [G acc: 0.062]\n",
      "1719 [D loss: (0.538)(R 0.548, F 0.529)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.270] [G acc: 0.062]\n",
      "1720 [D loss: (0.491)(R 0.437, F 0.545)] [D acc: (0.773)(0.703, 0.844)] [G loss: 1.239] [G acc: 0.109]\n",
      "1721 [D loss: (0.479)(R 0.477, F 0.480)] [D acc: (0.797)(0.719, 0.875)] [G loss: 1.437] [G acc: 0.094]\n",
      "1722 [D loss: (0.531)(R 0.492, F 0.570)] [D acc: (0.750)(0.703, 0.797)] [G loss: 1.409] [G acc: 0.078]\n",
      "1723 [D loss: (0.592)(R 0.533, F 0.651)] [D acc: (0.695)(0.719, 0.672)] [G loss: 1.325] [G acc: 0.125]\n",
      "1724 [D loss: (0.558)(R 0.552, F 0.565)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.334] [G acc: 0.109]\n",
      "1725 [D loss: (0.541)(R 0.622, F 0.460)] [D acc: (0.711)(0.578, 0.844)] [G loss: 1.370] [G acc: 0.125]\n",
      "1726 [D loss: (0.557)(R 0.585, F 0.528)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.269] [G acc: 0.141]\n",
      "1727 [D loss: (0.430)(R 0.286, F 0.574)] [D acc: (0.766)(0.812, 0.719)] [G loss: 1.265] [G acc: 0.188]\n",
      "1728 [D loss: (0.576)(R 0.584, F 0.569)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.417] [G acc: 0.094]\n",
      "1729 [D loss: (0.508)(R 0.440, F 0.576)] [D acc: (0.781)(0.766, 0.797)] [G loss: 1.545] [G acc: 0.078]\n",
      "1730 [D loss: (0.523)(R 0.603, F 0.442)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.353] [G acc: 0.109]\n",
      "1731 [D loss: (0.492)(R 0.420, F 0.564)] [D acc: (0.750)(0.734, 0.766)] [G loss: 1.364] [G acc: 0.094]\n",
      "1732 [D loss: (0.576)(R 0.467, F 0.686)] [D acc: (0.719)(0.734, 0.703)] [G loss: 1.464] [G acc: 0.078]\n",
      "1733 [D loss: (0.534)(R 0.624, F 0.444)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.555] [G acc: 0.047]\n",
      "1734 [D loss: (0.532)(R 0.520, F 0.545)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.444] [G acc: 0.062]\n",
      "1735 [D loss: (0.505)(R 0.561, F 0.450)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.452] [G acc: 0.016]\n",
      "1736 [D loss: (0.602)(R 0.567, F 0.637)] [D acc: (0.672)(0.641, 0.703)] [G loss: 1.411] [G acc: 0.047]\n",
      "1737 [D loss: (0.517)(R 0.597, F 0.436)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.405] [G acc: 0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1738 [D loss: (0.606)(R 0.686, F 0.526)] [D acc: (0.695)(0.547, 0.844)] [G loss: 1.322] [G acc: 0.094]\n",
      "1739 [D loss: (0.534)(R 0.532, F 0.537)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.198] [G acc: 0.141]\n",
      "1740 [D loss: (0.569)(R 0.560, F 0.578)] [D acc: (0.664)(0.594, 0.734)] [G loss: 1.397] [G acc: 0.078]\n",
      "1741 [D loss: (0.472)(R 0.455, F 0.490)] [D acc: (0.773)(0.688, 0.859)] [G loss: 1.461] [G acc: 0.078]\n",
      "1742 [D loss: (0.553)(R 0.609, F 0.497)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.421] [G acc: 0.109]\n",
      "1743 [D loss: (0.530)(R 0.529, F 0.530)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.361] [G acc: 0.078]\n",
      "1744 [D loss: (0.629)(R 0.611, F 0.647)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.249] [G acc: 0.109]\n",
      "1745 [D loss: (0.482)(R 0.510, F 0.454)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.305] [G acc: 0.109]\n",
      "1746 [D loss: (0.521)(R 0.555, F 0.487)] [D acc: (0.727)(0.609, 0.844)] [G loss: 1.258] [G acc: 0.094]\n",
      "1747 [D loss: (0.444)(R 0.433, F 0.456)] [D acc: (0.820)(0.781, 0.859)] [G loss: 1.422] [G acc: 0.078]\n",
      "1748 [D loss: (0.587)(R 0.453, F 0.721)] [D acc: (0.695)(0.688, 0.703)] [G loss: 1.389] [G acc: 0.047]\n",
      "1749 [D loss: (0.570)(R 0.589, F 0.551)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.336] [G acc: 0.125]\n",
      "1750 [D loss: (0.418)(R 0.484, F 0.351)] [D acc: (0.820)(0.719, 0.922)] [G loss: 1.294] [G acc: 0.109]\n",
      "1751 [D loss: (0.567)(R 0.498, F 0.635)] [D acc: (0.719)(0.734, 0.703)] [G loss: 1.355] [G acc: 0.141]\n",
      "1752 [D loss: (0.490)(R 0.475, F 0.505)] [D acc: (0.797)(0.734, 0.859)] [G loss: 1.419] [G acc: 0.094]\n",
      "1753 [D loss: (0.532)(R 0.499, F 0.565)] [D acc: (0.773)(0.688, 0.859)] [G loss: 1.284] [G acc: 0.203]\n",
      "1754 [D loss: (0.648)(R 0.594, F 0.702)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.218] [G acc: 0.141]\n",
      "1755 [D loss: (0.576)(R 0.621, F 0.530)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.161] [G acc: 0.234]\n",
      "1756 [D loss: (0.512)(R 0.531, F 0.493)] [D acc: (0.766)(0.719, 0.812)] [G loss: 1.166] [G acc: 0.188]\n",
      "1757 [D loss: (0.516)(R 0.536, F 0.495)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.327] [G acc: 0.094]\n",
      "1758 [D loss: (0.447)(R 0.425, F 0.468)] [D acc: (0.789)(0.766, 0.812)] [G loss: 1.290] [G acc: 0.078]\n",
      "1759 [D loss: (0.514)(R 0.493, F 0.534)] [D acc: (0.781)(0.797, 0.766)] [G loss: 1.206] [G acc: 0.266]\n",
      "1760 [D loss: (0.610)(R 0.474, F 0.745)] [D acc: (0.672)(0.703, 0.641)] [G loss: 1.525] [G acc: 0.094]\n",
      "1761 [D loss: (0.615)(R 0.668, F 0.561)] [D acc: (0.680)(0.594, 0.766)] [G loss: 1.335] [G acc: 0.109]\n",
      "1762 [D loss: (0.554)(R 0.533, F 0.574)] [D acc: (0.727)(0.688, 0.766)] [G loss: 1.314] [G acc: 0.141]\n",
      "1763 [D loss: (0.558)(R 0.511, F 0.604)] [D acc: (0.742)(0.750, 0.734)] [G loss: 1.316] [G acc: 0.109]\n",
      "1764 [D loss: (0.569)(R 0.612, F 0.526)] [D acc: (0.672)(0.562, 0.781)] [G loss: 1.206] [G acc: 0.156]\n",
      "1765 [D loss: (0.516)(R 0.561, F 0.472)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.238] [G acc: 0.156]\n",
      "1766 [D loss: (0.625)(R 0.605, F 0.645)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.319] [G acc: 0.094]\n",
      "1767 [D loss: (0.623)(R 0.646, F 0.599)] [D acc: (0.680)(0.531, 0.828)] [G loss: 1.234] [G acc: 0.031]\n",
      "1768 [D loss: (0.555)(R 0.609, F 0.501)] [D acc: (0.703)(0.547, 0.859)] [G loss: 1.212] [G acc: 0.094]\n",
      "1769 [D loss: (0.508)(R 0.479, F 0.537)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.410] [G acc: 0.062]\n",
      "1770 [D loss: (0.539)(R 0.599, F 0.480)] [D acc: (0.703)(0.578, 0.828)] [G loss: 1.281] [G acc: 0.047]\n",
      "1771 [D loss: (0.564)(R 0.577, F 0.551)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.370] [G acc: 0.125]\n",
      "1772 [D loss: (0.515)(R 0.514, F 0.517)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.333] [G acc: 0.109]\n",
      "1773 [D loss: (0.547)(R 0.460, F 0.634)] [D acc: (0.703)(0.781, 0.625)] [G loss: 1.382] [G acc: 0.109]\n",
      "1774 [D loss: (0.536)(R 0.529, F 0.542)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.422] [G acc: 0.078]\n",
      "1775 [D loss: (0.615)(R 0.706, F 0.524)] [D acc: (0.648)(0.516, 0.781)] [G loss: 1.409] [G acc: 0.125]\n",
      "1776 [D loss: (0.561)(R 0.565, F 0.557)] [D acc: (0.727)(0.625, 0.828)] [G loss: 1.371] [G acc: 0.047]\n",
      "1777 [D loss: (0.650)(R 0.751, F 0.548)] [D acc: (0.680)(0.531, 0.828)] [G loss: 1.316] [G acc: 0.047]\n",
      "1778 [D loss: (0.577)(R 0.603, F 0.552)] [D acc: (0.695)(0.594, 0.797)] [G loss: 1.300] [G acc: 0.094]\n",
      "1779 [D loss: (0.521)(R 0.578, F 0.464)] [D acc: (0.773)(0.688, 0.859)] [G loss: 1.306] [G acc: 0.062]\n",
      "1780 [D loss: (0.579)(R 0.642, F 0.515)] [D acc: (0.680)(0.547, 0.812)] [G loss: 1.170] [G acc: 0.141]\n",
      "1781 [D loss: (0.598)(R 0.493, F 0.703)] [D acc: (0.711)(0.719, 0.703)] [G loss: 1.306] [G acc: 0.094]\n",
      "1782 [D loss: (0.608)(R 0.649, F 0.567)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.120] [G acc: 0.172]\n",
      "1783 [D loss: (0.555)(R 0.549, F 0.562)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.309] [G acc: 0.078]\n",
      "1784 [D loss: (0.529)(R 0.553, F 0.505)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.391] [G acc: 0.078]\n",
      "1785 [D loss: (0.576)(R 0.592, F 0.560)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.292] [G acc: 0.062]\n",
      "1786 [D loss: (0.584)(R 0.672, F 0.497)] [D acc: (0.664)(0.484, 0.844)] [G loss: 1.174] [G acc: 0.141]\n",
      "1787 [D loss: (0.519)(R 0.496, F 0.543)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.232] [G acc: 0.094]\n",
      "1788 [D loss: (0.538)(R 0.544, F 0.532)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.223] [G acc: 0.141]\n",
      "1789 [D loss: (0.569)(R 0.548, F 0.590)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.252] [G acc: 0.172]\n",
      "1790 [D loss: (0.531)(R 0.520, F 0.542)] [D acc: (0.766)(0.734, 0.797)] [G loss: 1.286] [G acc: 0.141]\n",
      "1791 [D loss: (0.650)(R 0.659, F 0.640)] [D acc: (0.617)(0.594, 0.641)] [G loss: 1.282] [G acc: 0.141]\n",
      "1792 [D loss: (0.586)(R 0.569, F 0.603)] [D acc: (0.656)(0.656, 0.656)] [G loss: 1.214] [G acc: 0.109]\n",
      "1793 [D loss: (0.601)(R 0.622, F 0.580)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.176] [G acc: 0.156]\n",
      "1794 [D loss: (0.570)(R 0.512, F 0.628)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.188] [G acc: 0.094]\n",
      "1795 [D loss: (0.519)(R 0.513, F 0.526)] [D acc: (0.742)(0.688, 0.797)] [G loss: 1.167] [G acc: 0.172]\n",
      "1796 [D loss: (0.621)(R 0.590, F 0.652)] [D acc: (0.625)(0.594, 0.656)] [G loss: 1.396] [G acc: 0.125]\n",
      "1797 [D loss: (0.592)(R 0.644, F 0.540)] [D acc: (0.648)(0.547, 0.750)] [G loss: 1.154] [G acc: 0.109]\n",
      "1798 [D loss: (0.579)(R 0.539, F 0.620)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.197] [G acc: 0.125]\n",
      "1799 [D loss: (0.518)(R 0.500, F 0.537)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.205] [G acc: 0.094]\n",
      "1800 [D loss: (0.618)(R 0.611, F 0.625)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.194] [G acc: 0.188]\n",
      "1801 [D loss: (0.545)(R 0.562, F 0.527)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.273] [G acc: 0.125]\n",
      "1802 [D loss: (0.487)(R 0.422, F 0.553)] [D acc: (0.773)(0.750, 0.797)] [G loss: 1.378] [G acc: 0.141]\n",
      "1803 [D loss: (0.572)(R 0.531, F 0.612)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.384] [G acc: 0.078]\n",
      "1804 [D loss: (0.516)(R 0.516, F 0.516)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.452] [G acc: 0.078]\n",
      "1805 [D loss: (0.595)(R 0.622, F 0.567)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.249] [G acc: 0.031]\n",
      "1806 [D loss: (0.506)(R 0.578, F 0.435)] [D acc: (0.734)(0.562, 0.906)] [G loss: 1.472] [G acc: 0.031]\n",
      "1807 [D loss: (0.504)(R 0.481, F 0.527)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.397] [G acc: 0.125]\n",
      "1808 [D loss: (0.637)(R 0.612, F 0.661)] [D acc: (0.656)(0.578, 0.734)] [G loss: 1.326] [G acc: 0.062]\n",
      "1809 [D loss: (0.566)(R 0.511, F 0.621)] [D acc: (0.703)(0.672, 0.734)] [G loss: 1.326] [G acc: 0.141]\n",
      "1810 [D loss: (0.615)(R 0.630, F 0.600)] [D acc: (0.672)(0.547, 0.797)] [G loss: 1.336] [G acc: 0.156]\n",
      "1811 [D loss: (0.585)(R 0.632, F 0.537)] [D acc: (0.672)(0.578, 0.766)] [G loss: 1.372] [G acc: 0.062]\n",
      "1812 [D loss: (0.538)(R 0.634, F 0.442)] [D acc: (0.703)(0.594, 0.812)] [G loss: 1.368] [G acc: 0.078]\n",
      "1813 [D loss: (0.526)(R 0.501, F 0.551)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.285] [G acc: 0.109]\n",
      "1814 [D loss: (0.569)(R 0.542, F 0.595)] [D acc: (0.734)(0.688, 0.781)] [G loss: 1.278] [G acc: 0.156]\n",
      "1815 [D loss: (0.549)(R 0.511, F 0.587)] [D acc: (0.766)(0.719, 0.812)] [G loss: 1.268] [G acc: 0.047]\n",
      "1816 [D loss: (0.585)(R 0.577, F 0.593)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.302] [G acc: 0.109]\n",
      "1817 [D loss: (0.594)(R 0.608, F 0.580)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.271] [G acc: 0.062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1818 [D loss: (0.528)(R 0.571, F 0.485)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.256] [G acc: 0.109]\n",
      "1819 [D loss: (0.488)(R 0.484, F 0.492)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.357] [G acc: 0.156]\n",
      "1820 [D loss: (0.576)(R 0.498, F 0.655)] [D acc: (0.688)(0.672, 0.703)] [G loss: 1.357] [G acc: 0.031]\n",
      "1821 [D loss: (0.600)(R 0.559, F 0.640)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.250] [G acc: 0.062]\n",
      "1822 [D loss: (0.521)(R 0.579, F 0.463)] [D acc: (0.711)(0.578, 0.844)] [G loss: 1.264] [G acc: 0.016]\n",
      "1823 [D loss: (0.534)(R 0.505, F 0.564)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.355] [G acc: 0.047]\n",
      "1824 [D loss: (0.496)(R 0.533, F 0.460)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.370] [G acc: 0.047]\n",
      "1825 [D loss: (0.537)(R 0.484, F 0.591)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.215] [G acc: 0.172]\n",
      "1826 [D loss: (0.565)(R 0.582, F 0.548)] [D acc: (0.656)(0.609, 0.703)] [G loss: 1.282] [G acc: 0.109]\n",
      "1827 [D loss: (0.609)(R 0.605, F 0.614)] [D acc: (0.719)(0.578, 0.859)] [G loss: 1.326] [G acc: 0.094]\n",
      "1828 [D loss: (0.552)(R 0.631, F 0.474)] [D acc: (0.766)(0.625, 0.906)] [G loss: 1.212] [G acc: 0.062]\n",
      "1829 [D loss: (0.585)(R 0.525, F 0.644)] [D acc: (0.688)(0.625, 0.750)] [G loss: 1.256] [G acc: 0.047]\n",
      "1830 [D loss: (0.546)(R 0.571, F 0.521)] [D acc: (0.766)(0.688, 0.844)] [G loss: 1.148] [G acc: 0.234]\n",
      "1831 [D loss: (0.537)(R 0.594, F 0.479)] [D acc: (0.734)(0.641, 0.828)] [G loss: 1.333] [G acc: 0.125]\n",
      "1832 [D loss: (0.517)(R 0.452, F 0.583)] [D acc: (0.758)(0.766, 0.750)] [G loss: 1.347] [G acc: 0.141]\n",
      "1833 [D loss: (0.568)(R 0.586, F 0.551)] [D acc: (0.695)(0.641, 0.750)] [G loss: 1.151] [G acc: 0.125]\n",
      "1834 [D loss: (0.664)(R 0.576, F 0.751)] [D acc: (0.648)(0.625, 0.672)] [G loss: 1.290] [G acc: 0.062]\n",
      "1835 [D loss: (0.477)(R 0.532, F 0.423)] [D acc: (0.789)(0.641, 0.938)] [G loss: 1.270] [G acc: 0.094]\n",
      "1836 [D loss: (0.561)(R 0.604, F 0.518)] [D acc: (0.688)(0.562, 0.812)] [G loss: 1.431] [G acc: 0.078]\n",
      "1837 [D loss: (0.561)(R 0.636, F 0.486)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.310] [G acc: 0.109]\n",
      "1838 [D loss: (0.544)(R 0.382, F 0.706)] [D acc: (0.758)(0.812, 0.703)] [G loss: 1.256] [G acc: 0.125]\n",
      "1839 [D loss: (0.513)(R 0.588, F 0.437)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.219] [G acc: 0.141]\n",
      "1840 [D loss: (0.504)(R 0.450, F 0.558)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.405] [G acc: 0.125]\n",
      "1841 [D loss: (0.529)(R 0.542, F 0.515)] [D acc: (0.797)(0.719, 0.875)] [G loss: 1.270] [G acc: 0.109]\n",
      "1842 [D loss: (0.542)(R 0.581, F 0.502)] [D acc: (0.766)(0.672, 0.859)] [G loss: 1.337] [G acc: 0.078]\n",
      "1843 [D loss: (0.537)(R 0.573, F 0.501)] [D acc: (0.719)(0.609, 0.828)] [G loss: 1.297] [G acc: 0.109]\n",
      "1844 [D loss: (0.473)(R 0.449, F 0.496)] [D acc: (0.766)(0.719, 0.812)] [G loss: 1.537] [G acc: 0.078]\n",
      "1845 [D loss: (0.527)(R 0.568, F 0.486)] [D acc: (0.758)(0.656, 0.859)] [G loss: 1.449] [G acc: 0.078]\n",
      "1846 [D loss: (0.458)(R 0.473, F 0.443)] [D acc: (0.812)(0.750, 0.875)] [G loss: 1.501] [G acc: 0.031]\n",
      "1847 [D loss: (0.580)(R 0.580, F 0.579)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.353] [G acc: 0.047]\n",
      "1848 [D loss: (0.527)(R 0.465, F 0.589)] [D acc: (0.750)(0.719, 0.781)] [G loss: 1.429] [G acc: 0.078]\n",
      "1849 [D loss: (0.504)(R 0.507, F 0.501)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.368] [G acc: 0.125]\n",
      "1850 [D loss: (0.493)(R 0.537, F 0.448)] [D acc: (0.742)(0.641, 0.844)] [G loss: 1.522] [G acc: 0.062]\n",
      "1851 [D loss: (0.555)(R 0.577, F 0.534)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.410] [G acc: 0.109]\n",
      "1852 [D loss: (0.602)(R 0.624, F 0.580)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.328] [G acc: 0.094]\n",
      "1853 [D loss: (0.546)(R 0.544, F 0.548)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.246] [G acc: 0.078]\n",
      "1854 [D loss: (0.478)(R 0.504, F 0.452)] [D acc: (0.766)(0.672, 0.859)] [G loss: 1.288] [G acc: 0.141]\n",
      "1855 [D loss: (0.532)(R 0.543, F 0.520)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.491] [G acc: 0.094]\n",
      "1856 [D loss: (0.521)(R 0.569, F 0.473)] [D acc: (0.789)(0.688, 0.891)] [G loss: 1.499] [G acc: 0.078]\n",
      "1857 [D loss: (0.592)(R 0.654, F 0.531)] [D acc: (0.641)(0.562, 0.719)] [G loss: 1.394] [G acc: 0.062]\n",
      "1858 [D loss: (0.489)(R 0.501, F 0.476)] [D acc: (0.789)(0.719, 0.859)] [G loss: 1.450] [G acc: 0.078]\n",
      "1859 [D loss: (0.615)(R 0.590, F 0.640)] [D acc: (0.750)(0.672, 0.828)] [G loss: 1.488] [G acc: 0.078]\n",
      "1860 [D loss: (0.490)(R 0.518, F 0.462)] [D acc: (0.750)(0.703, 0.797)] [G loss: 1.475] [G acc: 0.078]\n",
      "1861 [D loss: (0.597)(R 0.593, F 0.600)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.324] [G acc: 0.109]\n",
      "1862 [D loss: (0.594)(R 0.742, F 0.445)] [D acc: (0.688)(0.531, 0.844)] [G loss: 1.291] [G acc: 0.078]\n",
      "1863 [D loss: (0.497)(R 0.474, F 0.521)] [D acc: (0.758)(0.719, 0.797)] [G loss: 1.423] [G acc: 0.078]\n",
      "1864 [D loss: (0.550)(R 0.515, F 0.585)] [D acc: (0.656)(0.641, 0.672)] [G loss: 1.337] [G acc: 0.078]\n",
      "1865 [D loss: (0.579)(R 0.602, F 0.557)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.326] [G acc: 0.047]\n",
      "1866 [D loss: (0.570)(R 0.658, F 0.481)] [D acc: (0.641)(0.484, 0.797)] [G loss: 1.297] [G acc: 0.094]\n",
      "1867 [D loss: (0.570)(R 0.492, F 0.647)] [D acc: (0.758)(0.750, 0.766)] [G loss: 1.310] [G acc: 0.172]\n",
      "1868 [D loss: (0.566)(R 0.442, F 0.689)] [D acc: (0.688)(0.734, 0.641)] [G loss: 1.383] [G acc: 0.078]\n",
      "1869 [D loss: (0.572)(R 0.649, F 0.496)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.490] [G acc: 0.031]\n",
      "1870 [D loss: (0.577)(R 0.618, F 0.535)] [D acc: (0.625)(0.547, 0.703)] [G loss: 1.312] [G acc: 0.078]\n",
      "1871 [D loss: (0.503)(R 0.507, F 0.498)] [D acc: (0.758)(0.688, 0.828)] [G loss: 1.273] [G acc: 0.078]\n",
      "1872 [D loss: (0.519)(R 0.509, F 0.528)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.179] [G acc: 0.125]\n",
      "1873 [D loss: (0.624)(R 0.759, F 0.489)] [D acc: (0.609)(0.406, 0.812)] [G loss: 1.337] [G acc: 0.062]\n",
      "1874 [D loss: (0.549)(R 0.506, F 0.593)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.283] [G acc: 0.109]\n",
      "1875 [D loss: (0.502)(R 0.479, F 0.524)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.408] [G acc: 0.062]\n",
      "1876 [D loss: (0.566)(R 0.557, F 0.575)] [D acc: (0.734)(0.656, 0.812)] [G loss: 1.387] [G acc: 0.094]\n",
      "1877 [D loss: (0.521)(R 0.584, F 0.458)] [D acc: (0.727)(0.562, 0.891)] [G loss: 1.321] [G acc: 0.047]\n",
      "1878 [D loss: (0.582)(R 0.554, F 0.609)] [D acc: (0.680)(0.641, 0.719)] [G loss: 1.212] [G acc: 0.078]\n",
      "1879 [D loss: (0.512)(R 0.520, F 0.505)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.321] [G acc: 0.062]\n",
      "1880 [D loss: (0.571)(R 0.559, F 0.582)] [D acc: (0.750)(0.656, 0.844)] [G loss: 1.297] [G acc: 0.141]\n",
      "1881 [D loss: (0.556)(R 0.537, F 0.574)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.377] [G acc: 0.125]\n",
      "1882 [D loss: (0.535)(R 0.548, F 0.522)] [D acc: (0.758)(0.672, 0.844)] [G loss: 1.302] [G acc: 0.109]\n",
      "1883 [D loss: (0.496)(R 0.505, F 0.488)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.415] [G acc: 0.141]\n",
      "1884 [D loss: (0.635)(R 0.550, F 0.721)] [D acc: (0.688)(0.750, 0.625)] [G loss: 1.158] [G acc: 0.078]\n",
      "1885 [D loss: (0.482)(R 0.434, F 0.531)] [D acc: (0.773)(0.750, 0.797)] [G loss: 1.295] [G acc: 0.141]\n",
      "1886 [D loss: (0.539)(R 0.564, F 0.514)] [D acc: (0.734)(0.719, 0.750)] [G loss: 1.404] [G acc: 0.109]\n",
      "1887 [D loss: (0.514)(R 0.531, F 0.498)] [D acc: (0.766)(0.688, 0.844)] [G loss: 1.260] [G acc: 0.188]\n",
      "1888 [D loss: (0.529)(R 0.483, F 0.575)] [D acc: (0.719)(0.688, 0.750)] [G loss: 1.317] [G acc: 0.156]\n",
      "1889 [D loss: (0.555)(R 0.536, F 0.574)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.346] [G acc: 0.156]\n",
      "1890 [D loss: (0.544)(R 0.544, F 0.543)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.335] [G acc: 0.109]\n",
      "1891 [D loss: (0.563)(R 0.586, F 0.540)] [D acc: (0.695)(0.578, 0.812)] [G loss: 1.237] [G acc: 0.188]\n",
      "1892 [D loss: (0.596)(R 0.574, F 0.618)] [D acc: (0.656)(0.562, 0.750)] [G loss: 1.244] [G acc: 0.141]\n",
      "1893 [D loss: (0.460)(R 0.500, F 0.420)] [D acc: (0.812)(0.719, 0.906)] [G loss: 1.336] [G acc: 0.094]\n",
      "1894 [D loss: (0.534)(R 0.491, F 0.577)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.348] [G acc: 0.062]\n",
      "1895 [D loss: (0.563)(R 0.630, F 0.495)] [D acc: (0.688)(0.594, 0.781)] [G loss: 1.224] [G acc: 0.125]\n",
      "1896 [D loss: (0.482)(R 0.480, F 0.483)] [D acc: (0.773)(0.672, 0.875)] [G loss: 1.417] [G acc: 0.047]\n",
      "1897 [D loss: (0.557)(R 0.555, F 0.559)] [D acc: (0.711)(0.641, 0.781)] [G loss: 1.235] [G acc: 0.125]\n",
      "1898 [D loss: (0.480)(R 0.567, F 0.392)] [D acc: (0.781)(0.672, 0.891)] [G loss: 1.225] [G acc: 0.125]\n",
      "1899 [D loss: (0.533)(R 0.551, F 0.514)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.263] [G acc: 0.109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1900 [D loss: (0.542)(R 0.564, F 0.519)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.188] [G acc: 0.203]\n",
      "1901 [D loss: (0.501)(R 0.493, F 0.510)] [D acc: (0.758)(0.688, 0.828)] [G loss: 1.282] [G acc: 0.188]\n",
      "1902 [D loss: (0.603)(R 0.515, F 0.692)] [D acc: (0.656)(0.625, 0.688)] [G loss: 1.276] [G acc: 0.109]\n",
      "1903 [D loss: (0.553)(R 0.502, F 0.604)] [D acc: (0.742)(0.719, 0.766)] [G loss: 1.324] [G acc: 0.125]\n",
      "1904 [D loss: (0.463)(R 0.433, F 0.492)] [D acc: (0.766)(0.734, 0.797)] [G loss: 1.360] [G acc: 0.125]\n",
      "1905 [D loss: (0.561)(R 0.579, F 0.543)] [D acc: (0.703)(0.625, 0.781)] [G loss: 1.352] [G acc: 0.094]\n",
      "1906 [D loss: (0.551)(R 0.592, F 0.510)] [D acc: (0.742)(0.656, 0.828)] [G loss: 1.316] [G acc: 0.094]\n",
      "1907 [D loss: (0.547)(R 0.596, F 0.497)] [D acc: (0.734)(0.609, 0.859)] [G loss: 1.247] [G acc: 0.156]\n",
      "1908 [D loss: (0.638)(R 0.571, F 0.705)] [D acc: (0.648)(0.609, 0.688)] [G loss: 1.269] [G acc: 0.172]\n",
      "1909 [D loss: (0.558)(R 0.588, F 0.527)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.264] [G acc: 0.094]\n",
      "1910 [D loss: (0.575)(R 0.548, F 0.603)] [D acc: (0.680)(0.625, 0.734)] [G loss: 1.225] [G acc: 0.125]\n",
      "1911 [D loss: (0.534)(R 0.433, F 0.635)] [D acc: (0.797)(0.766, 0.828)] [G loss: 1.272] [G acc: 0.141]\n",
      "1912 [D loss: (0.478)(R 0.458, F 0.498)] [D acc: (0.781)(0.734, 0.828)] [G loss: 1.329] [G acc: 0.156]\n",
      "1913 [D loss: (0.557)(R 0.507, F 0.607)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.429] [G acc: 0.047]\n",
      "1914 [D loss: (0.564)(R 0.615, F 0.513)] [D acc: (0.680)(0.578, 0.781)] [G loss: 1.392] [G acc: 0.141]\n",
      "1915 [D loss: (0.523)(R 0.543, F 0.504)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.407] [G acc: 0.062]\n",
      "1916 [D loss: (0.585)(R 0.575, F 0.595)] [D acc: (0.648)(0.578, 0.719)] [G loss: 1.228] [G acc: 0.094]\n",
      "1917 [D loss: (0.574)(R 0.644, F 0.504)] [D acc: (0.719)(0.594, 0.844)] [G loss: 1.265] [G acc: 0.109]\n",
      "1918 [D loss: (0.527)(R 0.543, F 0.510)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.288] [G acc: 0.125]\n",
      "1919 [D loss: (0.515)(R 0.522, F 0.508)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.308] [G acc: 0.125]\n",
      "1920 [D loss: (0.519)(R 0.560, F 0.479)] [D acc: (0.711)(0.594, 0.828)] [G loss: 1.203] [G acc: 0.109]\n",
      "1921 [D loss: (0.522)(R 0.483, F 0.562)] [D acc: (0.742)(0.672, 0.812)] [G loss: 1.301] [G acc: 0.078]\n",
      "1922 [D loss: (0.472)(R 0.459, F 0.485)] [D acc: (0.781)(0.734, 0.828)] [G loss: 1.363] [G acc: 0.062]\n",
      "1923 [D loss: (0.548)(R 0.542, F 0.554)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.404] [G acc: 0.125]\n",
      "1924 [D loss: (0.537)(R 0.494, F 0.581)] [D acc: (0.742)(0.703, 0.781)] [G loss: 1.240] [G acc: 0.125]\n",
      "1925 [D loss: (0.508)(R 0.523, F 0.492)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.349] [G acc: 0.031]\n",
      "1926 [D loss: (0.610)(R 0.551, F 0.668)] [D acc: (0.695)(0.703, 0.688)] [G loss: 1.292] [G acc: 0.125]\n",
      "1927 [D loss: (0.609)(R 0.661, F 0.557)] [D acc: (0.695)(0.562, 0.828)] [G loss: 1.340] [G acc: 0.141]\n",
      "1928 [D loss: (0.561)(R 0.614, F 0.509)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.399] [G acc: 0.078]\n",
      "1929 [D loss: (0.617)(R 0.718, F 0.516)] [D acc: (0.656)(0.500, 0.812)] [G loss: 1.392] [G acc: 0.094]\n",
      "1930 [D loss: (0.503)(R 0.470, F 0.535)] [D acc: (0.758)(0.703, 0.812)] [G loss: 1.294] [G acc: 0.062]\n",
      "1931 [D loss: (0.618)(R 0.654, F 0.582)] [D acc: (0.633)(0.578, 0.688)] [G loss: 1.184] [G acc: 0.156]\n",
      "1932 [D loss: (0.469)(R 0.464, F 0.474)] [D acc: (0.781)(0.703, 0.859)] [G loss: 1.206] [G acc: 0.109]\n",
      "1933 [D loss: (0.553)(R 0.467, F 0.639)] [D acc: (0.711)(0.719, 0.703)] [G loss: 1.355] [G acc: 0.031]\n",
      "1934 [D loss: (0.587)(R 0.643, F 0.531)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.362] [G acc: 0.047]\n",
      "1935 [D loss: (0.515)(R 0.533, F 0.498)] [D acc: (0.727)(0.656, 0.797)] [G loss: 1.802] [G acc: 0.047]\n",
      "1936 [D loss: (0.571)(R 0.638, F 0.505)] [D acc: (0.672)(0.516, 0.828)] [G loss: 1.214] [G acc: 0.109]\n",
      "1937 [D loss: (0.567)(R 0.595, F 0.540)] [D acc: (0.672)(0.547, 0.797)] [G loss: 1.295] [G acc: 0.094]\n",
      "1938 [D loss: (0.531)(R 0.505, F 0.556)] [D acc: (0.766)(0.688, 0.844)] [G loss: 1.285] [G acc: 0.141]\n",
      "1939 [D loss: (0.528)(R 0.527, F 0.529)] [D acc: (0.711)(0.656, 0.766)] [G loss: 1.238] [G acc: 0.078]\n",
      "1940 [D loss: (0.648)(R 0.534, F 0.763)] [D acc: (0.656)(0.672, 0.641)] [G loss: 1.385] [G acc: 0.047]\n",
      "1941 [D loss: (0.533)(R 0.543, F 0.522)] [D acc: (0.781)(0.750, 0.812)] [G loss: 1.317] [G acc: 0.078]\n",
      "1942 [D loss: (0.556)(R 0.554, F 0.558)] [D acc: (0.711)(0.672, 0.750)] [G loss: 1.223] [G acc: 0.094]\n",
      "1943 [D loss: (0.492)(R 0.518, F 0.467)] [D acc: (0.773)(0.703, 0.844)] [G loss: 1.270] [G acc: 0.141]\n",
      "1944 [D loss: (0.476)(R 0.453, F 0.498)] [D acc: (0.750)(0.688, 0.812)] [G loss: 1.411] [G acc: 0.125]\n",
      "1945 [D loss: (0.547)(R 0.519, F 0.576)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.331] [G acc: 0.078]\n",
      "1946 [D loss: (0.543)(R 0.608, F 0.478)] [D acc: (0.664)(0.547, 0.781)] [G loss: 1.236] [G acc: 0.188]\n",
      "1947 [D loss: (0.576)(R 0.511, F 0.642)] [D acc: (0.695)(0.672, 0.719)] [G loss: 1.303] [G acc: 0.078]\n",
      "1948 [D loss: (0.590)(R 0.601, F 0.579)] [D acc: (0.711)(0.609, 0.812)] [G loss: 1.307] [G acc: 0.062]\n",
      "1949 [D loss: (0.552)(R 0.555, F 0.549)] [D acc: (0.695)(0.656, 0.734)] [G loss: 1.285] [G acc: 0.078]\n",
      "1950 [D loss: (0.563)(R 0.643, F 0.483)] [D acc: (0.703)(0.562, 0.844)] [G loss: 1.172] [G acc: 0.125]\n",
      "1951 [D loss: (0.472)(R 0.415, F 0.528)] [D acc: (0.758)(0.734, 0.781)] [G loss: 1.365] [G acc: 0.125]\n",
      "1952 [D loss: (0.539)(R 0.507, F 0.570)] [D acc: (0.680)(0.656, 0.703)] [G loss: 1.552] [G acc: 0.047]\n",
      "1953 [D loss: (0.521)(R 0.605, F 0.437)] [D acc: (0.742)(0.625, 0.859)] [G loss: 1.378] [G acc: 0.109]\n",
      "1954 [D loss: (0.599)(R 0.543, F 0.655)] [D acc: (0.711)(0.688, 0.734)] [G loss: 1.253] [G acc: 0.094]\n",
      "1955 [D loss: (0.595)(R 0.536, F 0.655)] [D acc: (0.664)(0.609, 0.719)] [G loss: 1.183] [G acc: 0.219]\n",
      "1956 [D loss: (0.532)(R 0.587, F 0.477)] [D acc: (0.727)(0.578, 0.875)] [G loss: 1.254] [G acc: 0.125]\n",
      "1957 [D loss: (0.490)(R 0.427, F 0.553)] [D acc: (0.750)(0.734, 0.766)] [G loss: 1.364] [G acc: 0.125]\n",
      "1958 [D loss: (0.549)(R 0.562, F 0.537)] [D acc: (0.719)(0.641, 0.797)] [G loss: 1.267] [G acc: 0.125]\n",
      "1959 [D loss: (0.569)(R 0.591, F 0.547)] [D acc: (0.695)(0.609, 0.781)] [G loss: 1.217] [G acc: 0.125]\n",
      "1960 [D loss: (0.503)(R 0.495, F 0.512)] [D acc: (0.734)(0.672, 0.797)] [G loss: 1.362] [G acc: 0.078]\n",
      "1961 [D loss: (0.543)(R 0.520, F 0.567)] [D acc: (0.711)(0.625, 0.797)] [G loss: 1.252] [G acc: 0.094]\n",
      "1962 [D loss: (0.614)(R 0.599, F 0.629)] [D acc: (0.672)(0.609, 0.734)] [G loss: 1.171] [G acc: 0.156]\n",
      "1963 [D loss: (0.630)(R 0.628, F 0.633)] [D acc: (0.688)(0.547, 0.828)] [G loss: 1.152] [G acc: 0.203]\n",
      "1964 [D loss: (0.523)(R 0.594, F 0.451)] [D acc: (0.789)(0.641, 0.938)] [G loss: 1.254] [G acc: 0.172]\n",
      "1965 [D loss: (0.581)(R 0.529, F 0.633)] [D acc: (0.664)(0.625, 0.703)] [G loss: 1.339] [G acc: 0.062]\n",
      "1966 [D loss: (0.540)(R 0.556, F 0.523)] [D acc: (0.781)(0.672, 0.891)] [G loss: 1.203] [G acc: 0.125]\n",
      "1967 [D loss: (0.570)(R 0.552, F 0.587)] [D acc: (0.688)(0.641, 0.734)] [G loss: 1.330] [G acc: 0.078]\n",
      "1968 [D loss: (0.689)(R 0.703, F 0.674)] [D acc: (0.656)(0.469, 0.844)] [G loss: 1.244] [G acc: 0.078]\n",
      "1969 [D loss: (0.541)(R 0.565, F 0.517)] [D acc: (0.703)(0.609, 0.797)] [G loss: 1.175] [G acc: 0.125]\n",
      "1970 [D loss: (0.539)(R 0.508, F 0.570)] [D acc: (0.734)(0.703, 0.766)] [G loss: 1.266] [G acc: 0.094]\n",
      "1971 [D loss: (0.465)(R 0.469, F 0.460)] [D acc: (0.766)(0.688, 0.844)] [G loss: 1.258] [G acc: 0.125]\n",
      "1972 [D loss: (0.489)(R 0.457, F 0.522)] [D acc: (0.766)(0.719, 0.812)] [G loss: 1.367] [G acc: 0.078]\n",
      "1973 [D loss: (0.501)(R 0.491, F 0.512)] [D acc: (0.750)(0.703, 0.797)] [G loss: 1.433] [G acc: 0.016]\n",
      "1974 [D loss: (0.500)(R 0.483, F 0.516)] [D acc: (0.734)(0.625, 0.844)] [G loss: 1.349] [G acc: 0.109]\n",
      "1975 [D loss: (0.457)(R 0.438, F 0.476)] [D acc: (0.789)(0.688, 0.891)] [G loss: 1.454] [G acc: 0.062]\n",
      "1976 [D loss: (0.521)(R 0.603, F 0.439)] [D acc: (0.719)(0.578, 0.859)] [G loss: 1.378] [G acc: 0.047]\n",
      "1977 [D loss: (0.544)(R 0.499, F 0.589)] [D acc: (0.719)(0.672, 0.766)] [G loss: 1.384] [G acc: 0.062]\n",
      "1978 [D loss: (0.573)(R 0.678, F 0.469)] [D acc: (0.695)(0.531, 0.859)] [G loss: 1.384] [G acc: 0.078]\n",
      "1979 [D loss: (0.468)(R 0.452, F 0.485)] [D acc: (0.805)(0.750, 0.859)] [G loss: 1.490] [G acc: 0.062]\n",
      "1980 [D loss: (0.509)(R 0.601, F 0.417)] [D acc: (0.758)(0.609, 0.906)] [G loss: 1.463] [G acc: 0.078]\n",
      "1981 [D loss: (0.631)(R 0.598, F 0.663)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.280] [G acc: 0.094]\n",
      "1982 [D loss: (0.575)(R 0.657, F 0.493)] [D acc: (0.688)(0.531, 0.844)] [G loss: 1.296] [G acc: 0.109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1983 [D loss: (0.506)(R 0.480, F 0.531)] [D acc: (0.797)(0.734, 0.859)] [G loss: 1.469] [G acc: 0.094]\n",
      "1984 [D loss: (0.531)(R 0.547, F 0.515)] [D acc: (0.742)(0.625, 0.859)] [G loss: 1.323] [G acc: 0.078]\n",
      "1985 [D loss: (0.569)(R 0.564, F 0.574)] [D acc: (0.688)(0.656, 0.719)] [G loss: 1.450] [G acc: 0.047]\n",
      "1986 [D loss: (0.557)(R 0.684, F 0.430)] [D acc: (0.727)(0.578, 0.875)] [G loss: 1.298] [G acc: 0.078]\n",
      "1987 [D loss: (0.471)(R 0.449, F 0.492)] [D acc: (0.766)(0.703, 0.828)] [G loss: 1.172] [G acc: 0.094]\n",
      "1988 [D loss: (0.482)(R 0.439, F 0.525)] [D acc: (0.758)(0.719, 0.797)] [G loss: 1.248] [G acc: 0.125]\n",
      "1989 [D loss: (0.576)(R 0.518, F 0.633)] [D acc: (0.703)(0.656, 0.750)] [G loss: 1.443] [G acc: 0.062]\n",
      "1990 [D loss: (0.517)(R 0.520, F 0.513)] [D acc: (0.727)(0.672, 0.781)] [G loss: 1.581] [G acc: 0.000]\n",
      "1991 [D loss: (0.563)(R 0.640, F 0.487)] [D acc: (0.711)(0.562, 0.859)] [G loss: 1.378] [G acc: 0.062]\n",
      "1992 [D loss: (0.487)(R 0.537, F 0.436)] [D acc: (0.773)(0.656, 0.891)] [G loss: 1.384] [G acc: 0.062]\n",
      "1993 [D loss: (0.542)(R 0.617, F 0.467)] [D acc: (0.719)(0.578, 0.859)] [G loss: 1.436] [G acc: 0.062]\n",
      "1994 [D loss: (0.366)(R 0.287, F 0.445)] [D acc: (0.867)(0.844, 0.891)] [G loss: 1.468] [G acc: 0.078]\n",
      "1995 [D loss: (0.542)(R 0.459, F 0.624)] [D acc: (0.766)(0.750, 0.781)] [G loss: 1.459] [G acc: 0.031]\n",
      "1996 [D loss: (0.533)(R 0.592, F 0.474)] [D acc: (0.742)(0.609, 0.875)] [G loss: 1.461] [G acc: 0.047]\n",
      "1997 [D loss: (0.521)(R 0.462, F 0.580)] [D acc: (0.734)(0.750, 0.719)] [G loss: 1.405] [G acc: 0.078]\n",
      "1998 [D loss: (0.464)(R 0.473, F 0.456)] [D acc: (0.773)(0.750, 0.797)] [G loss: 1.560] [G acc: 0.125]\n",
      "1999 [D loss: (0.513)(R 0.580, F 0.447)] [D acc: (0.750)(0.641, 0.859)] [G loss: 1.482] [G acc: 0.109]\n"
     ]
    }
   ],
   "source": [
    "train(x_train, batch_size = batch_size, epochs= epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Image\n",
    "\n",
    "### Epoch 0\n",
    "![Epoch 0](../result/camel/sample_0.png)\n",
    "\n",
    "### Epoch 100\n",
    "![Epoch 0](../result/camel/sample_100.png)\n",
    "\n",
    "\n",
    "### Epoch 200\n",
    "![Epoch 0](../result/camel/sample_200.png)\n",
    "\n",
    "### Epoch 500\n",
    "![Epoch 0](../result/camel/sample_500.png)\n",
    "\n",
    "### Epoch 1000\n",
    "![Epoch 0](../result/camel/sample_1000.png)\n",
    "\n",
    "### Epoch 1950\n",
    "![Epoch 0](../result/camel/sample_1950.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
